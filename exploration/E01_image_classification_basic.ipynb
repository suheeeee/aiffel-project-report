{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 가위바위보 이미지 분류기\n",
    "\n",
    "<br />\n",
    "\n",
    "#### 목표\n",
    "  일반적인 딥러닝 개발 순서인 **데이터 준비 → 딥러닝 네트워크 설계 → 학습 → 테스트(평가)**에 따라 가위바위보 이미지를 분류하는 이미지 분류기 모델을 만들어 본다.\n",
    "  \n",
    "<br />\n",
    "<br />\n",
    "\n",
    "## 데이터 준비\n",
    "#### 1. 데이터 수집\n",
    "* 사진 데이터를 모아 rock, scissor, paper 폴더에 분류하였다.\n",
    "* data spec\n",
    "    * image amount : 8913(가위 : 3005, 바위 : 2902, 보 : 3006)\n",
    "    * test set : 램덤하게 뽑은 100개의 데이터\n",
    "    * validation set : 램덤하게 뽑은 100개의 데이터\n",
    "    * train set : 8713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합계 244\n",
      "drwxr-xr-x 2 aiffel aiffel 86016  1월  5 23:46 paper\n",
      "drwxr-xr-x 2 aiffel aiffel 77824  1월  5 23:46 rock\n",
      "drwxr-xr-x 2 aiffel aiffel 86016  1월  5 23:46 scissor\n"
     ]
    }
   ],
   "source": [
    "# 이미지 데이터들은 data 디렉토리 하위에 rock, scissor, paper로 분류되어 있다.\n",
    "!ls -l ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 데이터 리사이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import os, glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiffel/workspace/aiffel-exploation-workspace/E1_rock_scissor_paper/data/scissor 이미지 resize 완료!\n",
      "/home/aiffel/workspace/aiffel-exploation-workspace/E1_rock_scissor_paper/data/rock 이미지 resize 완료!\n",
      "/home/aiffel/workspace/aiffel-exploation-workspace/E1_rock_scissor_paper/data/paper 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "def resize_image(workspace_dir_path):\n",
    "    image_dir_paths = [ workspace_dir_path + \"/scissor\", workspace_dir_path + \"/rock\", workspace_dir_path + \"/paper\"]\n",
    "\n",
    "    for image_dir_path in image_dir_paths:\n",
    "        images=glob.glob(image_dir_path + \"/*.jpg\")\n",
    "\n",
    "        # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "        target_size=(28,28)\n",
    "        for img in images:\n",
    "            old_img=Image.open(img)\n",
    "            new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "            new_img.save(img,\"JPEG\")\n",
    "\n",
    "        print(image_dir_path + \" 이미지 resize 완료!\")\n",
    "        \n",
    "workspace_dir_path = os.getenv(\"HOME\") + \"/workspace/aiffel-exploation-workspace/E1_rock_scissor_paper/data\"\n",
    "resize_image(workspace_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_path, number_of_data=300):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "            \n",
    "    print(\"입력된 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8913, 28, 28, 3)\n",
      "입력된 이미지 개수는 8913 입니다.\n",
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAabklEQVR4nO2de4yc51XGnzO3vXt3fV9fYseOa+rcnLJJkyakbgohjaBpJQpNS5SKgosgEhUgUQWhRog/AuWiSkCFS0NTVFoVtSVBSmlNKISmaeJN4tZ27Ca+xV7v3bvrvY13dmYOf+wE3HTf59vuZWbF+/yk1ezOM+/3vfN988w3O+c955i7Qwjx/59UrScghKgOMrsQkSCzCxEJMrsQkSCzCxEJmWruLJeq88ZMQ1BPtYQ1ACg3ZYPaTHmGjvXJPNUbpstUz81YUCuVeUSj1FhHdW9upHoBfPvTk1NE5Mcl4/x5I0F3S4jmePicIZ2mQ6cTzmkqR7YNoIwSGcz3nWSM1EyR6vUIv14AoCkV3kOqSOYNoKku/HrquzyGS4X8nDtflNnN7B4AnwaQBvD37v4oe3xjpgHvXLsvqNfveyvdX/7WjUHtwmQvHVvsOk71a18jhgGwYzD8IWh0ip+c0Rt2Ur1wx16qn3P+wjrV9XJQy5y5QMe25y9THSV+XMppPrdSqSOoWVsTHXtyvJ/qjTs2UX2iOBbUMs1832uJGQGgrnuA6tc6f4O/uXltUGsaHKVjb9l5TVDbf+iLQW3BH+PNLA3gbwC8B8AeAPeb2Z6Fbk8Isbws5n/2WwCcdPfT7l4A8GUA9y3NtIQQS81izL4ZwPkr/u6u3PcjmNl+M+sys65CubCI3QkhFsNizD7XlwA/9m2Nux9w905378ylcovYnRBiMSzG7N0Atl7x9xYAPYubjhBiuViM2Q8B2GVmV5tZDsAHATy5NNMSQiw1Cw69uXvRzB4C8E3Mht4ec/djbEwpBQy3hOO25Qtn6D4bj4bDQJ07f+zrgh+hY+9tVB+fPEr1vtHwh5b05tV07HU3v43q57L1VD9//jzVpwfCIaaJsQk6Nm08HtyS5SGkdEKsnIXhLw4N0rHbr+ahtb4p/txA4vT1Lfx5Tw4PU33fLW/n4w+foHr/ePic7buOB7VOdp8LatPFcCh0UXF2d38KwFOL2YYQojpouawQkSCzCxEJMrsQkSCzCxEJMrsQkSCzCxEJVc1nn8kCg+vD+qoCj5tmXhwNau1943Ts7j08dpm/8Xqqd2+7KqiVm9rp2FHnOQFnXvgh1Ye6+6jeWgy/ZxfbwqmUAFCanqR6PiGnPFNKqgMQTqHNJWy7kBDrRmqa66WwXtfIawhc/9brqH7y5Emqb0lIoS3mwmnRow3clgNkaQMpu6AruxCxILMLEQkyuxCRILMLEQkyuxCRILMLEQlVDb2V08DEmrDeMcLH7xkNvzdteoVXIr00wsNfqbfvpvr2u8Mpss1brqZjXz74ItXzPUNUL5/mz621Y11QG6vn6bNDeR6yrEsoRd1c5qG3dS3hcs/ZhCrWpy68TvWfu+8XqP5LD3woqH3py1+mY08e5+HQjlae1lx0fh2tWxc+Z985f5qO3bVza1g8Ez7eurILEQkyuxCRILMLEQkyuxCRILMLEQkyuxCRILMLEQlVjbNbCshkw7WFU1O8Y+iulnBp4T3g7Z6/38e7mfa8fpbqm267Nqg1NvPWwU1tLVTfTGKuAJDvGaX6xdFLQW04k9D2uIGXgm5vIgsjADTN8JbN+UvhEtyjpJwyAPzWh3+V6u//lQ9Q/YbO8DkbOv4aHXvq+S6ql7I8RXbM+HW0pyGc+jtZz19Pm7dsCGrlrOLsQkSPzC5EJMjsQkSCzC5EJMjsQkSCzC5EJMjsQkRCVePsmTKwfjoc120o8JjtaClc9rjbwq1qAQCtzVTesnkb1QuXwvt++kneyPbSaZ6v3rSarxHYecM1VEd/b1Aan7hIh1qWv9+Xp3m55uEhXoQgNdkd1B7+xB/SsXe98y6qF0l7YgC48HI4ln7x+Kt0rI3yNR/NHbyV9WCel0W/NBx+Tdz8M3fQsX0eLgTASkkvyuxmdhbAOIASgKK7dy5me0KI5WMpruzvcnd+6RJC1Bz9zy5EJCzW7A7gW2b2opntn+sBZrbfzLrMrKtY4HXghBDLx2I/xt/u7j1mth7AQTM74e7PXPkAdz8A4AAANLW18W/ghBDLxqKu7O7eU7kdAPB1ALcsxaSEEEvPgs1uZk1m1vLG7wDuBnB0qSYmhFhaFvMxfgOAr5vZG9v5J3f/NzYgVwQ2D4RjhEm50ydHwjXOzxZ56+Hde3gL3u1btlP9+GvhtsmvPPssHTtdJMFPAJs38Rh/+07ednnHpnCcPnOOn+ILZ85Q3SfCedcAsLGd5+r/7h/9cVB7x81vp2Pb6/n6g9I0/6/wsc/+Q1D7728cpGP37txF9YLx49rczNd1pNeFz2l9x0Y6dnAw3EegaOHX2oLN7u6nAdy40PFCiOqi0JsQkSCzCxEJMrsQkSCzCxEJMrsQkVDVFNf0jKOtL5yW2LwjXCIXAPo2h8v3ptt4aV+/9i1UP/l6OE0UAI4/dyioNSekgQ5e5iWTL5Z4OuTaEg/FXL0+XGJ76zoeAvI+Xkp6954bqP7Ahz5M9Tv3XR/UilM8rJe5zEOWB7/5Dap/9xvfDGq7O0jbYwD5hFbV3UM896vjJn7ctnWGE0TPjQzTsbmm8DlNpcLXb13ZhYgEmV2ISJDZhYgEmV2ISJDZhYgEmV2ISJDZhYiE6paStgzW1YVT+w69epaOX/Xum4La7jt53YyXj/2Q6gPP8ha9rWPhWHpDmqdabmhbRfUTM7zcc//rg1Q/f+pIUPupxtV07IffezfV777r3VRvW8O3X7RwvLp+W3h9AAA88ed/S/XH/voA1a9aH46lG3gM/+JFHuu+ufOnqf6WO++k+r8fPRzUnnv1OB07OBlel1EcGQ1qurILEQkyuxCRILMLEQkyuxCRILMLEQkyuxCRILMLEQlVjbPPlMvomQq3wm3avJmObyNtlcfK/H1rKM/zk0dnwiWuAaA4mQ9rpJU0AAzPJLSTbuL58OMj4TLWALB9Szie/OsPfICOvbvzHVS3Ij8uZeetjWdbCszN3/3Jn9Cx//EErUyOlhZexrowE86XLxdKdOyOnVdTfe2GdVT/5r8+SfUjQ+FzOjR+iY61+nDtBvPwmg9d2YWIBJldiEiQ2YWIBJldiEiQ2YWIBJldiEiQ2YWIhKrG2QspoLsxHN9sWc3zvkeHRoPayfPn6dj+0+eoDhL/B4DLqfC8DTxmOznF46aTozx3+ufv4LnTv/+xjwW1n02IoyPP547pBL2llcoHvx2u3X7oULgWPwCMk7xtAGhGOIYPAPmx0aC2azdv4X3t28K1EwBguMzXH9x4fbhePgCketqC2siLL9KxI+fIa7lQCO+TbhWAmT1mZgNmdvSK+1ab2UEze61y2560HSFEbZnPx/jPA7jnTfd9AsDT7r4LwNOVv4UQK5hEs7v7MwDe/DnzPgCPV35/HMD7lnZaQoilZqFf0G1w914AqNyuDz3QzPabWZeZdc2U+BpwIcTysezfxrv7AXfvdPfObLpuuXcnhAiwULP3m1kHAFRuB5ZuSkKI5WChZn8SwIOV3x8E8MTSTEcIsVwkxtnN7EsA9gFYa2bdAD4J4FEAXzGzjwI4B4AnTVfwXAqFbeEc5DU7eR9yK4d7iQ+c47XVp873U33sEq/dbh6OX7Y28ffMjavbqP7+295F9V974H6qX7uX9AKf5OsHkhjPh/P4AeDI916g+p/96aNBbe91N9Kx199zLdXzl/jc1q4KfpWEd9zxTjq2Y/sOqvcmHNdPf/4fqL53z08FtZY2vnbhmeeeC2o9py4EtUSzu3volca7BwghVhRaLitEJMjsQkSCzC5EJMjsQkSCzC5EJFQ3xbVcwOvj4VTU+p5waA0AUkPh5bb9R0/RsVNj41TPtdZTfeeePUHttp/mIaKPvPdeqt+wtYPq5clRqg8dfTWord3E2yIPjIxR/Z+/8i9Uf/rgf1L9zlvDrYvvetfP0rHX7+Fpoi1NvJS0NbUFNb8cDqUCQM8QTzvONOWo3pCg1+eyQS1nPK04XQz7wDyceqsruxCRILMLEQkyuxCRILMLEQkyuxCRILMLEQkyuxCRYE5avC41qfo6z5GYcruHW9ECQOZiuO1ye6aNjt2dkE55zU08prvnpnCc/eYb3kLHTp0/S/Ud7byEdluCjlK4JfTJ7h469NXzvVRvaOSFg3df81aqb2gOx5tHLvES26va2qieW7+B6uV8OJZ+YZDXW+kZ4CnPFwZ5SvX3urqofvpceL3Js899l47tG2DnbATuMzaXoiu7EJEgswsRCTK7EJEgswsRCTK7EJEgswsRCTK7EJFQ1Xz2Na2r8L577w7qDc5zgNuza4LaW7bxeO+NN3Tyba8Llx0GgPJMOGabKV2mY6+5msfh0wXemhgp3kmnMBnef7nET/Htb99H9dYOng/fc57Ho6dGhoJaNs3rF+SaE9YXzITXXQDAsdMng9pLx47RsU996yDVv/Lk16meS/PXcrEUzllPJVyDmxsagtrU5fDaBV3ZhYgEmV2ISJDZhYgEmV2ISJDZhYgEmV2ISJDZhYiEquazX3/dHv/aP38hqDfkmuj4tDUHtfJ0uA43ABQvz5ni+79kSlxvzIS338JDqihN8bztuoaE5Q7Tk1Qup8LjU+u38G2Dx/DHxsM1ygEglbAGoLkQrr9eLPD1Cc++/BLVv/qNp6j+yrkzQS3XymP4qSw/qRdHRqje38NbhM8USN36MvdkuRyuXzA4cgaFmfzC8tnN7DEzGzCzo1fc94iZXTCzw5Uf3gVBCFFz5vMx/vMA7pnj/r9y972VH/4WK4SoOYlmd/dnAPBeOEKIFc9ivqB7yMx+UPmYHyxUZmb7zazLzLqGh/n/OUKI5WOhZv8MgJ0A9gLoBfAXoQe6+wF373T3ztWrefFCIcTysSCzu3u/u5fcvQzgswBuWdppCSGWmgWZ3cyurAf9fgBHQ48VQqwMEvPZzexLAPYBWGtm3QA+CWCfme0F4ADOAvjYfHZmlkIuG46VT8+E44cAkEa4x3ouE87xBYD6Jv6+5kUeZ0cmHPss1vHe7peN18O3Jj6+3BQ+ZgDgHs6NzhnvQ16Y4f3Zxy6H89EBYGqK5+I/+1/fD2ovvPACHds3wGPVPX19VE9lwi/v8Uv8eXds2Ur1ZrJtACi1t1H9yCvHg9qqhLF5kgtfIutmEs3u7vfPcffnksYJIVYWWi4rRCTI7EJEgswuRCTI7EJEgswuRCRUtZR0KpVGc3NLUC+RkAIApMrh96a08adSTgitlcq8LDGb2/Q0H5vO8rllMzxN1LJJ78nsuCWkMPOpI5vlqcPNCeWe165fF9TGJ3nY7tSpU1RvWx0uLQ4A06T898TlPB3b08NbXa/ZyEuPr1/P9a0T4dDf4EW+rDxP5l4ul4OaruxCRILMLkQkyOxCRILMLkQkyOxCRILMLkQkyOxCREJV4+xlLyN/ORz7tISQcDYVfm9KpXgcPUXGzm6cHwpzEuNP87FsbQEAIKGct5d46i8sHFstl/nahaRC4vV1PD2X7RsAfvFD9wW1V0/yOPqhl1+muidkJbOZ1dXxlOjBYd6KejKhDPau3byF+JYt4RLfF3p76VjW6rpk4YOiK7sQkSCzCxEJMrsQkSCzCxEJMrsQkSCzCxEJMrsQkVD1fPbGRl4WmY5HOL5oCS2Xy0UeD06K07M4e2Lb64R48HSex2yT1ghkSSnqdEIufCMSzkeJl6KezvO88NeOhtsmn+vh8eSJKX5cpnt5Kem6+vBxaW/n3YkaS3x9wejoKNUv9Jyn+jqS756iKwSA5lXhczYzTdai0K0KIf7fILMLEQkyuxCRILMLEQkyuxCRILMLEQkyuxCRUNU4e6lUxqXxcFw2mwrH0YHZls8h0jw0SePkAJBJ8froLIc4lVCzvjjDc8qR8LyzjbylM9LhQP70eLjNNQAUCglx9IS87aGLPO/7oY8/HNTOnAnH4AGgvonHussJ6xecrJ0YGOHzbm7m6w/SOf566U3ISW9f0xbU6uv4tutJ7YXUYvLZzWyrmX3bzI6b2TEz+53K/avN7KCZvVa55asUhBA1ZT4f44sAfs/d3wrgVgC/bWZ7AHwCwNPuvgvA05W/hRArlESzu3uvu79U+X0cwHEAmwHcB+DxysMeB/C+ZZqjEGIJ+Im+oDOz7QBuAvA8gA3u3gvMviEAmHOxr5ntN7MuM+saHh5e5HSFEAtl3mY3s2YAXwXwcXcPd6V7E+5+wN073b1z9erVC5mjEGIJmJfZzSyLWaN/0d2/Vrm738w6KnoHgIHlmaIQYilIDL2ZmQH4HIDj7v6XV0hPAngQwKOV2yfmtUcSPkNC+CtNxronxN4S6g57wvtekWSxlkq877GRcMj84Cm0hanpoHYxof1vXUOO6vUNPPzV0MBTXH946nRQGxnhc9t81VaqZzIJL19S5vpiH0+PTSWUFueFqgFLqIteLIRfM42NvMz15EQ4nFomrcXnE2e/HcADAI6Y2eHKfQ9j1uRfMbOPAjgH4APz2JYQokYkmt3dv4Nw+YV3L+10hBDLhZbLChEJMrsQkSCzCxEJMrsQkSCzCxEJVU1xLc6U0N8XTi1saODxxfq6uqCWJmWmAR6jB4BsKiFuWg7HTb3IU1iLdbzlconERgGg5Hw8KzXdvIq3i161hicrTieUc+7u7aH6TCp83KYSylRf6Odpoq0tq6jOYuENjXz9QLYuwRqp8GsR4CnRAJDPTwW1xga+7f6+8DFnryVd2YWIBJldiEiQ2YWIBJldiEiQ2YWIBJldiEiQ2YWIBEtsN7yEtLa2+6233hXUN2zYQMdv23pVULtqC8993raV61d1bKZ6e2tbUMsm5FWnE2Kuk2OXqF5PWg8DvKzxzEw41x0AxkhuNAA88QQvU/CpT32Kb9/Dc0s6Lkn56qtaebnngYFwPZUtHRvp2GyW11aYvszz+OvImhAASNPLLF/zcezYkaBWQgHucxfZ1pVdiEiQ2YWIBJldiEiQ2YWIBJldiEiQ2YWIBJldiEioapy9qbnVr7v+tqA+OjpKx48MDQa1UpHnfK9bs5bqN+zZQ/W33bg3qF29PRz/B4CNG3lMty7La7dPTPAGPK+cOB7UXnrpJTr2xIkTVH/9/Dmqj4zx2u/Z1eHnXi7zeHI6WNR4FpbHD4DWjQepTwAAWR4IRy7Hz1kuw9cQtDSH8+mbE2r1sw7fh4++jInJccXZhYgZmV2ISJDZhYgEmV2ISJDZhYgEmV2ISJDZhYiE+fRn3wrgCwA2YjbR9oC7f9rMHgHwGwDeCH4/7O5PsW1lM1msJznr9Ql141n+M4vBA0D/YD/V//MZPv75558Lag11PPc5Kc6ey/DxhQKv3T5yaTSojY3xGH2pxOPNntBbvqmR16UvZcPXExYGB5AYCy8n5H2jHK6hXppJqOWfsGl3Xuu/XEroU5AJ60QCwNcXuIcnPp8mEUUAv+fuL5lZC4AXzexgRfsrd//zeWxDCFFj5tOfvRdAb+X3cTM7DoCXdRFCrDh+ov/ZzWw7gJsAPF+56yEz+4GZPWZmc/YRMrP9ZtZlZl1JH0eFEMvHvM1uZs0Avgrg4+4+BuAzAHYC2IvZK/9fzDXO3Q+4e6e7d+ZyvJaaEGL5mJfZzSyLWaN/0d2/BgDu3u/uJZ/9RuCzAG5ZvmkKIRZLotnNzAB8DsBxd//LK+7vuOJh7wdwdOmnJ4RYKubzbfztAB4AcMTMDlfuexjA/Wa2F4ADOAvgY4k7y2WxvmNTUG9LSENlpaYnxnk55qmEEFTS+MuTE0GtmFCu+cSJV6jOwiUAUCjw1sZMzyaUoW5tbaV6U0Jr45mZGaqPsvAX0QAgKf3aErKzS6Xw3MqlhDbYc1dj/j894ZyVinx8Nh22XlLmboY8oEzClfP5Nv47wJyJxTSmLoRYWWgFnRCRILMLEQkyuxCRILMLEQkyuxCRILMLEQnzibMvGZs6NuGTjzwS1JPKQU9Ph9vk5id56+F8Qhw9Kc6eZ3H2Ao+zTyXMLamkcj7P2wMzPZXm6bNJJZGLCedkfHKK6ut3bA9qSc87ibTxaxWN0yfsO8PqNQOoS0hLTjputJR0YxMdmyZpx7/50P6gpiu7EJEgswsRCTK7EJEgswsRCTK7EJEgswsRCTK7EJFQ1ZbNZjYI4PUr7loLYKhqE/jJWKlzW6nzAjS3hbKUc9vm7uvmEqpq9h/buVmXu3fWbAKElTq3lTovQHNbKNWamz7GCxEJMrsQkVBrsx+o8f4ZK3VuK3VegOa2UKoyt5r+zy6EqB61vrILIaqEzC5EJNTE7GZ2j5n90MxOmtknajGHEGZ21syOmNlhM+uq8VweM7MBMzt6xX2rzeygmb1WuZ2zx16N5vaImV2oHLvDZnZvjea21cy+bWbHzeyYmf1O5f6aHjsyr6oct6r/z25maQCvAvg5AN0ADgG43915J4UqYWZnAXS6e80XYJjZnQAmAHzB3a+r3PdnAIbd/dHKG2W7u//BCpnbIwAmat3Gu9KtqOPKNuMA3gfgI6jhsSPz+mVU4bjV4sp+C4CT7n7a3QsAvgzgvhrMY8Xj7s8AGH7T3fcBeLzy++OYfbFUncDcVgTu3uvuL1V+HwfwRpvxmh47Mq+qUAuzbwZw/oq/u7Gy+r07gG+Z2YtmFq7xUzs2uHsvMPviAbC+xvN5M4ltvKvJm9qMr5hjt5D254ulFmafq4DWSor/3e7ubwPwHgC/Xfm4KubHvNp4V4s52oyvCBba/nyx1MLs3QC2XvH3FgA9NZjHnLh7T+V2AMDXsfJaUfe/0UG3cjtQ4/n8LyupjfdcbcaxAo5dLduf18LshwDsMrOrzSwH4IMAnqzBPH4MM2uqfHECM2sCcDdWXivqJwE8WPn9QQBP1HAuP8JKaeMdajOOGh+7mrc/d/eq/wC4F7PfyJ8C8Ie1mENgXjsAfL/yc6zWcwPwJcx+rJvB7CeijwJYA+BpAK9VblevoLn9I4AjAH6AWWN11Ghud2D2X8MfADhc+bm31seOzKsqx03LZYWIBK2gEyISZHYhIkFmFyISZHYhIkFmFyISZHYhIkFmFyIS/gdzPUZPXc1SnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/workspace/aiffel-exploation-workspace/E1_rock_scissor_paper/data\"\n",
    "(x, y)=load_data(image_dir_path, 8913)\n",
    "\n",
    "# 데이터가 잘 불러와 졌는지 확인\n",
    "plt.imshow(x[0])\n",
    "print('라벨: ', y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 데이터 전처리\n",
    "* 정규화 : 현재 이미지 데이터가 0\\~255 사이의 값을 가지므로 이를 0\\~1 사이의 값을 가지도록 정규화 시켜준다.\n",
    "* reshape : 이후 단계에서 만들 네트워크의 입력은 *( 데이터의 갯수, 이미지 크기 x, 이미지 크기 y, 채널수)*의 형태를 가지므로 입력 데이터의 형태를 변환 시켜준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Normalization - min : 0.0, max : 1.0\n",
      "After Reshape - x_reshaped shape : (8913, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "x_norm = x / 255.0\n",
    "x_reshaped = x_norm.reshape( -1, 28, 28, 3)\n",
    "\n",
    "print(\"After Normalization - min : {}, max : {}\".format(np.min(x_reshaped), np.max(x_reshaped)))\n",
    "print(\"After Reshape - x_reshaped shape : {}\".format(x_reshaped.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 데이터 분할\n",
    "데이터를 train set, validation set, test set으로 분할한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7130, 28, 28, 3)\n",
      "x_test shape: (891, 28, 28, 3)\n",
      "x_validation shape: (892, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "random_state = np.random.RandomState(seed=545)\n",
    "\n",
    "# 데이터를 랜덤하게 섞고, 학습 데이터와 테스트 데이터를 8:2 비율로 분할한다.\n",
    "# 테스트 데이터를 다시 절반으로 나누어 검증 데이터로 삼는다.\n",
    "# ==> train set : validation set : test set = 8 : 1 : 1\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x_reshaped, y, test_size=0.2, random_state=random_state)\n",
    "(x_test, x_validation, y_test, y_validation) = train_test_split(x_test, y_test, test_size=0.5, random_state=random_state)\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"x_validation shape: {}\".format(x_validation.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train set 확인\n",
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test set 확인\n",
    "plt.imshow(x_test[0])\n",
    "print('라벨: ', y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# validation set 확인\n",
    "plt.imshow(x_validation[0])\n",
    "print('라벨: ', y_validation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 네트워크 설계\n",
    "2개의 Conva2D 레이어를 가지는 type A 모델과 3개의 Conva2D 레이어를 가지는 type B 모델 두 가지를 설계하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modal_type_A(n_channel_1, n_channel_2, n_dense):\n",
    "    # print(n_channel_1, n_channel_2, n_dense)\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)),\n",
    "        keras.layers.MaxPool2D(2,2),\n",
    "        keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'),\n",
    "        keras.layers.MaxPool2D(2,2),\n",
    "        keras.layers.Flatten(), # 2D -> 1D\n",
    "        keras.layers.Dense(n_dense, activation='relu'),\n",
    "        keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modal_type_B(n_channel_1, n_channel_2, n_channel_3, n_dense):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)),\n",
    "        keras.layers.MaxPool2D(2,2),\n",
    "        keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'),\n",
    "        keras.layers.MaxPool2D(2,2),\n",
    "        keras.layers.Conv2D(n_channel_3, (3,3), activation='relu'),\n",
    "        keras.layers.MaxPool2D(2,2),\n",
    "        keras.layers.Flatten(), # 2D -> 1D\n",
    "        keras.layers.Dense(n_dense, activation='relu'),\n",
    "        keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습\n",
    "다양한 하이퍼 파라미터를 적용하여 학습시키고 validation set을 이용하여 이를 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x, y, repeat=10):\n",
    "    model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x, y, epochs=repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, x, y):\n",
    "    test_loss, test_accuracy = model.evaluate(x, y, verbose=2)\n",
    "    print(\"====> test_loss: {} \".format(test_loss))\n",
    "    print(\"====> test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16 16\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                6416      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 9,235\n",
      "Trainable params: 9,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 3s 14ms/step - loss: 1.0863 - accuracy: 0.3899\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.9419 - accuracy: 0.5658\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.7589 - accuracy: 0.6777\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.6460 - accuracy: 0.7342\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.5561 - accuracy: 0.7780\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.4995 - accuracy: 0.8056\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.8289\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.3938 - accuracy: 0.8464\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.3727 - accuracy: 0.8555\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.3237 - accuracy: 0.8764\n",
      "28/28 - 1s - loss: 0.3752 - accuracy: 0.8408\n",
      "====> test_loss: 0.3751842677593231 \n",
      "====> test_accuracy: 0.8408071994781494\n",
      "16 32 32\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,819\n",
      "Trainable params: 30,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 2s 10ms/step - loss: 1.0032 - accuracy: 0.4809\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.7223 - accuracy: 0.6948\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.5567 - accuracy: 0.7795\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.4478 - accuracy: 0.8295\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.3776 - accuracy: 0.8530\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.8917\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.2654 - accuracy: 0.9013\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.2238 - accuracy: 0.9222\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9355\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.1685 - accuracy: 0.9401\n",
      "28/28 - 1s - loss: 0.1980 - accuracy: 0.9249\n",
      "====> test_loss: 0.1979599893093109 \n",
      "====> test_accuracy: 0.9248878955841064\n",
      "32 32 32\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 35,875\n",
      "Trainable params: 35,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 4s 19ms/step - loss: 1.0248 - accuracy: 0.4558\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.7615 - accuracy: 0.6637\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.5530 - accuracy: 0.7767\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.4259 - accuracy: 0.8383\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8811\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9173\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2140 - accuracy: 0.9268\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.9376\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9539\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9673\n",
      "28/28 - 2s - loss: 0.2099 - accuracy: 0.9193\n",
      "====> test_loss: 0.2098749727010727 \n",
      "====> test_accuracy: 0.9192824959754944\n",
      "32 64 64\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 122,051\n",
      "Trainable params: 122,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 3s 12ms/step - loss: 0.9527 - accuracy: 0.5342\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.6141 - accuracy: 0.7450\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.8321\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3151 - accuracy: 0.8850\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2269 - accuracy: 0.9229\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1698 - accuracy: 0.9404\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9544\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9693\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0867 - accuracy: 0.9704\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0694 - accuracy: 0.9773\n",
      "28/28 - 1s - loss: 0.1572 - accuracy: 0.9473\n",
      "====> test_loss: 0.15715086460113525 \n",
      "====> test_accuracy: 0.9473094344139099\n",
      "64 64 64\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 26, 26, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 141,379\n",
      "Trainable params: 141,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 4s 18ms/step - loss: 0.9574 - accuracy: 0.5101\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.6210 - accuracy: 0.7482\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8394\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8784\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2215 - accuracy: 0.9227\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1677 - accuracy: 0.9407\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9576\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0884 - accuracy: 0.9715\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9798\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0522 - accuracy: 0.9846\n",
      "28/28 - 2s - loss: 0.1582 - accuracy: 0.9563\n",
      "====> test_loss: 0.15823042392730713 \n",
      "====> test_accuracy: 0.9562780261039734\n",
      "64 128 128\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 26, 26, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 11, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               409728    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 485,763\n",
      "Trainable params: 485,763\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 3s 12ms/step - loss: 0.9621 - accuracy: 0.5041\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.7621\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8623\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2364 - accuracy: 0.9161\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1575 - accuracy: 0.9414\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0984 - accuracy: 0.9680\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1031 - accuracy: 0.9686\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0629 - accuracy: 0.9805\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9856\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0241 - accuracy: 0.9938\n",
      "28/28 - 1s - loss: 0.0510 - accuracy: 0.9854\n",
      "====> test_loss: 0.050988197326660156 \n",
      "====> test_accuracy: 0.9854260087013245\n"
     ]
    }
   ],
   "source": [
    "hyper_params_A = [(16, 16, 16), (16, 32, 32), (32, 32, 32), (32, 64, 64), (64, 64, 64), (64, 128, 128)]\n",
    "for parm in hyper_params_A:\n",
    "    model_type_A = get_modal_type_A(*parm)\n",
    "    train_model(model_type_A, x_train, y_train)\n",
    "    test_model(model_type_A, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 11, 11, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 3, 3, 16)          2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 1, 1, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 5,411\n",
      "Trainable params: 5,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 3s 12ms/step - loss: 1.0811 - accuracy: 0.4031\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.9578 - accuracy: 0.5264\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.8508 - accuracy: 0.5947\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.7740 - accuracy: 0.6478\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.7070 - accuracy: 0.6926\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.6401 - accuracy: 0.7264\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.7626\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7776\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.8043\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.8170\n",
      "28/28 - 1s - loss: 0.4837 - accuracy: 0.7859\n",
      "====> test_loss: 0.483741819858551 \n",
      "====> test_accuracy: 0.7858744263648987\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 3, 3, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 15,491\n",
      "Trainable params: 15,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 3s 14ms/step - loss: 1.0706 - accuracy: 0.4094\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.8185 - accuracy: 0.6271\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.6029 - accuracy: 0.7506\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.8083\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8504\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8773\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.8983\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.9021\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2077 - accuracy: 0.9236\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.9314\n",
      "28/28 - 1s - loss: 0.2853 - accuracy: 0.8957\n",
      "====> test_loss: 0.28529027104377747 \n",
      "====> test_accuracy: 0.8957399129867554\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 27,939\n",
      "Trainable params: 27,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 3s 13ms/step - loss: 1.0672 - accuracy: 0.4072\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.8057 - accuracy: 0.6442\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.5746 - accuracy: 0.7645\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.4344 - accuracy: 0.8328\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3456 - accuracy: 0.8675\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2817 - accuracy: 0.8920\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.9133\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1848 - accuracy: 0.9325\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1581 - accuracy: 0.9439\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1327 - accuracy: 0.9485\n",
      "28/28 - 1s - loss: 0.2762 - accuracy: 0.9058\n",
      "====> test_loss: 0.2761583626270294 \n",
      "====> test_accuracy: 0.9058296084403992\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 3, 3, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 20,547\n",
      "Trainable params: 20,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 1.0877 - accuracy: 0.3777\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.8499 - accuracy: 0.6029\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.5916 - accuracy: 0.7526\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.8111\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8480\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8752\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.8959\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.9097\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2097 - accuracy: 0.9231\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9313\n",
      "28/28 - 0s - loss: 0.2724 - accuracy: 0.9126\n",
      "====> test_loss: 0.2723560631275177 \n",
      "====> test_accuracy: 0.9125560522079468\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 32,995\n",
      "Trainable params: 32,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 1.0615 - accuracy: 0.4112\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 1ms/step - loss: 0.7789 - accuracy: 0.6460\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7900\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8614\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2865 - accuracy: 0.8919\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2248 - accuracy: 0.9181\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1764 - accuracy: 0.9356\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1561 - accuracy: 0.9432\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1062 - accuracy: 0.9638\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0890 - accuracy: 0.9687\n",
      "28/28 - 0s - loss: 0.1781 - accuracy: 0.9417\n",
      "====> test_loss: 0.17806918919086456 \n",
      "====> test_accuracy: 0.9417040348052979\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 60,675\n",
      "Trainable params: 60,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 3s 16ms/step - loss: 1.0531 - accuracy: 0.4245\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.7713 - accuracy: 0.6505\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.5311 - accuracy: 0.7756\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8495\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.8816\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2324 - accuracy: 0.9143\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1874 - accuracy: 0.9321\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9425\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1087 - accuracy: 0.9652\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0940 - accuracy: 0.9689\n",
      "28/28 - 1s - loss: 0.1600 - accuracy: 0.9473\n",
      "====> test_loss: 0.15997973084449768 \n",
      "====> test_accuracy: 0.9473094344139099\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_36 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 110,147\n",
      "Trainable params: 110,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 3s 12ms/step - loss: 1.0494 - accuracy: 0.4230\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.7006 - accuracy: 0.6914\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.8102\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8756\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2291 - accuracy: 0.9147\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1826 - accuracy: 0.9337\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1279 - accuracy: 0.9534\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9638\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1169 - accuracy: 0.9603\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0697 - accuracy: 0.9767\n",
      "28/28 - 1s - loss: 0.1423 - accuracy: 0.9574\n",
      "====> test_loss: 0.142295241355896 \n",
      "====> test_accuracy: 0.9573991298675537\n"
     ]
    }
   ],
   "source": [
    "hyper_params_B = [(16, 16, 16, 16), (16, 32, 32, 32), (16, 32, 64, 64), (32, 32, 32, 32), (32, 32, 64, 64),\n",
    "                  (32, 64, 64, 64), (32, 64, 128, 128)]\n",
    "for parm in hyper_params_B:\n",
    "    model_type_B = get_modal_type_B(*parm)\n",
    "    train_model(model_type_B, x_train, y_train)\n",
    "    test_model(model_type_B, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하이퍼 파라미터 변화에 따른 model type A의 accuracy 변화\n",
    "\n",
    "| 회차 | n_channel_1 | n_channel_2 | n_dense | validation accuracy |\n",
    "|:----:|:-----------:|:-----------:|:-------:|:-------------------:|\n",
    "| 1    | 16          | 16          | 16      | 0.8408071994781494  |\n",
    "| 2    | 16          | 32          | 32      | 0.9248878955841064  |\n",
    "| 3    | 32          | 32          | 32      | 0.9192824959754944  |\n",
    "| 4    | 32          | 64          | 64      | 0.9473094344139099  |\n",
    "| 5    | 64          | 64          | 64      | 0.9562780261039734  |\n",
    "| 6    | 64          | 128         | 128     | 0.9854260087013245  |\n",
    "\n",
    "<br />\n",
    "\n",
    "* 하이퍼 파라미터 변화에 따른 model type B의 accuracy 변화\n",
    "\n",
    "| 회차 | n_channel_1 | n_channel_2 | n_channel_3 | n_dense | validation accuracy |\n",
    "|:----:|:-----------:|:-----------:|:-----------:|:-------:|:-------------------:|\n",
    "| 1    | 16          | 16          | 16          | 16      | 0.7858744263648987  |\n",
    "| 2    | 16          | 32          | 32          | 32      | 0.8957399129867554  |\n",
    "| 3    | 16          | 32          | 64          | 64      | 0.9058296084403992  |\n",
    "| 4    | 32          | 32          | 32          | 32      | 0.9125560522079468  |\n",
    "| 5    | 32          | 32          | 64          | 64      | 0.9417040348052979  |\n",
    "| 6    | 32          | 64          | 64          | 64      | 0.9473094344139099  |\n",
    "| 7    | 32          | 64          | 128         | 128     | 0.9573991298675537  |\n",
    "\n",
    "<br />\n",
    "\n",
    "가장 accuracy가 높은 6회차 하이퍼 파라미터를 가지는 type A 모델을 선정하여 다음 테스트를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 128 128\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_41 (Conv2D)           (None, 26, 26, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 11, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 128)               409728    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 485,763\n",
      "Trainable params: 485,763\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.9469 - accuracy: 0.5250\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.7719\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.3745 - accuracy: 0.8607\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.2563 - accuracy: 0.9058\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 1s 2ms/step - loss: 0.1786 - accuracy: 0.9351\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9548\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 1s 2ms/step - loss: 0.0894 - accuracy: 0.9722\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0629 - accuracy: 0.9798\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 0.9853\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 0.9909\n"
     ]
    }
   ],
   "source": [
    "final_model = get_modal_type_A(64, 128, 128)\n",
    "train_model(final_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 - 0s - loss: 0.0779 - accuracy: 0.9787\n",
      "====> test_loss: 0.07785475999116898 \n",
      "====> test_accuracy: 0.9786756634712219\n"
     ]
    }
   ],
   "source": [
    "test_model(final_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model_type_B, x_test_reshaped, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 프로젝트의 결과를 가장 크게 좌우한 것은 <U>데이터의 양</U>이었다. 300개의 데이터를 가지고 했던 초기단계에서는 accuracy가 0.4 근처를 맴돌았고, 이는 하이퍼 파라미터를 아무리 조절하여도 개선이 되지 않았다. 하지만 이후 다량의 데이터를 수집하여 프로젝트에 투입하였고 0.97이라는 놀라운 수치에 도달 할 수 있었다. 개인적으로 느끼기에는 accuracy가 너무 갑자기 급등하여 데이터양에 따른 accuracy 변화를 좀 더 자세히 살펴보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실험 내용\n",
    "* train data set의 양에 따른 test accuracy의 변화를 관찰한다.\n",
    "* `get_modal_type_A(64, 128, 128)`를 통해 얻어지는 동일한 모델을 이용하여 실험을 진행한다.\n",
    "* 실험 과정\n",
    "    1. 새로운 모델을 생성한다.\n",
    "    2. N개의 train data를 이용하여 1에서 생성한 모델을 학습시킨다. N는 10에서 시작하여 회차를 거듭할 때마다 10개 씩 증가한다.\n",
    "    3. N이 7130을 초과하게 되면 실험을 종료한다. 그렇지 않다면 1로 돌아가 실험을 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 train complete -- accruacy : 0.8999999761581421, loss : 0.40092024207115173\n",
      "10 test complete == accruacy : 0.3400673270225525, loss : 1.9005327224731445\n",
      "20 train complete -- accruacy : 0.699999988079071, loss : 0.6829968094825745\n",
      "20 test complete == accruacy : 0.34455668926239014, loss : 1.3223410844802856\n",
      "30 train complete -- accruacy : 0.6666666865348816, loss : 0.8006798028945923\n",
      "30 test complete == accruacy : 0.3411896824836731, loss : 1.2011361122131348\n",
      "40 train complete -- accruacy : 0.675000011920929, loss : 0.816357433795929\n",
      "40 test complete == accruacy : 0.3490460216999054, loss : 1.1658681631088257\n",
      "50 train complete -- accruacy : 0.6200000047683716, loss : 0.7232769131660461\n",
      "50 test complete == accruacy : 0.36139169335365295, loss : 1.2296626567840576\n",
      "60 train complete -- accruacy : 0.7333333492279053, loss : 0.7974259853363037\n",
      "60 test complete == accruacy : 0.3961840569972992, loss : 1.1196043491363525\n",
      "70 train complete -- accruacy : 0.5428571701049805, loss : 0.8134365677833557\n",
      "70 test complete == accruacy : 0.33782267570495605, loss : 1.3269211053848267\n",
      "80 train complete -- accruacy : 0.7124999761581421, loss : 0.9102303385734558\n",
      "80 test complete == accruacy : 0.36251401901245117, loss : 1.1162090301513672\n",
      "90 train complete -- accruacy : 0.6222222447395325, loss : 0.8471338748931885\n",
      "90 test complete == accruacy : 0.4029180705547333, loss : 1.0967613458633423\n",
      "100 train complete -- accruacy : 0.44999998807907104, loss : 0.9680219888687134\n",
      "100 test complete == accruacy : 0.4040403962135315, loss : 1.070940375328064\n",
      "110 train complete -- accruacy : 0.5909090638160706, loss : 0.8894444108009338\n",
      "110 test complete == accruacy : 0.4377104341983795, loss : 1.081910252571106\n",
      "120 train complete -- accruacy : 0.6083333492279053, loss : 0.8175685405731201\n",
      "120 test complete == accruacy : 0.4186307489871979, loss : 1.0740487575531006\n",
      "130 train complete -- accruacy : 0.5538461804389954, loss : 0.9955875277519226\n",
      "130 test complete == accruacy : 0.4141414165496826, loss : 1.0699876546859741\n",
      "140 train complete -- accruacy : 0.6214285492897034, loss : 0.847273051738739\n",
      "140 test complete == accruacy : 0.3973063826560974, loss : 1.1522376537322998\n",
      "150 train complete -- accruacy : 0.5666666626930237, loss : 0.8472023606300354\n",
      "150 test complete == accruacy : 0.4601571261882782, loss : 1.0699245929718018\n",
      "160 train complete -- accruacy : 0.7250000238418579, loss : 0.7093462944030762\n",
      "160 test complete == accruacy : 0.4601571261882782, loss : 1.0806041955947876\n",
      "170 train complete -- accruacy : 0.5647059082984924, loss : 0.8949247598648071\n",
      "170 test complete == accruacy : 0.4680134654045105, loss : 1.061231017112732\n",
      "180 train complete -- accruacy : 0.6000000238418579, loss : 0.9019058346748352\n",
      "180 test complete == accruacy : 0.4837261438369751, loss : 1.0097222328186035\n",
      "190 train complete -- accruacy : 0.699999988079071, loss : 0.7564027309417725\n",
      "190 test complete == accruacy : 0.5196408629417419, loss : 0.9787316918373108\n",
      "200 train complete -- accruacy : 0.6150000095367432, loss : 0.8403534889221191\n",
      "200 test complete == accruacy : 0.5005611777305603, loss : 1.0053935050964355\n",
      "210 train complete -- accruacy : 0.738095223903656, loss : 0.657791018486023\n",
      "210 test complete == accruacy : 0.5084174871444702, loss : 1.0081216096878052\n",
      "220 train complete -- accruacy : 0.5545454621315002, loss : 0.9680898785591125\n",
      "220 test complete == accruacy : 0.33670035004615784, loss : 1.0935635566711426\n",
      "230 train complete -- accruacy : 0.5260869860649109, loss : 0.9232958555221558\n",
      "230 test complete == accruacy : 0.4938271641731262, loss : 0.9942416548728943\n",
      "240 train complete -- accruacy : 0.5458333492279053, loss : 0.9129777550697327\n",
      "240 test complete == accruacy : 0.47362515330314636, loss : 1.0293610095977783\n",
      "250 train complete -- accruacy : 0.6679999828338623, loss : 0.7303928732872009\n",
      "250 test complete == accruacy : 0.4837261438369751, loss : 1.066119909286499\n",
      "260 train complete -- accruacy : 0.6576923131942749, loss : 0.7913516163825989\n",
      "260 test complete == accruacy : 0.5072951912879944, loss : 1.0018949508666992\n",
      "270 train complete -- accruacy : 0.5925925970077515, loss : 0.8319422006607056\n",
      "270 test complete == accruacy : 0.5151515007019043, loss : 0.9598385095596313\n",
      "280 train complete -- accruacy : 0.6892856955528259, loss : 0.7278106808662415\n",
      "280 test complete == accruacy : 0.5140292048454285, loss : 1.0071558952331543\n",
      "290 train complete -- accruacy : 0.6034482717514038, loss : 0.8116198182106018\n",
      "290 test complete == accruacy : 0.4848484992980957, loss : 0.9931045174598694\n",
      "300 train complete -- accruacy : 0.70333331823349, loss : 0.714047372341156\n",
      "300 test complete == accruacy : 0.5454545617103577, loss : 0.9575055837631226\n",
      "310 train complete -- accruacy : 0.6193548440933228, loss : 0.8257667422294617\n",
      "310 test complete == accruacy : 0.4983164966106415, loss : 1.0146082639694214\n",
      "320 train complete -- accruacy : 0.6499999761581421, loss : 0.7521199584007263\n",
      "320 test complete == accruacy : 0.5622895359992981, loss : 0.9051458835601807\n",
      "330 train complete -- accruacy : 0.5484848618507385, loss : 0.8846851587295532\n",
      "330 test complete == accruacy : 0.5241301655769348, loss : 0.9488740563392639\n",
      "340 train complete -- accruacy : 0.7088235020637512, loss : 0.7058764696121216\n",
      "340 test complete == accruacy : 0.558922529220581, loss : 0.9237052798271179\n",
      "350 train complete -- accruacy : 0.7371428608894348, loss : 0.6669832468032837\n",
      "350 test complete == accruacy : 0.6184062957763672, loss : 0.8902543783187866\n",
      "360 train complete -- accruacy : 0.7472222447395325, loss : 0.6702632904052734\n",
      "360 test complete == accruacy : 0.583613932132721, loss : 0.8581612706184387\n",
      "370 train complete -- accruacy : 0.6810810565948486, loss : 0.7331358790397644\n",
      "370 test complete == accruacy : 0.5634118914604187, loss : 0.8809095025062561\n",
      "380 train complete -- accruacy : 0.74210524559021, loss : 0.6478698253631592\n",
      "380 test complete == accruacy : 0.590347945690155, loss : 0.8724089860916138\n",
      "390 train complete -- accruacy : 0.656410276889801, loss : 0.7684095501899719\n",
      "390 test complete == accruacy : 0.492704838514328, loss : 1.0056699514389038\n",
      "400 train complete -- accruacy : 0.7099999785423279, loss : 0.691851794719696\n",
      "400 test complete == accruacy : 0.5701459050178528, loss : 0.9496103525161743\n",
      "410 train complete -- accruacy : 0.7731707096099854, loss : 0.5690048933029175\n",
      "410 test complete == accruacy : 0.5342311859130859, loss : 0.9744839072227478\n",
      "420 train complete -- accruacy : 0.7166666388511658, loss : 0.6605346202850342\n",
      "420 test complete == accruacy : 0.47250279784202576, loss : 1.2555030584335327\n",
      "430 train complete -- accruacy : 0.7790697813034058, loss : 0.5773530006408691\n",
      "430 test complete == accruacy : 0.6184062957763672, loss : 0.8686610460281372\n",
      "440 train complete -- accruacy : 0.7136363387107849, loss : 0.7214764952659607\n",
      "440 test complete == accruacy : 0.5364758968353271, loss : 0.9535712003707886\n",
      "450 train complete -- accruacy : 0.7244444489479065, loss : 0.6616638898849487\n",
      "450 test complete == accruacy : 0.6184062957763672, loss : 0.8535141944885254\n",
      "460 train complete -- accruacy : 0.710869550704956, loss : 0.6999382376670837\n",
      "460 test complete == accruacy : 0.5634118914604187, loss : 0.9019266963005066\n",
      "470 train complete -- accruacy : 0.806382954120636, loss : 0.5354984998703003\n",
      "470 test complete == accruacy : 0.6464646458625793, loss : 0.8102288842201233\n",
      "480 train complete -- accruacy : 0.7333333492279053, loss : 0.6379996538162231\n",
      "480 test complete == accruacy : 0.6464646458625793, loss : 0.7838960886001587\n",
      "490 train complete -- accruacy : 0.7367346882820129, loss : 0.646234929561615\n",
      "490 test complete == accruacy : 0.6071829199790955, loss : 0.8264312148094177\n",
      "500 train complete -- accruacy : 0.7919999957084656, loss : 0.5488540530204773\n",
      "500 test complete == accruacy : 0.6464646458625793, loss : 0.8460811376571655\n",
      "510 train complete -- accruacy : 0.8039215803146362, loss : 0.5275548696517944\n",
      "510 test complete == accruacy : 0.6599326729774475, loss : 0.7842543125152588\n",
      "520 train complete -- accruacy : 0.7211538553237915, loss : 0.6729695200920105\n",
      "520 test complete == accruacy : 0.6094276309013367, loss : 0.8277537822723389\n",
      "530 train complete -- accruacy : 0.7641509175300598, loss : 0.6034013032913208\n",
      "530 test complete == accruacy : 0.6386082768440247, loss : 0.825481653213501\n",
      "540 train complete -- accruacy : 0.8055555820465088, loss : 0.5011401772499084\n",
      "540 test complete == accruacy : 0.6442199945449829, loss : 0.8475095629692078\n",
      "550 train complete -- accruacy : 0.7636363506317139, loss : 0.6066175103187561\n",
      "550 test complete == accruacy : 0.6408529877662659, loss : 0.8166470527648926\n",
      "560 train complete -- accruacy : 0.7642857432365417, loss : 0.6206755638122559\n",
      "560 test complete == accruacy : 0.6341189742088318, loss : 0.8500404357910156\n",
      "570 train complete -- accruacy : 0.7263157963752747, loss : 0.6330110430717468\n",
      "570 test complete == accruacy : 0.6722783446311951, loss : 0.7857999205589294\n",
      "580 train complete -- accruacy : 0.6086207032203674, loss : 0.8544011116027832\n",
      "580 test complete == accruacy : 0.5802469253540039, loss : 0.9296215772628784\n",
      "590 train complete -- accruacy : 0.7576271295547485, loss : 0.6318414807319641\n",
      "590 test complete == accruacy : 0.6531986594200134, loss : 0.8022128939628601\n",
      "600 train complete -- accruacy : 0.8116666674613953, loss : 0.5109935402870178\n",
      "600 test complete == accruacy : 0.6520763039588928, loss : 0.8091087937355042\n",
      "610 train complete -- accruacy : 0.742622971534729, loss : 0.6010660529136658\n",
      "610 test complete == accruacy : 0.6217733025550842, loss : 0.8779596090316772\n",
      "620 train complete -- accruacy : 0.800000011920929, loss : 0.4928334951400757\n",
      "620 test complete == accruacy : 0.6262626051902771, loss : 0.810149610042572\n",
      "630 train complete -- accruacy : 0.7523809671401978, loss : 0.6044188737869263\n",
      "630 test complete == accruacy : 0.6520763039588928, loss : 0.826752781867981\n",
      "640 train complete -- accruacy : 0.807812511920929, loss : 0.5321594476699829\n",
      "640 test complete == accruacy : 0.683501660823822, loss : 0.7586105465888977\n",
      "650 train complete -- accruacy : 0.7261538505554199, loss : 0.6985755562782288\n",
      "650 test complete == accruacy : 0.6453422904014587, loss : 0.829020619392395\n",
      "660 train complete -- accruacy : 0.7636363506317139, loss : 0.5799962878227234\n",
      "660 test complete == accruacy : 0.6251403093338013, loss : 0.8040207624435425\n",
      "670 train complete -- accruacy : 0.8044776320457458, loss : 0.4902472198009491\n",
      "670 test complete == accruacy : 0.7003366947174072, loss : 0.7171111106872559\n",
      "680 train complete -- accruacy : 0.7779411673545837, loss : 0.5433308482170105\n",
      "680 test complete == accruacy : 0.6722783446311951, loss : 0.7657715082168579\n",
      "690 train complete -- accruacy : 0.7942029237747192, loss : 0.5368497967720032\n",
      "690 test complete == accruacy : 0.683501660823822, loss : 0.736953616142273\n",
      "700 train complete -- accruacy : 0.7971428632736206, loss : 0.5140751600265503\n",
      "700 test complete == accruacy : 0.6745229959487915, loss : 0.7319278120994568\n",
      "710 train complete -- accruacy : 0.7718309760093689, loss : 0.5726277232170105\n",
      "710 test complete == accruacy : 0.6790123581886292, loss : 0.742705225944519\n",
      "720 train complete -- accruacy : 0.831944465637207, loss : 0.4494587481021881\n",
      "720 test complete == accruacy : 0.6891133785247803, loss : 0.7534485459327698\n",
      "730 train complete -- accruacy : 0.8205479383468628, loss : 0.5075801610946655\n",
      "730 test complete == accruacy : 0.6285073161125183, loss : 0.8640962243080139\n",
      "740 train complete -- accruacy : 0.8067567348480225, loss : 0.5189835429191589\n",
      "740 test complete == accruacy : 0.6902356743812561, loss : 0.7180652022361755\n",
      "750 train complete -- accruacy : 0.8040000200271606, loss : 0.5153634548187256\n",
      "750 test complete == accruacy : 0.680134654045105, loss : 0.7695776224136353\n",
      "760 train complete -- accruacy : 0.7723684310913086, loss : 0.5891069769859314\n",
      "760 test complete == accruacy : 0.6868686676025391, loss : 0.7284959554672241\n",
      "770 train complete -- accruacy : 0.7727272510528564, loss : 0.5535265803337097\n",
      "770 test complete == accruacy : 0.6217733025550842, loss : 0.917843759059906\n",
      "780 train complete -- accruacy : 0.8038461804389954, loss : 0.4500182271003723\n",
      "780 test complete == accruacy : 0.7014590501785278, loss : 0.7234994173049927\n",
      "790 train complete -- accruacy : 0.8594936728477478, loss : 0.3827541172504425\n",
      "790 test complete == accruacy : 0.7194163799285889, loss : 0.7033588886260986\n",
      "800 train complete -- accruacy : 0.7887499928474426, loss : 0.5046429634094238\n",
      "800 test complete == accruacy : 0.6846240162849426, loss : 0.760280191898346\n",
      "810 train complete -- accruacy : 0.8679012060165405, loss : 0.37045130133628845\n",
      "810 test complete == accruacy : 0.711560070514679, loss : 0.7334559559822083\n",
      "820 train complete -- accruacy : 0.7780487537384033, loss : 0.5532450079917908\n",
      "820 test complete == accruacy : 0.6666666865348816, loss : 0.7691857218742371\n",
      "830 train complete -- accruacy : 0.83253014087677, loss : 0.43967223167419434\n",
      "830 test complete == accruacy : 0.7373737096786499, loss : 0.6587592363357544\n",
      "840 train complete -- accruacy : 0.8273809552192688, loss : 0.4584139883518219\n",
      "840 test complete == accruacy : 0.7126823663711548, loss : 0.7104418277740479\n",
      "850 train complete -- accruacy : 0.7788235545158386, loss : 0.5615272521972656\n",
      "850 test complete == accruacy : 0.6879910230636597, loss : 0.7397186160087585\n",
      "860 train complete -- accruacy : 0.7697674632072449, loss : 0.5919469594955444\n",
      "860 test complete == accruacy : 0.6778900027275085, loss : 0.7912831902503967\n",
      "870 train complete -- accruacy : 0.8287356495857239, loss : 0.4516552686691284\n",
      "870 test complete == accruacy : 0.744107723236084, loss : 0.6424222588539124\n",
      "880 train complete -- accruacy : 0.7897727489471436, loss : 0.5020761489868164\n",
      "880 test complete == accruacy : 0.7126823663711548, loss : 0.6927339434623718\n",
      "890 train complete -- accruacy : 0.8303371071815491, loss : 0.4278152287006378\n",
      "890 test complete == accruacy : 0.7283950448036194, loss : 0.7230212092399597\n",
      "900 train complete -- accruacy : 0.8155555725097656, loss : 0.5016497373580933\n",
      "900 test complete == accruacy : 0.7362514138221741, loss : 0.6820061802864075\n",
      "910 train complete -- accruacy : 0.8274725079536438, loss : 0.4523392617702484\n",
      "910 test complete == accruacy : 0.7340067625045776, loss : 0.6889964938163757\n",
      "920 train complete -- accruacy : 0.8586956262588501, loss : 0.39018866419792175\n",
      "920 test complete == accruacy : 0.6969696879386902, loss : 0.7767719030380249\n",
      "930 train complete -- accruacy : 0.7290322780609131, loss : 0.63065105676651\n",
      "930 test complete == accruacy : 0.6722783446311951, loss : 0.7668876051902771\n",
      "940 train complete -- accruacy : 0.8382978439331055, loss : 0.43072396516799927\n",
      "940 test complete == accruacy : 0.7216610312461853, loss : 0.6727445721626282\n",
      "950 train complete -- accruacy : 0.8642105460166931, loss : 0.3657050132751465\n",
      "950 test complete == accruacy : 0.7351290583610535, loss : 0.6436492800712585\n",
      "960 train complete -- accruacy : 0.8104166388511658, loss : 0.4988647699356079\n",
      "960 test complete == accruacy : 0.732884407043457, loss : 0.6614711880683899\n",
      "970 train complete -- accruacy : 0.8463917374610901, loss : 0.379194974899292\n",
      "970 test complete == accruacy : 0.7429854273796082, loss : 0.6493157148361206\n",
      "980 train complete -- accruacy : 0.8265306353569031, loss : 0.46935904026031494\n",
      "980 test complete == accruacy : 0.7351290583610535, loss : 0.6716738939285278\n",
      "990 train complete -- accruacy : 0.8606060743331909, loss : 0.37087762355804443\n",
      "990 test complete == accruacy : 0.7306397557258606, loss : 0.7145401239395142\n",
      "1000 train complete -- accruacy : 0.8489999771118164, loss : 0.3849506080150604\n",
      "1000 test complete == accruacy : 0.7429854273796082, loss : 0.6619203686714172\n",
      "1010 train complete -- accruacy : 0.8633663654327393, loss : 0.4171179234981537\n",
      "1010 test complete == accruacy : 0.7519640922546387, loss : 0.6275323629379272\n",
      "1020 train complete -- accruacy : 0.8960784077644348, loss : 0.31129252910614014\n",
      "1020 test complete == accruacy : 0.7665544152259827, loss : 0.6412950754165649\n",
      "1030 train complete -- accruacy : 0.8398058414459229, loss : 0.43948492407798767\n",
      "1030 test complete == accruacy : 0.7620651125907898, loss : 0.6267724633216858\n",
      "1040 train complete -- accruacy : 0.8384615182876587, loss : 0.42065972089767456\n",
      "1040 test complete == accruacy : 0.7654321193695068, loss : 0.6512812376022339\n",
      "1050 train complete -- accruacy : 0.883809506893158, loss : 0.3746284246444702\n",
      "1050 test complete == accruacy : 0.72951740026474, loss : 0.6462594866752625\n",
      "1060 train complete -- accruacy : 0.8075471520423889, loss : 0.47901788353919983\n",
      "1060 test complete == accruacy : 0.7205387353897095, loss : 0.6607427597045898\n",
      "1070 train complete -- accruacy : 0.8934579491615295, loss : 0.31504061818122864\n",
      "1070 test complete == accruacy : 0.804713785648346, loss : 0.6016601324081421\n",
      "1080 train complete -- accruacy : 0.8583333492279053, loss : 0.35784685611724854\n",
      "1080 test complete == accruacy : 0.796857476234436, loss : 0.5622122287750244\n",
      "1090 train complete -- accruacy : 0.7697247862815857, loss : 0.5519003868103027\n",
      "1090 test complete == accruacy : 0.7216610312461853, loss : 0.6894240975379944\n",
      "1100 train complete -- accruacy : 0.8527272939682007, loss : 0.3666522800922394\n",
      "1100 test complete == accruacy : 0.7732884287834167, loss : 0.6255742907524109\n",
      "1110 train complete -- accruacy : 0.8945946097373962, loss : 0.28913238644599915\n",
      "1110 test complete == accruacy : 0.7744107842445374, loss : 0.6170207262039185\n",
      "1120 train complete -- accruacy : 0.8669642806053162, loss : 0.36623290181159973\n",
      "1120 test complete == accruacy : 0.7845118045806885, loss : 0.6217038035392761\n",
      "1130 train complete -- accruacy : 0.8203539848327637, loss : 0.473754346370697\n",
      "1130 test complete == accruacy : 0.7542087435722351, loss : 0.6295814514160156\n",
      "1140 train complete -- accruacy : 0.8921052813529968, loss : 0.30015233159065247\n",
      "1140 test complete == accruacy : 0.7553310990333557, loss : 0.6018173098564148\n",
      "1150 train complete -- accruacy : 0.8895652294158936, loss : 0.3070646822452545\n",
      "1150 test complete == accruacy : 0.8092031478881836, loss : 0.5613430142402649\n",
      "1160 train complete -- accruacy : 0.826724112033844, loss : 0.44753938913345337\n",
      "1160 test complete == accruacy : 0.7239057421684265, loss : 0.6542987823486328\n",
      "1170 train complete -- accruacy : 0.867521345615387, loss : 0.3447778820991516\n",
      "1170 test complete == accruacy : 0.7946127653121948, loss : 0.5722737312316895\n",
      "1180 train complete -- accruacy : 0.8203389644622803, loss : 0.444718599319458\n",
      "1180 test complete == accruacy : 0.744107723236084, loss : 0.6565817594528198\n",
      "1190 train complete -- accruacy : 0.8865545988082886, loss : 0.31554582715034485\n",
      "1190 test complete == accruacy : 0.790123462677002, loss : 0.5786240100860596\n",
      "1200 train complete -- accruacy : 0.8366666436195374, loss : 0.42735555768013\n",
      "1200 test complete == accruacy : 0.72951740026474, loss : 0.6755284070968628\n",
      "1210 train complete -- accruacy : 0.8586776852607727, loss : 0.3783494532108307\n",
      "1210 test complete == accruacy : 0.7732884287834167, loss : 0.5900157690048218\n",
      "1220 train complete -- accruacy : 0.8319672346115112, loss : 0.4492259621620178\n",
      "1220 test complete == accruacy : 0.7777777910232544, loss : 0.5729034543037415\n",
      "1230 train complete -- accruacy : 0.8991869688034058, loss : 0.2856258749961853\n",
      "1230 test complete == accruacy : 0.7912458181381226, loss : 0.6142955422401428\n",
      "1240 train complete -- accruacy : 0.8838709592819214, loss : 0.33274054527282715\n",
      "1240 test complete == accruacy : 0.804713785648346, loss : 0.5626829862594604\n",
      "1250 train complete -- accruacy : 0.8640000224113464, loss : 0.3437272012233734\n",
      "1250 test complete == accruacy : 0.7946127653121948, loss : 0.5244742035865784\n",
      "1260 train complete -- accruacy : 0.8865079283714294, loss : 0.30799371004104614\n",
      "1260 test complete == accruacy : 0.7766554355621338, loss : 0.6192783713340759\n",
      "1270 train complete -- accruacy : 0.8874015808105469, loss : 0.3185751438140869\n",
      "1270 test complete == accruacy : 0.7575757503509521, loss : 0.6058685779571533\n",
      "1280 train complete -- accruacy : 0.8804687261581421, loss : 0.31420812010765076\n",
      "1280 test complete == accruacy : 0.7867564558982849, loss : 0.5612751245498657\n",
      "1290 train complete -- accruacy : 0.8635658621788025, loss : 0.33791831135749817\n",
      "1290 test complete == accruacy : 0.8204264640808105, loss : 0.4929315447807312\n",
      "1300 train complete -- accruacy : 0.8753846287727356, loss : 0.3529645800590515\n",
      "1300 test complete == accruacy : 0.7878788113594055, loss : 0.5666203498840332\n",
      "1310 train complete -- accruacy : 0.8832061290740967, loss : 0.32416027784347534\n",
      "1310 test complete == accruacy : 0.8226711750030518, loss : 0.5095699429512024\n",
      "1320 train complete -- accruacy : 0.8681818246841431, loss : 0.36948347091674805\n",
      "1320 test complete == accruacy : 0.7800224423408508, loss : 0.5517966747283936\n",
      "1330 train complete -- accruacy : 0.8887218236923218, loss : 0.3037444055080414\n",
      "1330 test complete == accruacy : 0.8148148059844971, loss : 0.49508699774742126\n",
      "1340 train complete -- accruacy : 0.904477596282959, loss : 0.2495335340499878\n",
      "1340 test complete == accruacy : 0.7979797720909119, loss : 0.530478298664093\n",
      "1350 train complete -- accruacy : 0.9044444561004639, loss : 0.2689823508262634\n",
      "1350 test complete == accruacy : 0.7586981058120728, loss : 0.6238962411880493\n",
      "1360 train complete -- accruacy : 0.8845587968826294, loss : 0.3276904225349426\n",
      "1360 test complete == accruacy : 0.7497194409370422, loss : 0.6365607976913452\n",
      "1370 train complete -- accruacy : 0.8912408947944641, loss : 0.317304790019989\n",
      "1370 test complete == accruacy : 0.7991021275520325, loss : 0.5150076746940613\n",
      "1380 train complete -- accruacy : 0.8891304135322571, loss : 0.3258366286754608\n",
      "1380 test complete == accruacy : 0.793490469455719, loss : 0.49410784244537354\n",
      "1390 train complete -- accruacy : 0.8705036044120789, loss : 0.35853689908981323\n",
      "1390 test complete == accruacy : 0.8136925101280212, loss : 0.4972991645336151\n",
      "1400 train complete -- accruacy : 0.8492857217788696, loss : 0.3919121325016022\n",
      "1400 test complete == accruacy : 0.8103255033493042, loss : 0.5204774737358093\n",
      "1410 train complete -- accruacy : 0.8588652610778809, loss : 0.38596871495246887\n",
      "1410 test complete == accruacy : 0.7687991261482239, loss : 0.5725433826446533\n",
      "1420 train complete -- accruacy : 0.8852112889289856, loss : 0.3023037910461426\n",
      "1420 test complete == accruacy : 0.8159371614456177, loss : 0.4927588403224945\n",
      "1430 train complete -- accruacy : 0.8328671455383301, loss : 0.4316924810409546\n",
      "1430 test complete == accruacy : 0.7676767706871033, loss : 0.5581546425819397\n",
      "1440 train complete -- accruacy : 0.8916666507720947, loss : 0.2840343415737152\n",
      "1440 test complete == accruacy : 0.8148148059844971, loss : 0.48354029655456543\n",
      "1450 train complete -- accruacy : 0.9006896615028381, loss : 0.26854753494262695\n",
      "1450 test complete == accruacy : 0.804713785648346, loss : 0.4885988235473633\n",
      "1460 train complete -- accruacy : 0.8958904147148132, loss : 0.27868011593818665\n",
      "1460 test complete == accruacy : 0.8383838534355164, loss : 0.4561935365200043\n",
      "1470 train complete -- accruacy : 0.9129251837730408, loss : 0.25711575150489807\n",
      "1470 test complete == accruacy : 0.8125701546669006, loss : 0.47922641038894653\n",
      "1480 train complete -- accruacy : 0.8810811042785645, loss : 0.3132202625274658\n",
      "1480 test complete == accruacy : 0.7811447978019714, loss : 0.536137044429779\n",
      "1490 train complete -- accruacy : 0.9100671410560608, loss : 0.2607143223285675\n",
      "1490 test complete == accruacy : 0.8136925101280212, loss : 0.4610607922077179\n",
      "1500 train complete -- accruacy : 0.8913333415985107, loss : 0.2794516384601593\n",
      "1500 test complete == accruacy : 0.8103255033493042, loss : 0.5045957565307617\n",
      "1510 train complete -- accruacy : 0.865562915802002, loss : 0.35232219099998474\n",
      "1510 test complete == accruacy : 0.7766554355621338, loss : 0.5286200642585754\n",
      "1520 train complete -- accruacy : 0.9131578803062439, loss : 0.26260215044021606\n",
      "1520 test complete == accruacy : 0.8473625183105469, loss : 0.4756283164024353\n",
      "1530 train complete -- accruacy : 0.9281045794487, loss : 0.20094071328639984\n",
      "1530 test complete == accruacy : 0.81144779920578, loss : 0.4932419955730438\n",
      "1540 train complete -- accruacy : 0.8961039185523987, loss : 0.29998886585235596\n",
      "1540 test complete == accruacy : 0.8125701546669006, loss : 0.4710750877857208\n",
      "1550 train complete -- accruacy : 0.8845161199569702, loss : 0.3185044229030609\n",
      "1550 test complete == accruacy : 0.796857476234436, loss : 0.5152884721755981\n",
      "1560 train complete -- accruacy : 0.9269230961799622, loss : 0.22983570396900177\n",
      "1560 test complete == accruacy : 0.8327721953392029, loss : 0.4403640031814575\n",
      "1570 train complete -- accruacy : 0.8955414295196533, loss : 0.28185075521469116\n",
      "1570 test complete == accruacy : 0.8282828330993652, loss : 0.454740434885025\n",
      "1580 train complete -- accruacy : 0.8784810304641724, loss : 0.3334652781486511\n",
      "1580 test complete == accruacy : 0.804713785648346, loss : 0.5026810765266418\n",
      "1590 train complete -- accruacy : 0.9037736058235168, loss : 0.2741667628288269\n",
      "1590 test complete == accruacy : 0.8372614979743958, loss : 0.4374409317970276\n",
      "1600 train complete -- accruacy : 0.8787500262260437, loss : 0.3422173261642456\n",
      "1600 test complete == accruacy : 0.7957351207733154, loss : 0.5364349484443665\n",
      "1610 train complete -- accruacy : 0.8925465941429138, loss : 0.2961100935935974\n",
      "1610 test complete == accruacy : 0.7890011072158813, loss : 0.5559974908828735\n",
      "1620 train complete -- accruacy : 0.9185185432434082, loss : 0.24196267127990723\n",
      "1620 test complete == accruacy : 0.8282828330993652, loss : 0.44786569476127625\n",
      "1630 train complete -- accruacy : 0.904294490814209, loss : 0.25675472617149353\n",
      "1630 test complete == accruacy : 0.8170594573020935, loss : 0.46050316095352173\n",
      "1640 train complete -- accruacy : 0.8914633989334106, loss : 0.317861407995224\n",
      "1640 test complete == accruacy : 0.8226711750030518, loss : 0.48394322395324707\n",
      "1650 train complete -- accruacy : 0.9163636565208435, loss : 0.23118773102760315\n",
      "1650 test complete == accruacy : 0.7979797720909119, loss : 0.4946960508823395\n",
      "1660 train complete -- accruacy : 0.8927710652351379, loss : 0.2901015281677246\n",
      "1660 test complete == accruacy : 0.8136925101280212, loss : 0.46562421321868896\n",
      "1670 train complete -- accruacy : 0.9191616773605347, loss : 0.2449234426021576\n",
      "1670 test complete == accruacy : 0.8395061492919922, loss : 0.4092518985271454\n",
      "1680 train complete -- accruacy : 0.9107142686843872, loss : 0.24531814455986023\n",
      "1680 test complete == accruacy : 0.8316498398780823, loss : 0.4519031345844269\n",
      "1690 train complete -- accruacy : 0.918343186378479, loss : 0.2491244524717331\n",
      "1690 test complete == accruacy : 0.8439955115318298, loss : 0.4345143437385559\n",
      "1700 train complete -- accruacy : 0.8788235187530518, loss : 0.33419740200042725\n",
      "1700 test complete == accruacy : 0.7800224423408508, loss : 0.586887538433075\n",
      "1710 train complete -- accruacy : 0.9005848169326782, loss : 0.27510467171669006\n",
      "1710 test complete == accruacy : 0.8159371614456177, loss : 0.44844862818717957\n",
      "1720 train complete -- accruacy : 0.9133720993995667, loss : 0.23934073746204376\n",
      "1720 test complete == accruacy : 0.8327721953392029, loss : 0.4502684772014618\n",
      "1730 train complete -- accruacy : 0.886705219745636, loss : 0.322211891412735\n",
      "1730 test complete == accruacy : 0.8271604776382446, loss : 0.5052699446678162\n",
      "1740 train complete -- accruacy : 0.9160919785499573, loss : 0.22589679062366486\n",
      "1740 test complete == accruacy : 0.8496071696281433, loss : 0.45320674777030945\n",
      "1750 train complete -- accruacy : 0.9108571410179138, loss : 0.2576771080493927\n",
      "1750 test complete == accruacy : 0.8226711750030518, loss : 0.44251465797424316\n",
      "1760 train complete -- accruacy : 0.9357954263687134, loss : 0.18444667756557465\n",
      "1760 test complete == accruacy : 0.8799102306365967, loss : 0.36647966504096985\n",
      "1770 train complete -- accruacy : 0.8926553726196289, loss : 0.3021424412727356\n",
      "1770 test complete == accruacy : 0.8193041682243347, loss : 0.49464938044548035\n",
      "1780 train complete -- accruacy : 0.9151685237884521, loss : 0.24503347277641296\n",
      "1780 test complete == accruacy : 0.8439955115318298, loss : 0.4097859859466553\n",
      "1790 train complete -- accruacy : 0.9044692516326904, loss : 0.2634676694869995\n",
      "1790 test complete == accruacy : 0.8529741764068604, loss : 0.4206487536430359\n",
      "1800 train complete -- accruacy : 0.9311110973358154, loss : 0.21307136118412018\n",
      "1800 test complete == accruacy : 0.8338944911956787, loss : 0.44094404578208923\n",
      "1810 train complete -- accruacy : 0.8977900743484497, loss : 0.2797527015209198\n",
      "1810 test complete == accruacy : 0.8249158263206482, loss : 0.49106159806251526\n",
      "1820 train complete -- accruacy : 0.9373626112937927, loss : 0.20023910701274872\n",
      "1820 test complete == accruacy : 0.8451178669929504, loss : 0.39329347014427185\n",
      "1830 train complete -- accruacy : 0.9131147265434265, loss : 0.2428724765777588\n",
      "1830 test complete == accruacy : 0.8383838534355164, loss : 0.47499480843544006\n",
      "1840 train complete -- accruacy : 0.914673924446106, loss : 0.25647950172424316\n",
      "1840 test complete == accruacy : 0.8159371614456177, loss : 0.4792386293411255\n",
      "1850 train complete -- accruacy : 0.9254053831100464, loss : 0.21775156259536743\n",
      "1850 test complete == accruacy : 0.8383838534355164, loss : 0.4489932358264923\n",
      "1860 train complete -- accruacy : 0.9220430254936218, loss : 0.21837304532527924\n",
      "1860 test complete == accruacy : 0.8271604776382446, loss : 0.4449732303619385\n",
      "1870 train complete -- accruacy : 0.9315508008003235, loss : 0.20363004505634308\n",
      "1870 test complete == accruacy : 0.8305274844169617, loss : 0.4684983789920807\n",
      "1880 train complete -- accruacy : 0.9244681000709534, loss : 0.1965886354446411\n",
      "1880 test complete == accruacy : 0.8585858345031738, loss : 0.41156402230262756\n",
      "1890 train complete -- accruacy : 0.9296296238899231, loss : 0.2018641233444214\n",
      "1890 test complete == accruacy : 0.8439955115318298, loss : 0.4140012860298157\n",
      "1900 train complete -- accruacy : 0.9031578898429871, loss : 0.2572270631790161\n",
      "1900 test complete == accruacy : 0.8529741764068604, loss : 0.4480679929256439\n",
      "1910 train complete -- accruacy : 0.9151832461357117, loss : 0.2444637566804886\n",
      "1910 test complete == accruacy : 0.854096531867981, loss : 0.43173032999038696\n",
      "1920 train complete -- accruacy : 0.9468749761581421, loss : 0.16373951733112335\n",
      "1920 test complete == accruacy : 0.8597081899642944, loss : 0.43164920806884766\n",
      "1930 train complete -- accruacy : 0.9139896631240845, loss : 0.22963130474090576\n",
      "1930 test complete == accruacy : 0.8529741764068604, loss : 0.4344072639942169\n",
      "1940 train complete -- accruacy : 0.9185566902160645, loss : 0.2288292497396469\n",
      "1940 test complete == accruacy : 0.81144779920578, loss : 0.4764179289340973\n",
      "1950 train complete -- accruacy : 0.9374359250068665, loss : 0.17354418337345123\n",
      "1950 test complete == accruacy : 0.8765432238578796, loss : 0.39483121037483215\n",
      "1960 train complete -- accruacy : 0.9035714268684387, loss : 0.27618226408958435\n",
      "1960 test complete == accruacy : 0.8395061492919922, loss : 0.41422244906425476\n",
      "1970 train complete -- accruacy : 0.9370558261871338, loss : 0.1812218725681305\n",
      "1970 test complete == accruacy : 0.8843995332717896, loss : 0.3939584791660309\n",
      "1980 train complete -- accruacy : 0.9282828569412231, loss : 0.20211860537528992\n",
      "1980 test complete == accruacy : 0.8507295250892639, loss : 0.39962953329086304\n",
      "1990 train complete -- accruacy : 0.9371859431266785, loss : 0.1775931417942047\n",
      "1990 test complete == accruacy : 0.8361391425132751, loss : 0.4450518786907196\n",
      "2000 train complete -- accruacy : 0.9380000233650208, loss : 0.18553587794303894\n",
      "2000 test complete == accruacy : 0.8529741764068604, loss : 0.4147818982601166\n",
      "2010 train complete -- accruacy : 0.9442785978317261, loss : 0.16156677901744843\n",
      "2010 test complete == accruacy : 0.8787878751754761, loss : 0.3727366030216217\n",
      "2020 train complete -- accruacy : 0.9024752378463745, loss : 0.2644580900669098\n",
      "2020 test complete == accruacy : 0.8585858345031738, loss : 0.3869454264640808\n",
      "2030 train complete -- accruacy : 0.9325123429298401, loss : 0.19766247272491455\n",
      "2030 test complete == accruacy : 0.8529741764068604, loss : 0.39193958044052124\n",
      "2040 train complete -- accruacy : 0.9220588207244873, loss : 0.21367081999778748\n",
      "2040 test complete == accruacy : 0.8484848737716675, loss : 0.4155007004737854\n",
      "2050 train complete -- accruacy : 0.9385365843772888, loss : 0.18593235313892365\n",
      "2050 test complete == accruacy : 0.8439955115318298, loss : 0.40135931968688965\n",
      "2060 train complete -- accruacy : 0.9169902801513672, loss : 0.24659891426563263\n",
      "2060 test complete == accruacy : 0.8473625183105469, loss : 0.4669048488140106\n",
      "2070 train complete -- accruacy : 0.9222221970558167, loss : 0.21824447810649872\n",
      "2070 test complete == accruacy : 0.8395061492919922, loss : 0.43870386481285095\n",
      "2080 train complete -- accruacy : 0.9197115302085876, loss : 0.2264215350151062\n",
      "2080 test complete == accruacy : 0.8496071696281433, loss : 0.4326035976409912\n",
      "2090 train complete -- accruacy : 0.921531081199646, loss : 0.220829039812088\n",
      "2090 test complete == accruacy : 0.860830545425415, loss : 0.4094589054584503\n",
      "2100 train complete -- accruacy : 0.9195238351821899, loss : 0.2290181964635849\n",
      "2100 test complete == accruacy : 0.8776655197143555, loss : 0.3712150752544403\n",
      "2110 train complete -- accruacy : 0.9113743901252747, loss : 0.24627558887004852\n",
      "2110 test complete == accruacy : 0.8664422035217285, loss : 0.3828291594982147\n",
      "2120 train complete -- accruacy : 0.949999988079071, loss : 0.15246109664440155\n",
      "2120 test complete == accruacy : 0.8731762170791626, loss : 0.36519715189933777\n",
      "2130 train complete -- accruacy : 0.9333333373069763, loss : 0.19040247797966003\n",
      "2130 test complete == accruacy : 0.8765432238578796, loss : 0.38302290439605713\n",
      "2140 train complete -- accruacy : 0.941121518611908, loss : 0.17507517337799072\n",
      "2140 test complete == accruacy : 0.8529741764068604, loss : 0.41605260968208313\n",
      "2150 train complete -- accruacy : 0.9311627745628357, loss : 0.20287658274173737\n",
      "2150 test complete == accruacy : 0.8428731560707092, loss : 0.45986685156822205\n",
      "2160 train complete -- accruacy : 0.9074074029922485, loss : 0.2343960404396057\n",
      "2160 test complete == accruacy : 0.8630751967430115, loss : 0.4184165596961975\n",
      "2170 train complete -- accruacy : 0.9059907793998718, loss : 0.2593621611595154\n",
      "2170 test complete == accruacy : 0.8439955115318298, loss : 0.40774092078208923\n",
      "2180 train complete -- accruacy : 0.9325687885284424, loss : 0.18375562131404877\n",
      "2180 test complete == accruacy : 0.854096531867981, loss : 0.38996878266334534\n",
      "2190 train complete -- accruacy : 0.9319634437561035, loss : 0.19283738732337952\n",
      "2190 test complete == accruacy : 0.8451178669929504, loss : 0.4264441728591919\n",
      "2200 train complete -- accruacy : 0.9354545474052429, loss : 0.1710028499364853\n",
      "2200 test complete == accruacy : 0.8552188277244568, loss : 0.4149126410484314\n",
      "2210 train complete -- accruacy : 0.9339366555213928, loss : 0.1958252191543579\n",
      "2210 test complete == accruacy : 0.8585858345031738, loss : 0.42330241203308105\n",
      "2220 train complete -- accruacy : 0.9103603363037109, loss : 0.25279200077056885\n",
      "2220 test complete == accruacy : 0.8462401628494263, loss : 0.4204864799976349\n",
      "2230 train complete -- accruacy : 0.9627802968025208, loss : 0.11354774236679077\n",
      "2230 test complete == accruacy : 0.8709315657615662, loss : 0.3780285716056824\n",
      "2240 train complete -- accruacy : 0.9446428418159485, loss : 0.1657605618238449\n",
      "2240 test complete == accruacy : 0.860830545425415, loss : 0.4129723906517029\n",
      "2250 train complete -- accruacy : 0.9222221970558167, loss : 0.2265728861093521\n",
      "2250 test complete == accruacy : 0.8529741764068604, loss : 0.4446772336959839\n",
      "2260 train complete -- accruacy : 0.941150426864624, loss : 0.17281492054462433\n",
      "2260 test complete == accruacy : 0.8103255033493042, loss : 0.5709007978439331\n",
      "2270 train complete -- accruacy : 0.9365638494491577, loss : 0.19600126147270203\n",
      "2270 test complete == accruacy : 0.8698092103004456, loss : 0.367328941822052\n",
      "2280 train complete -- accruacy : 0.9280701875686646, loss : 0.19718974828720093\n",
      "2280 test complete == accruacy : 0.8507295250892639, loss : 0.418534517288208\n",
      "2290 train complete -- accruacy : 0.9327511191368103, loss : 0.19314329326152802\n",
      "2290 test complete == accruacy : 0.8799102306365967, loss : 0.3830397129058838\n",
      "2300 train complete -- accruacy : 0.9495652318000793, loss : 0.14592374861240387\n",
      "2300 test complete == accruacy : 0.8900112509727478, loss : 0.3513343930244446\n",
      "2310 train complete -- accruacy : 0.9432900547981262, loss : 0.1724615842103958\n",
      "2310 test complete == accruacy : 0.8484848737716675, loss : 0.3936329483985901\n",
      "2320 train complete -- accruacy : 0.9465517401695251, loss : 0.15394918620586395\n",
      "2320 test complete == accruacy : 0.8799102306365967, loss : 0.3507390320301056\n",
      "2330 train complete -- accruacy : 0.9257510900497437, loss : 0.20860300958156586\n",
      "2330 test complete == accruacy : 0.8787878751754761, loss : 0.3325482904911041\n",
      "2340 train complete -- accruacy : 0.8752136826515198, loss : 0.3300307095050812\n",
      "2340 test complete == accruacy : 0.8507295250892639, loss : 0.41743436455726624\n",
      "2350 train complete -- accruacy : 0.9382978677749634, loss : 0.17814181745052338\n",
      "2350 test complete == accruacy : 0.8518518805503845, loss : 0.3977883458137512\n",
      "2360 train complete -- accruacy : 0.9508474469184875, loss : 0.14736072719097137\n",
      "2360 test complete == accruacy : 0.8709315657615662, loss : 0.3330775201320648\n",
      "2370 train complete -- accruacy : 0.891983151435852, loss : 0.30189988017082214\n",
      "2370 test complete == accruacy : 0.8507295250892639, loss : 0.41192060708999634\n",
      "2380 train complete -- accruacy : 0.9310924410820007, loss : 0.19817422330379486\n",
      "2380 test complete == accruacy : 0.8585858345031738, loss : 0.40608662366867065\n",
      "2390 train complete -- accruacy : 0.9468619227409363, loss : 0.15389299392700195\n",
      "2390 test complete == accruacy : 0.8877665400505066, loss : 0.3392921984195709\n",
      "2400 train complete -- accruacy : 0.9479166865348816, loss : 0.15315121412277222\n",
      "2400 test complete == accruacy : 0.8978675603866577, loss : 0.3307105600833893\n",
      "2410 train complete -- accruacy : 0.9381742477416992, loss : 0.17639948427677155\n",
      "2410 test complete == accruacy : 0.8810325264930725, loss : 0.3538052439689636\n",
      "2420 train complete -- accruacy : 0.9297520518302917, loss : 0.2125364989042282\n",
      "2420 test complete == accruacy : 0.8428731560707092, loss : 0.4568130671977997\n",
      "2430 train complete -- accruacy : 0.9452674984931946, loss : 0.15834756195545197\n",
      "2430 test complete == accruacy : 0.872053861618042, loss : 0.3509002923965454\n",
      "2440 train complete -- accruacy : 0.9401639103889465, loss : 0.17388880252838135\n",
      "2440 test complete == accruacy : 0.8742985129356384, loss : 0.35916706919670105\n",
      "2450 train complete -- accruacy : 0.9506122469902039, loss : 0.15024086833000183\n",
      "2450 test complete == accruacy : 0.8787878751754761, loss : 0.3557918071746826\n",
      "2460 train complete -- accruacy : 0.9410569071769714, loss : 0.18515415489673615\n",
      "2460 test complete == accruacy : 0.8888888955116272, loss : 0.31094419956207275\n",
      "2470 train complete -- accruacy : 0.9307692050933838, loss : 0.19748395681381226\n",
      "2470 test complete == accruacy : 0.8776655197143555, loss : 0.3653546869754791\n",
      "2480 train complete -- accruacy : 0.9407258033752441, loss : 0.17040976881980896\n",
      "2480 test complete == accruacy : 0.8855218887329102, loss : 0.3422241806983948\n",
      "2490 train complete -- accruacy : 0.9586345553398132, loss : 0.11825975775718689\n",
      "2490 test complete == accruacy : 0.8989899158477783, loss : 0.3076752722263336\n",
      "2500 train complete -- accruacy : 0.9348000288009644, loss : 0.1803242564201355\n",
      "2500 test complete == accruacy : 0.8922559022903442, loss : 0.33563926815986633\n",
      "2510 train complete -- accruacy : 0.9486055970191956, loss : 0.13610631227493286\n",
      "2510 test complete == accruacy : 0.8855218887329102, loss : 0.329111248254776\n",
      "2520 train complete -- accruacy : 0.9301587343215942, loss : 0.1974283754825592\n",
      "2520 test complete == accruacy : 0.8877665400505066, loss : 0.3320769667625427\n",
      "2530 train complete -- accruacy : 0.9505928754806519, loss : 0.14738477766513824\n",
      "2530 test complete == accruacy : 0.8922559022903442, loss : 0.31552156805992126\n",
      "2540 train complete -- accruacy : 0.9574803113937378, loss : 0.12449271976947784\n",
      "2540 test complete == accruacy : 0.9034792184829712, loss : 0.2880495488643646\n",
      "2550 train complete -- accruacy : 0.9466666579246521, loss : 0.15249119699001312\n",
      "2550 test complete == accruacy : 0.8989899158477783, loss : 0.3256661891937256\n",
      "2560 train complete -- accruacy : 0.966015636920929, loss : 0.11481600999832153\n",
      "2560 test complete == accruacy : 0.8978675603866577, loss : 0.3213718831539154\n",
      "2570 train complete -- accruacy : 0.9494163393974304, loss : 0.15123631060123444\n",
      "2570 test complete == accruacy : 0.8989899158477783, loss : 0.3046891689300537\n",
      "2580 train complete -- accruacy : 0.9608527421951294, loss : 0.11961234360933304\n",
      "2580 test complete == accruacy : 0.9158248901367188, loss : 0.29908549785614014\n",
      "2590 train complete -- accruacy : 0.9571428298950195, loss : 0.13902926445007324\n",
      "2590 test complete == accruacy : 0.8978675603866577, loss : 0.3125814199447632\n",
      "2600 train complete -- accruacy : 0.9573076963424683, loss : 0.126174658536911\n",
      "2600 test complete == accruacy : 0.8877665400505066, loss : 0.3362404406070709\n",
      "2610 train complete -- accruacy : 0.9590038061141968, loss : 0.12708157300949097\n",
      "2610 test complete == accruacy : 0.8709315657615662, loss : 0.3706279397010803\n",
      "2620 train complete -- accruacy : 0.9492366313934326, loss : 0.16078853607177734\n",
      "2620 test complete == accruacy : 0.8945005536079407, loss : 0.32308250665664673\n",
      "2630 train complete -- accruacy : 0.9452471733093262, loss : 0.15078234672546387\n",
      "2630 test complete == accruacy : 0.8832772374153137, loss : 0.301967591047287\n",
      "2640 train complete -- accruacy : 0.9473484754562378, loss : 0.15782994031906128\n",
      "2640 test complete == accruacy : 0.8799102306365967, loss : 0.36599984765052795\n",
      "2650 train complete -- accruacy : 0.9535849094390869, loss : 0.14122608304023743\n",
      "2650 test complete == accruacy : 0.8698092103004456, loss : 0.4339076280593872\n",
      "2660 train complete -- accruacy : 0.9593985080718994, loss : 0.12447765469551086\n",
      "2660 test complete == accruacy : 0.8866442441940308, loss : 0.31989458203315735\n",
      "2670 train complete -- accruacy : 0.9370786547660828, loss : 0.1697559505701065\n",
      "2670 test complete == accruacy : 0.8776655197143555, loss : 0.3527645766735077\n",
      "2680 train complete -- accruacy : 0.9358208775520325, loss : 0.18289455771446228\n",
      "2680 test complete == accruacy : 0.8799102306365967, loss : 0.36692509055137634\n",
      "2690 train complete -- accruacy : 0.9509293437004089, loss : 0.15157060325145721\n",
      "2690 test complete == accruacy : 0.8664422035217285, loss : 0.41683170199394226\n",
      "2700 train complete -- accruacy : 0.9644444584846497, loss : 0.11749362200498581\n",
      "2700 test complete == accruacy : 0.9023569226264954, loss : 0.33216679096221924\n",
      "2710 train complete -- accruacy : 0.9483394622802734, loss : 0.15252532064914703\n",
      "2710 test complete == accruacy : 0.8799102306365967, loss : 0.412884920835495\n",
      "2720 train complete -- accruacy : 0.9430146813392639, loss : 0.15566055476665497\n",
      "2720 test complete == accruacy : 0.8978675603866577, loss : 0.31440269947052\n",
      "2730 train complete -- accruacy : 0.9567765593528748, loss : 0.1304996758699417\n",
      "2730 test complete == accruacy : 0.8956229090690613, loss : 0.3094218969345093\n",
      "2740 train complete -- accruacy : 0.9649634957313538, loss : 0.11030296981334686\n",
      "2740 test complete == accruacy : 0.9102132320404053, loss : 0.29992789030075073\n",
      "2750 train complete -- accruacy : 0.9527272582054138, loss : 0.13274796307086945\n",
      "2750 test complete == accruacy : 0.9113355875015259, loss : 0.2773357927799225\n",
      "2760 train complete -- accruacy : 0.9387680888175964, loss : 0.17234422266483307\n",
      "2760 test complete == accruacy : 0.8967452049255371, loss : 0.2981439232826233\n",
      "2770 train complete -- accruacy : 0.9512635469436646, loss : 0.1425906866788864\n",
      "2770 test complete == accruacy : 0.8630751967430115, loss : 0.37760186195373535\n",
      "2780 train complete -- accruacy : 0.9640287756919861, loss : 0.10806559026241302\n",
      "2780 test complete == accruacy : 0.8989899158477783, loss : 0.2753828465938568\n",
      "2790 train complete -- accruacy : 0.9641577005386353, loss : 0.10618460923433304\n",
      "2790 test complete == accruacy : 0.9079685807228088, loss : 0.3138715922832489\n",
      "2800 train complete -- accruacy : 0.9635714292526245, loss : 0.110244981944561\n",
      "2800 test complete == accruacy : 0.8956229090690613, loss : 0.27172407507896423\n",
      "2810 train complete -- accruacy : 0.9715302586555481, loss : 0.09363114088773727\n",
      "2810 test complete == accruacy : 0.9046015739440918, loss : 0.32262489199638367\n",
      "2820 train complete -- accruacy : 0.9343971610069275, loss : 0.18482917547225952\n",
      "2820 test complete == accruacy : 0.8922559022903442, loss : 0.3722195029258728\n",
      "2830 train complete -- accruacy : 0.9247350096702576, loss : 0.21582891047000885\n",
      "2830 test complete == accruacy : 0.8765432238578796, loss : 0.4056732952594757\n",
      "2840 train complete -- accruacy : 0.9394366145133972, loss : 0.15804077684879303\n",
      "2840 test complete == accruacy : 0.8989899158477783, loss : 0.30559173226356506\n",
      "2850 train complete -- accruacy : 0.9599999785423279, loss : 0.12200670689344406\n",
      "2850 test complete == accruacy : 0.9068462252616882, loss : 0.29300668835639954\n",
      "2860 train complete -- accruacy : 0.9569929838180542, loss : 0.12900717556476593\n",
      "2860 test complete == accruacy : 0.9124578833580017, loss : 0.29605522751808167\n",
      "2870 train complete -- accruacy : 0.9498257637023926, loss : 0.14183515310287476\n",
      "2870 test complete == accruacy : 0.9023569226264954, loss : 0.2815385162830353\n",
      "2880 train complete -- accruacy : 0.9545139074325562, loss : 0.12823668122291565\n",
      "2880 test complete == accruacy : 0.9001122117042542, loss : 0.31896495819091797\n",
      "2890 train complete -- accruacy : 0.9515570998191833, loss : 0.14523650705814362\n",
      "2890 test complete == accruacy : 0.8956229090690613, loss : 0.3172549903392792\n",
      "2900 train complete -- accruacy : 0.9506896734237671, loss : 0.1262301802635193\n",
      "2900 test complete == accruacy : 0.9046015739440918, loss : 0.29253533482551575\n",
      "2910 train complete -- accruacy : 0.960824728012085, loss : 0.1165766566991806\n",
      "2910 test complete == accruacy : 0.8843995332717896, loss : 0.3582397997379303\n",
      "2920 train complete -- accruacy : 0.949315071105957, loss : 0.14863216876983643\n",
      "2920 test complete == accruacy : 0.9147025942802429, loss : 0.26997286081314087\n",
      "2930 train complete -- accruacy : 0.9477815628051758, loss : 0.15392199158668518\n",
      "2930 test complete == accruacy : 0.9012345671653748, loss : 0.30415797233581543\n",
      "2940 train complete -- accruacy : 0.9642857313156128, loss : 0.10460764914751053\n",
      "2940 test complete == accruacy : 0.9124578833580017, loss : 0.2617773711681366\n",
      "2950 train complete -- accruacy : 0.9349152445793152, loss : 0.18045975267887115\n",
      "2950 test complete == accruacy : 0.8731762170791626, loss : 0.3387170433998108\n",
      "2960 train complete -- accruacy : 0.9516891837120056, loss : 0.13222365081310272\n",
      "2960 test complete == accruacy : 0.8956229090690613, loss : 0.3001328110694885\n",
      "2970 train complete -- accruacy : 0.9663299918174744, loss : 0.09727439284324646\n",
      "2970 test complete == accruacy : 0.9102132320404053, loss : 0.2581339478492737\n",
      "2980 train complete -- accruacy : 0.9553691148757935, loss : 0.13175219297409058\n",
      "2980 test complete == accruacy : 0.8552188277244568, loss : 0.49512937664985657\n",
      "2990 train complete -- accruacy : 0.944481611251831, loss : 0.1576804667711258\n",
      "2990 test complete == accruacy : 0.9001122117042542, loss : 0.29590335488319397\n",
      "3000 train complete -- accruacy : 0.9496666789054871, loss : 0.14325763285160065\n",
      "3000 test complete == accruacy : 0.8888888955116272, loss : 0.3344799876213074\n",
      "3010 train complete -- accruacy : 0.9534883499145508, loss : 0.13367700576782227\n",
      "3010 test complete == accruacy : 0.9034792184829712, loss : 0.2777214050292969\n",
      "3020 train complete -- accruacy : 0.9642384052276611, loss : 0.11878236383199692\n",
      "3020 test complete == accruacy : 0.9068462252616882, loss : 0.2861917018890381\n",
      "3030 train complete -- accruacy : 0.9422442317008972, loss : 0.1522590070962906\n",
      "3030 test complete == accruacy : 0.854096531867981, loss : 0.45677733421325684\n",
      "3040 train complete -- accruacy : 0.96085524559021, loss : 0.11211363971233368\n",
      "3040 test complete == accruacy : 0.9102132320404053, loss : 0.25505518913269043\n",
      "3050 train complete -- accruacy : 0.956721305847168, loss : 0.12294323742389679\n",
      "3050 test complete == accruacy : 0.8956229090690613, loss : 0.3401477038860321\n",
      "3060 train complete -- accruacy : 0.9669934511184692, loss : 0.10768657177686691\n",
      "3060 test complete == accruacy : 0.9023569226264954, loss : 0.2664191424846649\n",
      "3070 train complete -- accruacy : 0.9338762164115906, loss : 0.18604150414466858\n",
      "3070 test complete == accruacy : 0.9001122117042542, loss : 0.3024691343307495\n",
      "3080 train complete -- accruacy : 0.9642857313156128, loss : 0.11392983794212341\n",
      "3080 test complete == accruacy : 0.8888888955116272, loss : 0.3783930242061615\n",
      "3090 train complete -- accruacy : 0.955339789390564, loss : 0.12461153417825699\n",
      "3090 test complete == accruacy : 0.8989899158477783, loss : 0.2886142134666443\n",
      "3100 train complete -- accruacy : 0.9729032516479492, loss : 0.09604115784168243\n",
      "3100 test complete == accruacy : 0.9113355875015259, loss : 0.28792643547058105\n",
      "3110 train complete -- accruacy : 0.9508038759231567, loss : 0.14770059287548065\n",
      "3110 test complete == accruacy : 0.8989899158477783, loss : 0.3010118305683136\n",
      "3120 train complete -- accruacy : 0.9628205299377441, loss : 0.10773692280054092\n",
      "3120 test complete == accruacy : 0.8799102306365967, loss : 0.35706648230552673\n",
      "3130 train complete -- accruacy : 0.9638977646827698, loss : 0.11322963982820511\n",
      "3130 test complete == accruacy : 0.9124578833580017, loss : 0.2755911648273468\n",
      "3140 train complete -- accruacy : 0.9617834687232971, loss : 0.1137775331735611\n",
      "3140 test complete == accruacy : 0.9259259104728699, loss : 0.23061628639698029\n",
      "3150 train complete -- accruacy : 0.9644444584846497, loss : 0.10919845104217529\n",
      "3150 test complete == accruacy : 0.9124578833580017, loss : 0.2589307725429535\n",
      "3160 train complete -- accruacy : 0.9648734331130981, loss : 0.10978678613901138\n",
      "3160 test complete == accruacy : 0.9191918969154358, loss : 0.24404369294643402\n",
      "3170 train complete -- accruacy : 0.9649842381477356, loss : 0.10728994011878967\n",
      "3170 test complete == accruacy : 0.8967452049255371, loss : 0.297011137008667\n",
      "3180 train complete -- accruacy : 0.9720125794410706, loss : 0.09061552584171295\n",
      "3180 test complete == accruacy : 0.91806960105896, loss : 0.2519555687904358\n",
      "3190 train complete -- accruacy : 0.957993745803833, loss : 0.13244669139385223\n",
      "3190 test complete == accruacy : 0.8945005536079407, loss : 0.3272239565849304\n",
      "3200 train complete -- accruacy : 0.9643750190734863, loss : 0.1146540716290474\n",
      "3200 test complete == accruacy : 0.9034792184829712, loss : 0.29473814368247986\n",
      "3210 train complete -- accruacy : 0.9610592126846313, loss : 0.1175396516919136\n",
      "3210 test complete == accruacy : 0.9034792184829712, loss : 0.2649889588356018\n",
      "3220 train complete -- accruacy : 0.9652174115180969, loss : 0.10270578414201736\n",
      "3220 test complete == accruacy : 0.8967452049255371, loss : 0.27728405594825745\n",
      "3230 train complete -- accruacy : 0.9628483057022095, loss : 0.11215844005346298\n",
      "3230 test complete == accruacy : 0.9102132320404053, loss : 0.2749755084514618\n",
      "3240 train complete -- accruacy : 0.9620370268821716, loss : 0.10798858106136322\n",
      "3240 test complete == accruacy : 0.91806960105896, loss : 0.24844256043434143\n",
      "3250 train complete -- accruacy : 0.9572307467460632, loss : 0.12874436378479004\n",
      "3250 test complete == accruacy : 0.9203142523765564, loss : 0.25945186614990234\n",
      "3260 train complete -- accruacy : 0.9460122585296631, loss : 0.15004731714725494\n",
      "3260 test complete == accruacy : 0.9113355875015259, loss : 0.254136860370636\n",
      "3270 train complete -- accruacy : 0.9602446556091309, loss : 0.12313393503427505\n",
      "3270 test complete == accruacy : 0.9292929172515869, loss : 0.2348577380180359\n",
      "3280 train complete -- accruacy : 0.9658536314964294, loss : 0.107764832675457\n",
      "3280 test complete == accruacy : 0.9046015739440918, loss : 0.3155629634857178\n",
      "3290 train complete -- accruacy : 0.9516717195510864, loss : 0.13570819795131683\n",
      "3290 test complete == accruacy : 0.9090909361839294, loss : 0.27759575843811035\n",
      "3300 train complete -- accruacy : 0.9678787589073181, loss : 0.10324333608150482\n",
      "3300 test complete == accruacy : 0.9023569226264954, loss : 0.31984055042266846\n",
      "3310 train complete -- accruacy : 0.9601208567619324, loss : 0.12382706254720688\n",
      "3310 test complete == accruacy : 0.9057239294052124, loss : 0.28597137331962585\n",
      "3320 train complete -- accruacy : 0.9762048125267029, loss : 0.07007814198732376\n",
      "3320 test complete == accruacy : 0.9236812591552734, loss : 0.23506401479244232\n",
      "3330 train complete -- accruacy : 0.9579579830169678, loss : 0.13204851746559143\n",
      "3330 test complete == accruacy : 0.9102132320404053, loss : 0.27197739481925964\n",
      "3340 train complete -- accruacy : 0.9667664766311646, loss : 0.10570517927408218\n",
      "3340 test complete == accruacy : 0.9102132320404053, loss : 0.3212589919567108\n",
      "3350 train complete -- accruacy : 0.9588059782981873, loss : 0.11517912149429321\n",
      "3350 test complete == accruacy : 0.9124578833580017, loss : 0.2876896262168884\n",
      "3360 train complete -- accruacy : 0.9758928418159485, loss : 0.08049102127552032\n",
      "3360 test complete == accruacy : 0.9225589036941528, loss : 0.24287496507167816\n",
      "3370 train complete -- accruacy : 0.9548961520195007, loss : 0.12519653141498566\n",
      "3370 test complete == accruacy : 0.9046015739440918, loss : 0.2689773142337799\n",
      "3380 train complete -- accruacy : 0.9579881429672241, loss : 0.11991755664348602\n",
      "3380 test complete == accruacy : 0.9191918969154358, loss : 0.25237470865249634\n",
      "3390 train complete -- accruacy : 0.9681416153907776, loss : 0.10267682373523712\n",
      "3390 test complete == accruacy : 0.9147025942802429, loss : 0.27755528688430786\n",
      "3400 train complete -- accruacy : 0.9602941274642944, loss : 0.11621062457561493\n",
      "3400 test complete == accruacy : 0.8866442441940308, loss : 0.3030019998550415\n",
      "3410 train complete -- accruacy : 0.9592375159263611, loss : 0.12810374796390533\n",
      "3410 test complete == accruacy : 0.9259259104728699, loss : 0.22525610029697418\n",
      "3420 train complete -- accruacy : 0.9649122953414917, loss : 0.09876292943954468\n",
      "3420 test complete == accruacy : 0.9382715821266174, loss : 0.20147067308425903\n",
      "3430 train complete -- accruacy : 0.9597667455673218, loss : 0.11365199834108353\n",
      "3430 test complete == accruacy : 0.8945005536079407, loss : 0.31528395414352417\n",
      "3440 train complete -- accruacy : 0.9497092962265015, loss : 0.14403001964092255\n",
      "3440 test complete == accruacy : 0.8776655197143555, loss : 0.30013737082481384\n",
      "3450 train complete -- accruacy : 0.9553623199462891, loss : 0.1340261995792389\n",
      "3450 test complete == accruacy : 0.921436607837677, loss : 0.22804148495197296\n",
      "3460 train complete -- accruacy : 0.9679190516471863, loss : 0.08753471076488495\n",
      "3460 test complete == accruacy : 0.9281706213951111, loss : 0.2481464445590973\n",
      "3470 train complete -- accruacy : 0.9510086178779602, loss : 0.13357317447662354\n",
      "3470 test complete == accruacy : 0.9068462252616882, loss : 0.27454113960266113\n",
      "3480 train complete -- accruacy : 0.9801723957061768, loss : 0.07073334604501724\n",
      "3480 test complete == accruacy : 0.9270482659339905, loss : 0.273707777261734\n",
      "3490 train complete -- accruacy : 0.96590256690979, loss : 0.10593400150537491\n",
      "3490 test complete == accruacy : 0.9225589036941528, loss : 0.24928031861782074\n",
      "3500 train complete -- accruacy : 0.9668571352958679, loss : 0.09804513305425644\n",
      "3500 test complete == accruacy : 0.9023569226264954, loss : 0.2509210705757141\n",
      "3510 train complete -- accruacy : 0.9774928689002991, loss : 0.07150732725858688\n",
      "3510 test complete == accruacy : 0.9158248901367188, loss : 0.2468753159046173\n",
      "3520 train complete -- accruacy : 0.9602272510528564, loss : 0.11437315493822098\n",
      "3520 test complete == accruacy : 0.9169472455978394, loss : 0.253980815410614\n",
      "3530 train complete -- accruacy : 0.9679886698722839, loss : 0.09506398439407349\n",
      "3530 test complete == accruacy : 0.9169472455978394, loss : 0.23604604601860046\n",
      "3540 train complete -- accruacy : 0.9610169529914856, loss : 0.11192382872104645\n",
      "3540 test complete == accruacy : 0.9259259104728699, loss : 0.1961163729429245\n",
      "3550 train complete -- accruacy : 0.9774647951126099, loss : 0.07889462262392044\n",
      "3550 test complete == accruacy : 0.9382715821266174, loss : 0.21333980560302734\n",
      "3560 train complete -- accruacy : 0.9623595476150513, loss : 0.10965967178344727\n",
      "3560 test complete == accruacy : 0.9102132320404053, loss : 0.24639251828193665\n",
      "3570 train complete -- accruacy : 0.9641456604003906, loss : 0.10599350929260254\n",
      "3570 test complete == accruacy : 0.9124578833580017, loss : 0.2414805293083191\n",
      "3580 train complete -- accruacy : 0.9572625756263733, loss : 0.13329443335533142\n",
      "3580 test complete == accruacy : 0.9090909361839294, loss : 0.2510751485824585\n",
      "3590 train complete -- accruacy : 0.9540389776229858, loss : 0.12968240678310394\n",
      "3590 test complete == accruacy : 0.9169472455978394, loss : 0.24586576223373413\n",
      "3600 train complete -- accruacy : 0.9752777814865112, loss : 0.07997666299343109\n",
      "3600 test complete == accruacy : 0.9259259104728699, loss : 0.20489418506622314\n",
      "3610 train complete -- accruacy : 0.9526315927505493, loss : 0.13270534574985504\n",
      "3610 test complete == accruacy : 0.9046015739440918, loss : 0.25393983721733093\n",
      "3620 train complete -- accruacy : 0.9729281663894653, loss : 0.08211606740951538\n",
      "3620 test complete == accruacy : 0.9169472455978394, loss : 0.31839993596076965\n",
      "3630 train complete -- accruacy : 0.9724518060684204, loss : 0.09405601769685745\n",
      "3630 test complete == accruacy : 0.939393937587738, loss : 0.19211454689502716\n",
      "3640 train complete -- accruacy : 0.9725274443626404, loss : 0.08590727299451828\n",
      "3640 test complete == accruacy : 0.9371492862701416, loss : 0.19489216804504395\n",
      "3650 train complete -- accruacy : 0.962465763092041, loss : 0.10638724267482758\n",
      "3650 test complete == accruacy : 0.9034792184829712, loss : 0.2706080973148346\n",
      "3660 train complete -- accruacy : 0.9680327773094177, loss : 0.10010538250207901\n",
      "3660 test complete == accruacy : 0.932659924030304, loss : 0.19740110635757446\n",
      "3670 train complete -- accruacy : 0.9700272679328918, loss : 0.09724269062280655\n",
      "3670 test complete == accruacy : 0.9281706213951111, loss : 0.22847239673137665\n",
      "3680 train complete -- accruacy : 0.969293475151062, loss : 0.09001250565052032\n",
      "3680 test complete == accruacy : 0.9315375685691833, loss : 0.20180939137935638\n",
      "3690 train complete -- accruacy : 0.9701896905899048, loss : 0.09638062864542007\n",
      "3690 test complete == accruacy : 0.9382715821266174, loss : 0.18658965826034546\n",
      "3700 train complete -- accruacy : 0.9724324345588684, loss : 0.0852290466427803\n",
      "3700 test complete == accruacy : 0.9371492862701416, loss : 0.1834622025489807\n",
      "3710 train complete -- accruacy : 0.9684635996818542, loss : 0.09618458896875381\n",
      "3710 test complete == accruacy : 0.9405162930488586, loss : 0.1707324981689453\n",
      "3720 train complete -- accruacy : 0.9561827778816223, loss : 0.13931158185005188\n",
      "3720 test complete == accruacy : 0.9057239294052124, loss : 0.22898036241531372\n",
      "3730 train complete -- accruacy : 0.9680964946746826, loss : 0.10013898462057114\n",
      "3730 test complete == accruacy : 0.9292929172515869, loss : 0.22224445641040802\n",
      "3740 train complete -- accruacy : 0.9735293984413147, loss : 0.07929769158363342\n",
      "3740 test complete == accruacy : 0.9203142523765564, loss : 0.25005248188972473\n",
      "3750 train complete -- accruacy : 0.9821333289146423, loss : 0.06037485599517822\n",
      "3750 test complete == accruacy : 0.9438832998275757, loss : 0.1924024224281311\n",
      "3760 train complete -- accruacy : 0.9704787135124207, loss : 0.09581011533737183\n",
      "3760 test complete == accruacy : 0.9124578833580017, loss : 0.26592308282852173\n",
      "3770 train complete -- accruacy : 0.9753315448760986, loss : 0.07884854823350906\n",
      "3770 test complete == accruacy : 0.9270482659339905, loss : 0.2446628212928772\n",
      "3780 train complete -- accruacy : 0.9722222089767456, loss : 0.087918221950531\n",
      "3780 test complete == accruacy : 0.932659924030304, loss : 0.18360915780067444\n",
      "3790 train complete -- accruacy : 0.960422158241272, loss : 0.11385203152894974\n",
      "3790 test complete == accruacy : 0.8709315657615662, loss : 0.3124840259552002\n",
      "3800 train complete -- accruacy : 0.9744736552238464, loss : 0.08746567368507385\n",
      "3800 test complete == accruacy : 0.9371492862701416, loss : 0.18932197988033295\n",
      "3810 train complete -- accruacy : 0.9700787663459778, loss : 0.09173963963985443\n",
      "3810 test complete == accruacy : 0.924803614616394, loss : 0.23737899959087372\n",
      "3820 train complete -- accruacy : 0.9756544232368469, loss : 0.0745806097984314\n",
      "3820 test complete == accruacy : 0.9405162930488586, loss : 0.19793599843978882\n",
      "3830 train complete -- accruacy : 0.9684073328971863, loss : 0.08830269426107407\n",
      "3830 test complete == accruacy : 0.924803614616394, loss : 0.22554875910282135\n",
      "3840 train complete -- accruacy : 0.9747396111488342, loss : 0.08005783706903458\n",
      "3840 test complete == accruacy : 0.9416385889053345, loss : 0.17677779495716095\n",
      "3850 train complete -- accruacy : 0.9680519700050354, loss : 0.09508545696735382\n",
      "3850 test complete == accruacy : 0.9349045753479004, loss : 0.21334515511989594\n",
      "3860 train complete -- accruacy : 0.9808290004730225, loss : 0.06624075770378113\n",
      "3860 test complete == accruacy : 0.9562289714813232, loss : 0.1659228354692459\n",
      "3870 train complete -- accruacy : 0.9697674512863159, loss : 0.09548184275627136\n",
      "3870 test complete == accruacy : 0.9416385889053345, loss : 0.1844542771577835\n",
      "3880 train complete -- accruacy : 0.9783505201339722, loss : 0.06858091056346893\n",
      "3880 test complete == accruacy : 0.9450055956840515, loss : 0.20419426262378693\n",
      "3890 train complete -- accruacy : 0.9717223644256592, loss : 0.09197700768709183\n",
      "3890 test complete == accruacy : 0.924803614616394, loss : 0.2248220145702362\n",
      "3900 train complete -- accruacy : 0.9689743518829346, loss : 0.08687003701925278\n",
      "3900 test complete == accruacy : 0.9292929172515869, loss : 0.23511812090873718\n",
      "3910 train complete -- accruacy : 0.95652174949646, loss : 0.1224026307463646\n",
      "3910 test complete == accruacy : 0.8945005536079407, loss : 0.2687044143676758\n",
      "3920 train complete -- accruacy : 0.9755101799964905, loss : 0.08136413991451263\n",
      "3920 test complete == accruacy : 0.9281706213951111, loss : 0.21576498448848724\n",
      "3930 train complete -- accruacy : 0.970992386341095, loss : 0.08603481948375702\n",
      "3930 test complete == accruacy : 0.9450055956840515, loss : 0.17202433943748474\n",
      "3940 train complete -- accruacy : 0.9730964303016663, loss : 0.08823773264884949\n",
      "3940 test complete == accruacy : 0.9349045753479004, loss : 0.22416530549526215\n",
      "3950 train complete -- accruacy : 0.9794936776161194, loss : 0.07356350123882294\n",
      "3950 test complete == accruacy : 0.9292929172515869, loss : 0.21487954258918762\n",
      "3960 train complete -- accruacy : 0.9724747538566589, loss : 0.08073518425226212\n",
      "3960 test complete == accruacy : 0.9349045753479004, loss : 0.2074466347694397\n",
      "3970 train complete -- accruacy : 0.9629722833633423, loss : 0.11317050457000732\n",
      "3970 test complete == accruacy : 0.932659924030304, loss : 0.19740241765975952\n",
      "3980 train complete -- accruacy : 0.9758793711662292, loss : 0.08324287086725235\n",
      "3980 test complete == accruacy : 0.9259259104728699, loss : 0.20339609682559967\n",
      "3990 train complete -- accruacy : 0.9756892323493958, loss : 0.07435895502567291\n",
      "3990 test complete == accruacy : 0.9203142523765564, loss : 0.2538498640060425\n",
      "4000 train complete -- accruacy : 0.9700000286102295, loss : 0.09205204993486404\n",
      "4000 test complete == accruacy : 0.9483726024627686, loss : 0.16813348233699799\n",
      "4010 train complete -- accruacy : 0.9748129844665527, loss : 0.08352427929639816\n",
      "4010 test complete == accruacy : 0.9259259104728699, loss : 0.22485436499118805\n",
      "4020 train complete -- accruacy : 0.9666666388511658, loss : 0.10511413216590881\n",
      "4020 test complete == accruacy : 0.9270482659339905, loss : 0.1964767575263977\n",
      "4030 train complete -- accruacy : 0.9781637787818909, loss : 0.07663369923830032\n",
      "4030 test complete == accruacy : 0.9236812591552734, loss : 0.21580186486244202\n",
      "4040 train complete -- accruacy : 0.9663366079330444, loss : 0.09766335040330887\n",
      "4040 test complete == accruacy : 0.9259259104728699, loss : 0.21677298843860626\n",
      "4050 train complete -- accruacy : 0.970370352268219, loss : 0.08694985508918762\n",
      "4050 test complete == accruacy : 0.9135802388191223, loss : 0.22625724971294403\n",
      "4060 train complete -- accruacy : 0.9768472909927368, loss : 0.06952357292175293\n",
      "4060 test complete == accruacy : 0.9236812591552734, loss : 0.2359178364276886\n",
      "4070 train complete -- accruacy : 0.9707616567611694, loss : 0.08594965189695358\n",
      "4070 test complete == accruacy : 0.9281706213951111, loss : 0.19131913781166077\n",
      "4080 train complete -- accruacy : 0.9715686440467834, loss : 0.08003337681293488\n",
      "4080 test complete == accruacy : 0.9337822794914246, loss : 0.18701006472110748\n",
      "4090 train complete -- accruacy : 0.9674816727638245, loss : 0.09466033428907394\n",
      "4090 test complete == accruacy : 0.9124578833580017, loss : 0.2740447521209717\n",
      "4100 train complete -- accruacy : 0.9719512462615967, loss : 0.0845244899392128\n",
      "4100 test complete == accruacy : 0.924803614616394, loss : 0.24514620006084442\n",
      "4110 train complete -- accruacy : 0.9788321256637573, loss : 0.07543165981769562\n",
      "4110 test complete == accruacy : 0.9450055956840515, loss : 0.15703780949115753\n",
      "4120 train complete -- accruacy : 0.974271833896637, loss : 0.07468289881944656\n",
      "4120 test complete == accruacy : 0.9203142523765564, loss : 0.21218369901180267\n",
      "4130 train complete -- accruacy : 0.9707021713256836, loss : 0.0971774235367775\n",
      "4130 test complete == accruacy : 0.9079685807228088, loss : 0.27326151728630066\n",
      "4140 train complete -- accruacy : 0.9705314040184021, loss : 0.08850538730621338\n",
      "4140 test complete == accruacy : 0.9382715821266174, loss : 0.17596812546253204\n",
      "4150 train complete -- accruacy : 0.9725301265716553, loss : 0.07931895554065704\n",
      "4150 test complete == accruacy : 0.9461279511451721, loss : 0.1643783450126648\n",
      "4160 train complete -- accruacy : 0.9838942289352417, loss : 0.062463708221912384\n",
      "4160 test complete == accruacy : 0.9349045753479004, loss : 0.20377463102340698\n",
      "4170 train complete -- accruacy : 0.9666666388511658, loss : 0.09373806416988373\n",
      "4170 test complete == accruacy : 0.9349045753479004, loss : 0.20264123380184174\n",
      "4180 train complete -- accruacy : 0.9777511954307556, loss : 0.07783761620521545\n",
      "4180 test complete == accruacy : 0.9405162930488586, loss : 0.17644591629505157\n",
      "4190 train complete -- accruacy : 0.9527446031570435, loss : 0.14343790709972382\n",
      "4190 test complete == accruacy : 0.9270482659339905, loss : 0.1976558417081833\n",
      "4200 train complete -- accruacy : 0.9671428799629211, loss : 0.10059846192598343\n",
      "4200 test complete == accruacy : 0.9001122117042542, loss : 0.2752043902873993\n",
      "4210 train complete -- accruacy : 0.9750593900680542, loss : 0.07319500297307968\n",
      "4210 test complete == accruacy : 0.9506173133850098, loss : 0.1585131287574768\n",
      "4220 train complete -- accruacy : 0.9770142436027527, loss : 0.07821670174598694\n",
      "4220 test complete == accruacy : 0.9349045753479004, loss : 0.16797642409801483\n",
      "4230 train complete -- accruacy : 0.9865248203277588, loss : 0.04883282631635666\n",
      "4230 test complete == accruacy : 0.9517396092414856, loss : 0.14540313184261322\n",
      "4240 train complete -- accruacy : 0.972169816493988, loss : 0.08156443387269974\n",
      "4240 test complete == accruacy : 0.9461279511451721, loss : 0.17738260328769684\n",
      "4250 train complete -- accruacy : 0.9785882234573364, loss : 0.07121143490076065\n",
      "4250 test complete == accruacy : 0.9304152727127075, loss : 0.26161807775497437\n",
      "4260 train complete -- accruacy : 0.9570422768592834, loss : 0.14524991810321808\n",
      "4260 test complete == accruacy : 0.9292929172515869, loss : 0.22806045413017273\n",
      "4270 train complete -- accruacy : 0.9594847559928894, loss : 0.1182311475276947\n",
      "4270 test complete == accruacy : 0.924803614616394, loss : 0.20723295211791992\n",
      "4280 train complete -- accruacy : 0.9731308221817017, loss : 0.08075981587171555\n",
      "4280 test complete == accruacy : 0.9337822794914246, loss : 0.16693846881389618\n",
      "4290 train complete -- accruacy : 0.9755244851112366, loss : 0.07958855479955673\n",
      "4290 test complete == accruacy : 0.9528619647026062, loss : 0.14265353977680206\n",
      "4300 train complete -- accruacy : 0.9644185900688171, loss : 0.10211572051048279\n",
      "4300 test complete == accruacy : 0.9203142523765564, loss : 0.19985197484493256\n",
      "4310 train complete -- accruacy : 0.9791183471679688, loss : 0.06391912698745728\n",
      "4310 test complete == accruacy : 0.9450055956840515, loss : 0.16979029774665833\n",
      "4320 train complete -- accruacy : 0.9736111164093018, loss : 0.0847243219614029\n",
      "4320 test complete == accruacy : 0.9427609443664551, loss : 0.1711077094078064\n",
      "4330 train complete -- accruacy : 0.9773672223091125, loss : 0.07173985242843628\n",
      "4330 test complete == accruacy : 0.9472503066062927, loss : 0.16199113428592682\n",
      "4340 train complete -- accruacy : 0.9739631414413452, loss : 0.08147504925727844\n",
      "4340 test complete == accruacy : 0.9382715821266174, loss : 0.1875862032175064\n",
      "4350 train complete -- accruacy : 0.978620707988739, loss : 0.07553288340568542\n",
      "4350 test complete == accruacy : 0.9337822794914246, loss : 0.20219293236732483\n",
      "4360 train complete -- accruacy : 0.9683486223220825, loss : 0.09134967625141144\n",
      "4360 test complete == accruacy : 0.9382715821266174, loss : 0.18199585378170013\n",
      "4370 train complete -- accruacy : 0.9764301776885986, loss : 0.07371603697538376\n",
      "4370 test complete == accruacy : 0.9349045753479004, loss : 0.16852951049804688\n",
      "4380 train complete -- accruacy : 0.9812785387039185, loss : 0.06430505216121674\n",
      "4380 test complete == accruacy : 0.9562289714813232, loss : 0.1524313986301422\n",
      "4390 train complete -- accruacy : 0.9719817638397217, loss : 0.0840940773487091\n",
      "4390 test complete == accruacy : 0.9494949579238892, loss : 0.15908494591712952\n",
      "4400 train complete -- accruacy : 0.9674999713897705, loss : 0.0922369733452797\n",
      "4400 test complete == accruacy : 0.9483726024627686, loss : 0.15606807172298431\n",
      "4410 train complete -- accruacy : 0.9782313108444214, loss : 0.07167673110961914\n",
      "4410 test complete == accruacy : 0.9270482659339905, loss : 0.22486938536167145\n",
      "4420 train complete -- accruacy : 0.9782805442810059, loss : 0.07110193371772766\n",
      "4420 test complete == accruacy : 0.9259259104728699, loss : 0.20669157803058624\n",
      "4430 train complete -- accruacy : 0.9693002104759216, loss : 0.09537681937217712\n",
      "4430 test complete == accruacy : 0.9203142523765564, loss : 0.20027785003185272\n",
      "4440 train complete -- accruacy : 0.98153156042099, loss : 0.061947405338287354\n",
      "4440 test complete == accruacy : 0.9438832998275757, loss : 0.20053823292255402\n",
      "4450 train complete -- accruacy : 0.9800000190734863, loss : 0.06623706966638565\n",
      "4450 test complete == accruacy : 0.9259259104728699, loss : 0.21386057138442993\n",
      "4460 train complete -- accruacy : 0.9795964360237122, loss : 0.06522222608327866\n",
      "4460 test complete == accruacy : 0.9528619647026062, loss : 0.14294375479221344\n",
      "4470 train complete -- accruacy : 0.9791946411132812, loss : 0.07096116244792938\n",
      "4470 test complete == accruacy : 0.9416385889053345, loss : 0.21996062994003296\n",
      "4480 train complete -- accruacy : 0.9651785492897034, loss : 0.09962552040815353\n",
      "4480 test complete == accruacy : 0.9427609443664551, loss : 0.15812644362449646\n",
      "4490 train complete -- accruacy : 0.9730512499809265, loss : 0.07297047972679138\n",
      "4490 test complete == accruacy : 0.9461279511451721, loss : 0.1686781495809555\n",
      "4500 train complete -- accruacy : 0.9788888692855835, loss : 0.06550775468349457\n",
      "4500 test complete == accruacy : 0.9382715821266174, loss : 0.18344928324222565\n",
      "4510 train complete -- accruacy : 0.9753880500793457, loss : 0.07633165270090103\n",
      "4510 test complete == accruacy : 0.9438832998275757, loss : 0.13779327273368835\n",
      "4520 train complete -- accruacy : 0.9692477583885193, loss : 0.0895179882645607\n",
      "4520 test complete == accruacy : 0.932659924030304, loss : 0.20510713756084442\n",
      "4530 train complete -- accruacy : 0.9788079261779785, loss : 0.06516384333372116\n",
      "4530 test complete == accruacy : 0.9517396092414856, loss : 0.15359875559806824\n",
      "4540 train complete -- accruacy : 0.9786343574523926, loss : 0.06998702883720398\n",
      "4540 test complete == accruacy : 0.924803614616394, loss : 0.20815075933933258\n",
      "4550 train complete -- accruacy : 0.9817582368850708, loss : 0.05724960193037987\n",
      "4550 test complete == accruacy : 0.9595959782600403, loss : 0.1186094805598259\n",
      "4560 train complete -- accruacy : 0.9835526347160339, loss : 0.057404011487960815\n",
      "4560 test complete == accruacy : 0.9685746431350708, loss : 0.11586689203977585\n",
      "4570 train complete -- accruacy : 0.9827133417129517, loss : 0.05782299116253853\n",
      "4570 test complete == accruacy : 0.9270482659339905, loss : 0.2094523310661316\n",
      "4580 train complete -- accruacy : 0.9751091599464417, loss : 0.07131849229335785\n",
      "4580 test complete == accruacy : 0.8911335468292236, loss : 0.32249459624290466\n",
      "4590 train complete -- accruacy : 0.9640522599220276, loss : 0.1042577475309372\n",
      "4590 test complete == accruacy : 0.9405162930488586, loss : 0.16908882558345795\n",
      "4600 train complete -- accruacy : 0.9808695912361145, loss : 0.0647769644856453\n",
      "4600 test complete == accruacy : 0.939393937587738, loss : 0.15251721441745758\n",
      "4610 train complete -- accruacy : 0.9793926477432251, loss : 0.061343032866716385\n",
      "4610 test complete == accruacy : 0.936026930809021, loss : 0.18424250185489655\n",
      "4620 train complete -- accruacy : 0.9813852906227112, loss : 0.06211414560675621\n",
      "4620 test complete == accruacy : 0.9472503066062927, loss : 0.13726598024368286\n",
      "4630 train complete -- accruacy : 0.978185772895813, loss : 0.06568389385938644\n",
      "4630 test complete == accruacy : 0.9584736227989197, loss : 0.14324936270713806\n",
      "4640 train complete -- accruacy : 0.9801723957061768, loss : 0.06442847102880478\n",
      "4640 test complete == accruacy : 0.9147025942802429, loss : 0.2312086522579193\n",
      "4650 train complete -- accruacy : 0.9686021208763123, loss : 0.09636814892292023\n",
      "4650 test complete == accruacy : 0.9506173133850098, loss : 0.1288381665945053\n",
      "4660 train complete -- accruacy : 0.9886265993118286, loss : 0.04861719161272049\n",
      "4660 test complete == accruacy : 0.9483726024627686, loss : 0.15535366535186768\n",
      "4670 train complete -- accruacy : 0.9788008332252502, loss : 0.07123284041881561\n",
      "4670 test complete == accruacy : 0.9506173133850098, loss : 0.14446505904197693\n",
      "4680 train complete -- accruacy : 0.9724358916282654, loss : 0.08282998949289322\n",
      "4680 test complete == accruacy : 0.9405162930488586, loss : 0.1643274426460266\n",
      "4690 train complete -- accruacy : 0.983582079410553, loss : 0.060423340648412704\n",
      "4690 test complete == accruacy : 0.9483726024627686, loss : 0.1600322276353836\n",
      "4700 train complete -- accruacy : 0.976808488368988, loss : 0.0703682154417038\n",
      "4700 test complete == accruacy : 0.9494949579238892, loss : 0.16017761826515198\n",
      "4710 train complete -- accruacy : 0.9770700931549072, loss : 0.06732717901468277\n",
      "4710 test complete == accruacy : 0.9461279511451721, loss : 0.1646553874015808\n",
      "4720 train complete -- accruacy : 0.9762712121009827, loss : 0.07200866937637329\n",
      "4720 test complete == accruacy : 0.939393937587738, loss : 0.16652171313762665\n",
      "4730 train complete -- accruacy : 0.9782240986824036, loss : 0.07484384626150131\n",
      "4730 test complete == accruacy : 0.9506173133850098, loss : 0.1430744081735611\n",
      "4740 train complete -- accruacy : 0.9729957580566406, loss : 0.08063196390867233\n",
      "4740 test complete == accruacy : 0.9494949579238892, loss : 0.17899543046951294\n",
      "4750 train complete -- accruacy : 0.972631573677063, loss : 0.07961653918027878\n",
      "4750 test complete == accruacy : 0.9416385889053345, loss : 0.1604851931333542\n",
      "4760 train complete -- accruacy : 0.9728991389274597, loss : 0.08253026753664017\n",
      "4760 test complete == accruacy : 0.9494949579238892, loss : 0.15017099678516388\n",
      "4770 train complete -- accruacy : 0.9792453050613403, loss : 0.07350028306245804\n",
      "4770 test complete == accruacy : 0.9584736227989197, loss : 0.14645807445049286\n",
      "4780 train complete -- accruacy : 0.9740585684776306, loss : 0.07880417257547379\n",
      "4780 test complete == accruacy : 0.9494949579238892, loss : 0.1510004997253418\n",
      "4790 train complete -- accruacy : 0.9803757667541504, loss : 0.0696554183959961\n",
      "4790 test complete == accruacy : 0.9461279511451721, loss : 0.14575234055519104\n",
      "4800 train complete -- accruacy : 0.9697916507720947, loss : 0.099943146109581\n",
      "4800 test complete == accruacy : 0.939393937587738, loss : 0.1710689216852188\n",
      "4810 train complete -- accruacy : 0.9819126725196838, loss : 0.05809769779443741\n",
      "4810 test complete == accruacy : 0.9607182741165161, loss : 0.11626554280519485\n",
      "4820 train complete -- accruacy : 0.9831950068473816, loss : 0.057075463235378265\n",
      "4820 test complete == accruacy : 0.9450055956840515, loss : 0.18421077728271484\n",
      "4830 train complete -- accruacy : 0.9724637866020203, loss : 0.08821266889572144\n",
      "4830 test complete == accruacy : 0.9607182741165161, loss : 0.11100002378225327\n",
      "4840 train complete -- accruacy : 0.9727272987365723, loss : 0.08101280778646469\n",
      "4840 test complete == accruacy : 0.9607182741165161, loss : 0.13730444014072418\n",
      "4850 train complete -- accruacy : 0.9752577543258667, loss : 0.0769842118024826\n",
      "4850 test complete == accruacy : 0.9147025942802429, loss : 0.2785940170288086\n",
      "4860 train complete -- accruacy : 0.9740740656852722, loss : 0.0816684141755104\n",
      "4860 test complete == accruacy : 0.9517396092414856, loss : 0.13770118355751038\n",
      "4870 train complete -- accruacy : 0.9841889142990112, loss : 0.0566522479057312\n",
      "4870 test complete == accruacy : 0.9371492862701416, loss : 0.19546712934970856\n",
      "4880 train complete -- accruacy : 0.9758196473121643, loss : 0.07484984397888184\n",
      "4880 test complete == accruacy : 0.9292929172515869, loss : 0.19918492436408997\n",
      "4890 train complete -- accruacy : 0.9852761030197144, loss : 0.05613769590854645\n",
      "4890 test complete == accruacy : 0.9618406295776367, loss : 0.10539134591817856\n",
      "4900 train complete -- accruacy : 0.9836734533309937, loss : 0.05571862682700157\n",
      "4900 test complete == accruacy : 0.9562289714813232, loss : 0.13358469307422638\n",
      "4910 train complete -- accruacy : 0.9832993745803833, loss : 0.0579560324549675\n",
      "4910 test complete == accruacy : 0.9607182741165161, loss : 0.09945684671401978\n",
      "4920 train complete -- accruacy : 0.9676828980445862, loss : 0.09510700404644012\n",
      "4920 test complete == accruacy : 0.953984260559082, loss : 0.15450483560562134\n",
      "4930 train complete -- accruacy : 0.9884381294250488, loss : 0.04374745115637779\n",
      "4930 test complete == accruacy : 0.9551066160202026, loss : 0.1245817169547081\n",
      "4940 train complete -- accruacy : 0.9775303602218628, loss : 0.06747085601091385\n",
      "4940 test complete == accruacy : 0.9472503066062927, loss : 0.14760862290859222\n",
      "4950 train complete -- accruacy : 0.9789898991584778, loss : 0.06434089690446854\n",
      "4950 test complete == accruacy : 0.9640852808952332, loss : 0.11371440440416336\n",
      "4960 train complete -- accruacy : 0.9828628897666931, loss : 0.05493843927979469\n",
      "4960 test complete == accruacy : 0.9595959782600403, loss : 0.12163201719522476\n",
      "4970 train complete -- accruacy : 0.9824949502944946, loss : 0.06152602285146713\n",
      "4970 test complete == accruacy : 0.9595959782600403, loss : 0.1480405330657959\n",
      "4980 train complete -- accruacy : 0.977911651134491, loss : 0.06692685931921005\n",
      "4980 test complete == accruacy : 0.9349045753479004, loss : 0.1758350133895874\n",
      "4990 train complete -- accruacy : 0.9875751733779907, loss : 0.04716234654188156\n",
      "4990 test complete == accruacy : 0.9427609443664551, loss : 0.18891708552837372\n",
      "5000 train complete -- accruacy : 0.9721999764442444, loss : 0.090605728328228\n",
      "5000 test complete == accruacy : 0.9517396092414856, loss : 0.13022330403327942\n",
      "5010 train complete -- accruacy : 0.9702594876289368, loss : 0.08857161551713943\n",
      "5010 test complete == accruacy : 0.9506173133850098, loss : 0.14662861824035645\n",
      "5020 train complete -- accruacy : 0.9747011661529541, loss : 0.07912767678499222\n",
      "5020 test complete == accruacy : 0.9472503066062927, loss : 0.14826713502407074\n",
      "5030 train complete -- accruacy : 0.9886679649353027, loss : 0.041914407163858414\n",
      "5030 test complete == accruacy : 0.9528619647026062, loss : 0.16206203401088715\n",
      "5040 train complete -- accruacy : 0.9708333611488342, loss : 0.09240829199552536\n",
      "5040 test complete == accruacy : 0.936026930809021, loss : 0.17549826204776764\n",
      "5050 train complete -- accruacy : 0.9798019528388977, loss : 0.06350003182888031\n",
      "5050 test complete == accruacy : 0.9528619647026062, loss : 0.15395207703113556\n",
      "5060 train complete -- accruacy : 0.9754940867424011, loss : 0.08720661699771881\n",
      "5060 test complete == accruacy : 0.9461279511451721, loss : 0.16877780854701996\n",
      "5070 train complete -- accruacy : 0.9708086848258972, loss : 0.08626075088977814\n",
      "5070 test complete == accruacy : 0.9607182741165161, loss : 0.12017512321472168\n",
      "5080 train complete -- accruacy : 0.9899606108665466, loss : 0.03593085706233978\n",
      "5080 test complete == accruacy : 0.9696969985961914, loss : 0.10073338449001312\n",
      "5090 train complete -- accruacy : 0.9838899970054626, loss : 0.05588169023394585\n",
      "5090 test complete == accruacy : 0.936026930809021, loss : 0.17695042490959167\n",
      "5100 train complete -- accruacy : 0.9815686345100403, loss : 0.05931276082992554\n",
      "5100 test complete == accruacy : 0.9685746431350708, loss : 0.11810256540775299\n",
      "5110 train complete -- accruacy : 0.9778864979743958, loss : 0.06829868257045746\n",
      "5110 test complete == accruacy : 0.9494949579238892, loss : 0.13670186698436737\n",
      "5120 train complete -- accruacy : 0.9775390625, loss : 0.07175184786319733\n",
      "5120 test complete == accruacy : 0.9517396092414856, loss : 0.13309581577777863\n",
      "5130 train complete -- accruacy : 0.979337215423584, loss : 0.06492186337709427\n",
      "5130 test complete == accruacy : 0.9584736227989197, loss : 0.13994130492210388\n",
      "5140 train complete -- accruacy : 0.9813229441642761, loss : 0.0645279586315155\n",
      "5140 test complete == accruacy : 0.9551066160202026, loss : 0.14857299625873566\n",
      "5150 train complete -- accruacy : 0.9836893081665039, loss : 0.06034308299422264\n",
      "5150 test complete == accruacy : 0.9584736227989197, loss : 0.11723621934652328\n",
      "5160 train complete -- accruacy : 0.9870154857635498, loss : 0.04567088931798935\n",
      "5160 test complete == accruacy : 0.9640852808952332, loss : 0.10869276523590088\n",
      "5170 train complete -- accruacy : 0.9839458465576172, loss : 0.0494048111140728\n",
      "5170 test complete == accruacy : 0.9506173133850098, loss : 0.13777552545070648\n",
      "5180 train complete -- accruacy : 0.9857142567634583, loss : 0.050092920660972595\n",
      "5180 test complete == accruacy : 0.9674522876739502, loss : 0.09391888976097107\n",
      "5190 train complete -- accruacy : 0.9699422121047974, loss : 0.09509912133216858\n",
      "5190 test complete == accruacy : 0.9461279511451721, loss : 0.18174315989017487\n",
      "5200 train complete -- accruacy : 0.9801923036575317, loss : 0.06292296200990677\n",
      "5200 test complete == accruacy : 0.9607182741165161, loss : 0.13135547935962677\n",
      "5210 train complete -- accruacy : 0.9756237864494324, loss : 0.07789573073387146\n",
      "5210 test complete == accruacy : 0.939393937587738, loss : 0.16063044965267181\n",
      "5220 train complete -- accruacy : 0.9858237504959106, loss : 0.04930870234966278\n",
      "5220 test complete == accruacy : 0.9640852808952332, loss : 0.1197783499956131\n",
      "5230 train complete -- accruacy : 0.9686424732208252, loss : 0.08868613839149475\n",
      "5230 test complete == accruacy : 0.9584736227989197, loss : 0.1469413787126541\n",
      "5240 train complete -- accruacy : 0.9770992398262024, loss : 0.07365944981575012\n",
      "5240 test complete == accruacy : 0.9405162930488586, loss : 0.13401326537132263\n",
      "5250 train complete -- accruacy : 0.9874285459518433, loss : 0.03998406231403351\n",
      "5250 test complete == accruacy : 0.9618406295776367, loss : 0.12185175716876984\n",
      "5260 train complete -- accruacy : 0.980038046836853, loss : 0.061109669506549835\n",
      "5260 test complete == accruacy : 0.9595959782600403, loss : 0.12746207416057587\n",
      "5270 train complete -- accruacy : 0.9781783819198608, loss : 0.07224588096141815\n",
      "5270 test complete == accruacy : 0.9517396092414856, loss : 0.14291489124298096\n",
      "5280 train complete -- accruacy : 0.9816287755966187, loss : 0.05505727604031563\n",
      "5280 test complete == accruacy : 0.9607182741165161, loss : 0.10496538132429123\n",
      "5290 train complete -- accruacy : 0.9775047302246094, loss : 0.06685630977153778\n",
      "5290 test complete == accruacy : 0.9517396092414856, loss : 0.1402105838060379\n",
      "5300 train complete -- accruacy : 0.9828301668167114, loss : 0.052560776472091675\n",
      "5300 test complete == accruacy : 0.939393937587738, loss : 0.17519612610340118\n",
      "5310 train complete -- accruacy : 0.9796609878540039, loss : 0.07141663879156113\n",
      "5310 test complete == accruacy : 0.9472503066062927, loss : 0.1267985850572586\n",
      "5320 train complete -- accruacy : 0.9815789461135864, loss : 0.06095864996314049\n",
      "5320 test complete == accruacy : 0.9450055956840515, loss : 0.16744297742843628\n",
      "5330 train complete -- accruacy : 0.9855534434318542, loss : 0.05211230367422104\n",
      "5330 test complete == accruacy : 0.9461279511451721, loss : 0.1430409699678421\n",
      "5340 train complete -- accruacy : 0.987265944480896, loss : 0.044635508209466934\n",
      "5340 test complete == accruacy : 0.9584736227989197, loss : 0.11708024144172668\n",
      "5350 train complete -- accruacy : 0.9751402139663696, loss : 0.07684087753295898\n",
      "5350 test complete == accruacy : 0.953984260559082, loss : 0.13895052671432495\n",
      "5360 train complete -- accruacy : 0.9863805770874023, loss : 0.04883286729454994\n",
      "5360 test complete == accruacy : 0.939393937587738, loss : 0.16785500943660736\n",
      "5370 train complete -- accruacy : 0.9808193445205688, loss : 0.06204180419445038\n",
      "5370 test complete == accruacy : 0.9696969985961914, loss : 0.09821480512619019\n",
      "5380 train complete -- accruacy : 0.9732341766357422, loss : 0.07956822961568832\n",
      "5380 test complete == accruacy : 0.9438832998275757, loss : 0.13420748710632324\n",
      "5390 train complete -- accruacy : 0.9833024144172668, loss : 0.057052262127399445\n",
      "5390 test complete == accruacy : 0.9337822794914246, loss : 0.19098441302776337\n",
      "5400 train complete -- accruacy : 0.978518545627594, loss : 0.07112414389848709\n",
      "5400 test complete == accruacy : 0.9371492862701416, loss : 0.18042878806591034\n",
      "5410 train complete -- accruacy : 0.980406641960144, loss : 0.0644196942448616\n",
      "5410 test complete == accruacy : 0.9573512673377991, loss : 0.11435535550117493\n",
      "5420 train complete -- accruacy : 0.9688192009925842, loss : 0.08366630226373672\n",
      "5420 test complete == accruacy : 0.9416385889053345, loss : 0.20122861862182617\n",
      "5430 train complete -- accruacy : 0.9848986864089966, loss : 0.04989833012223244\n",
      "5430 test complete == accruacy : 0.9674522876739502, loss : 0.12756775319576263\n",
      "5440 train complete -- accruacy : 0.9845588207244873, loss : 0.043367668986320496\n",
      "5440 test complete == accruacy : 0.9461279511451721, loss : 0.17625810205936432\n",
      "5450 train complete -- accruacy : 0.9827523231506348, loss : 0.05319184809923172\n",
      "5450 test complete == accruacy : 0.9551066160202026, loss : 0.14328667521476746\n",
      "5460 train complete -- accruacy : 0.988095223903656, loss : 0.044636763632297516\n",
      "5460 test complete == accruacy : 0.9405162930488586, loss : 0.2009574919939041\n",
      "5470 train complete -- accruacy : 0.9802559614181519, loss : 0.0659727081656456\n",
      "5470 test complete == accruacy : 0.953984260559082, loss : 0.1282409131526947\n",
      "5480 train complete -- accruacy : 0.9779196977615356, loss : 0.07161729037761688\n",
      "5480 test complete == accruacy : 0.9584736227989197, loss : 0.12502224743366241\n",
      "5490 train complete -- accruacy : 0.9834244251251221, loss : 0.05461752042174339\n",
      "5490 test complete == accruacy : 0.9584736227989197, loss : 0.12369539588689804\n",
      "5500 train complete -- accruacy : 0.9860000014305115, loss : 0.05077609419822693\n",
      "5500 test complete == accruacy : 0.9663299918174744, loss : 0.11393299698829651\n",
      "5510 train complete -- accruacy : 0.9822141528129578, loss : 0.059178490191698074\n",
      "5510 test complete == accruacy : 0.9551066160202026, loss : 0.11921817064285278\n",
      "5520 train complete -- accruacy : 0.9838768243789673, loss : 0.051409877836704254\n",
      "5520 test complete == accruacy : 0.9584736227989197, loss : 0.1381085067987442\n",
      "5530 train complete -- accruacy : 0.9761301875114441, loss : 0.07083000987768173\n",
      "5530 test complete == accruacy : 0.9337822794914246, loss : 0.20256195962429047\n",
      "5540 train complete -- accruacy : 0.9823104739189148, loss : 0.059745706617832184\n",
      "5540 test complete == accruacy : 0.9551066160202026, loss : 0.11877493560314178\n",
      "5550 train complete -- accruacy : 0.9855855703353882, loss : 0.04673302546143532\n",
      "5550 test complete == accruacy : 0.9573512673377991, loss : 0.12935252487659454\n",
      "5560 train complete -- accruacy : 0.9827337861061096, loss : 0.05264078080654144\n",
      "5560 test complete == accruacy : 0.9674522876739502, loss : 0.11927227675914764\n",
      "5570 train complete -- accruacy : 0.983662486076355, loss : 0.06024155765771866\n",
      "5570 test complete == accruacy : 0.9438832998275757, loss : 0.1755896657705307\n",
      "5580 train complete -- accruacy : 0.9849462509155273, loss : 0.05053582787513733\n",
      "5580 test complete == accruacy : 0.9685746431350708, loss : 0.10790317505598068\n",
      "5590 train complete -- accruacy : 0.9846153855323792, loss : 0.05082138627767563\n",
      "5590 test complete == accruacy : 0.9517396092414856, loss : 0.12230387330055237\n",
      "5600 train complete -- accruacy : 0.9857142567634583, loss : 0.049255598336458206\n",
      "5600 test complete == accruacy : 0.9741863012313843, loss : 0.10024093091487885\n",
      "5610 train complete -- accruacy : 0.9866310358047485, loss : 0.04774194583296776\n",
      "5610 test complete == accruacy : 0.9652076363563538, loss : 0.12074774503707886\n",
      "5620 train complete -- accruacy : 0.9818505048751831, loss : 0.05813341960310936\n",
      "5620 test complete == accruacy : 0.9416385889053345, loss : 0.18737177550792694\n",
      "5630 train complete -- accruacy : 0.9824156165122986, loss : 0.05841975286602974\n",
      "5630 test complete == accruacy : 0.9483726024627686, loss : 0.18020494282245636\n",
      "5640 train complete -- accruacy : 0.9822695255279541, loss : 0.0649210512638092\n",
      "5640 test complete == accruacy : 0.9640852808952332, loss : 0.1356535404920578\n",
      "5650 train complete -- accruacy : 0.9805309772491455, loss : 0.06732065975666046\n",
      "5650 test complete == accruacy : 0.9584736227989197, loss : 0.1346750259399414\n",
      "5660 train complete -- accruacy : 0.9856890439987183, loss : 0.053134575486183167\n",
      "5660 test complete == accruacy : 0.9652076363563538, loss : 0.1196795180439949\n",
      "5670 train complete -- accruacy : 0.980246901512146, loss : 0.05872936174273491\n",
      "5670 test complete == accruacy : 0.9584736227989197, loss : 0.11117043346166611\n",
      "5680 train complete -- accruacy : 0.9815140962600708, loss : 0.0546945258975029\n",
      "5680 test complete == accruacy : 0.9595959782600403, loss : 0.10186343640089035\n",
      "5690 train complete -- accruacy : 0.9725834727287292, loss : 0.0840344950556755\n",
      "5690 test complete == accruacy : 0.9663299918174744, loss : 0.09701289981603622\n",
      "5700 train complete -- accruacy : 0.9785965085029602, loss : 0.06907546520233154\n",
      "5700 test complete == accruacy : 0.9573512673377991, loss : 0.170802503824234\n",
      "5710 train complete -- accruacy : 0.9835376739501953, loss : 0.05314008891582489\n",
      "5710 test complete == accruacy : 0.9472503066062927, loss : 0.16396118700504303\n",
      "5720 train complete -- accruacy : 0.9767482280731201, loss : 0.06919418275356293\n",
      "5720 test complete == accruacy : 0.9450055956840515, loss : 0.16481560468673706\n",
      "5730 train complete -- accruacy : 0.9898778200149536, loss : 0.038300786167383194\n",
      "5730 test complete == accruacy : 0.9674522876739502, loss : 0.1251312643289566\n",
      "5740 train complete -- accruacy : 0.9831010699272156, loss : 0.05625375732779503\n",
      "5740 test complete == accruacy : 0.9528619647026062, loss : 0.1628999412059784\n",
      "5750 train complete -- accruacy : 0.9806956648826599, loss : 0.06171267852187157\n",
      "5750 test complete == accruacy : 0.9629629850387573, loss : 0.12760545313358307\n",
      "5760 train complete -- accruacy : 0.9871527552604675, loss : 0.04283025115728378\n",
      "5760 test complete == accruacy : 0.9562289714813232, loss : 0.13235075771808624\n",
      "5770 train complete -- accruacy : 0.9698440432548523, loss : 0.08697530627250671\n",
      "5770 test complete == accruacy : 0.9595959782600403, loss : 0.12493754178285599\n",
      "5780 train complete -- accruacy : 0.9820069074630737, loss : 0.054525356739759445\n",
      "5780 test complete == accruacy : 0.9584736227989197, loss : 0.14108525216579437\n",
      "5790 train complete -- accruacy : 0.9851468205451965, loss : 0.046091631054878235\n",
      "5790 test complete == accruacy : 0.9629629850387573, loss : 0.11892689019441605\n",
      "5800 train complete -- accruacy : 0.990517258644104, loss : 0.032616082578897476\n",
      "5800 test complete == accruacy : 0.9517396092414856, loss : 0.1454390585422516\n",
      "5810 train complete -- accruacy : 0.9857142567634583, loss : 0.051781587302684784\n",
      "5810 test complete == accruacy : 0.9708192944526672, loss : 0.10117945820093155\n",
      "5820 train complete -- accruacy : 0.9781786799430847, loss : 0.07081306725740433\n",
      "5820 test complete == accruacy : 0.9573512673377991, loss : 0.14556440711021423\n",
      "5830 train complete -- accruacy : 0.9852486848831177, loss : 0.04948301240801811\n",
      "5830 test complete == accruacy : 0.9584736227989197, loss : 0.15983466804027557\n",
      "5840 train complete -- accruacy : 0.9832192063331604, loss : 0.05275413766503334\n",
      "5840 test complete == accruacy : 0.9551066160202026, loss : 0.13767686486244202\n",
      "5850 train complete -- accruacy : 0.9801709651947021, loss : 0.05878588929772377\n",
      "5850 test complete == accruacy : 0.9551066160202026, loss : 0.13981284201145172\n",
      "5860 train complete -- accruacy : 0.9836177229881287, loss : 0.05466866493225098\n",
      "5860 test complete == accruacy : 0.9786756634712219, loss : 0.08157613128423691\n",
      "5870 train complete -- accruacy : 0.9841567277908325, loss : 0.0500318706035614\n",
      "5870 test complete == accruacy : 0.9607182741165161, loss : 0.13109956681728363\n",
      "5880 train complete -- accruacy : 0.9840136170387268, loss : 0.04898476228117943\n",
      "5880 test complete == accruacy : 0.9584736227989197, loss : 0.16022957861423492\n",
      "5890 train complete -- accruacy : 0.9848896265029907, loss : 0.05417520925402641\n",
      "5890 test complete == accruacy : 0.9719416499137878, loss : 0.09109716862440109\n",
      "5900 train complete -- accruacy : 0.9806779623031616, loss : 0.06648694723844528\n",
      "5900 test complete == accruacy : 0.9438832998275757, loss : 0.15548495948314667\n",
      "5910 train complete -- accruacy : 0.9844331741333008, loss : 0.0500447079539299\n",
      "5910 test complete == accruacy : 0.9652076363563538, loss : 0.12596696615219116\n",
      "5920 train complete -- accruacy : 0.9890202879905701, loss : 0.04263995587825775\n",
      "5920 test complete == accruacy : 0.9640852808952332, loss : 0.11816073209047318\n",
      "5930 train complete -- accruacy : 0.9873524308204651, loss : 0.04559117183089256\n",
      "5930 test complete == accruacy : 0.9584736227989197, loss : 0.15016336739063263\n",
      "5940 train complete -- accruacy : 0.978787899017334, loss : 0.07206611335277557\n",
      "5940 test complete == accruacy : 0.9685746431350708, loss : 0.08846405893564224\n",
      "5950 train complete -- accruacy : 0.9875630140304565, loss : 0.04745211452245712\n",
      "5950 test complete == accruacy : 0.9629629850387573, loss : 0.1371602714061737\n",
      "5960 train complete -- accruacy : 0.9874160885810852, loss : 0.038982853293418884\n",
      "5960 test complete == accruacy : 0.9674522876739502, loss : 0.11320126056671143\n",
      "5970 train complete -- accruacy : 0.9814070463180542, loss : 0.056442443281412125\n",
      "5970 test complete == accruacy : 0.9663299918174744, loss : 0.10459497570991516\n",
      "5980 train complete -- accruacy : 0.9795986413955688, loss : 0.06255978345870972\n",
      "5980 test complete == accruacy : 0.9528619647026062, loss : 0.13723599910736084\n",
      "5990 train complete -- accruacy : 0.9801335334777832, loss : 0.0626317709684372\n",
      "5990 test complete == accruacy : 0.9483726024627686, loss : 0.16610898077487946\n",
      "6000 train complete -- accruacy : 0.9909999966621399, loss : 0.03410112485289574\n",
      "6000 test complete == accruacy : 0.9618406295776367, loss : 0.0939142107963562\n",
      "6010 train complete -- accruacy : 0.9893510937690735, loss : 0.041311077773571014\n",
      "6010 test complete == accruacy : 0.9663299918174744, loss : 0.09082464873790741\n",
      "6020 train complete -- accruacy : 0.989700973033905, loss : 0.040089238435029984\n",
      "6020 test complete == accruacy : 0.9775533080101013, loss : 0.08325494080781937\n",
      "6030 train complete -- accruacy : 0.983913779258728, loss : 0.04905102401971817\n",
      "6030 test complete == accruacy : 0.9685746431350708, loss : 0.11293499171733856\n",
      "6040 train complete -- accruacy : 0.9894039630889893, loss : 0.037022724747657776\n",
      "6040 test complete == accruacy : 0.9685746431350708, loss : 0.14150694012641907\n",
      "6050 train complete -- accruacy : 0.9823140501976013, loss : 0.04981810972094536\n",
      "6050 test complete == accruacy : 0.9719416499137878, loss : 0.0841296911239624\n",
      "6060 train complete -- accruacy : 0.9896039366722107, loss : 0.04077855125069618\n",
      "6060 test complete == accruacy : 0.9696969985961914, loss : 0.09048352390527725\n",
      "6070 train complete -- accruacy : 0.984349250793457, loss : 0.050001874566078186\n",
      "6070 test complete == accruacy : 0.939393937587738, loss : 0.1886032521724701\n",
      "6080 train complete -- accruacy : 0.9904605150222778, loss : 0.03527037426829338\n",
      "6080 test complete == accruacy : 0.9584736227989197, loss : 0.12039367854595184\n",
      "6090 train complete -- accruacy : 0.9763546586036682, loss : 0.07530731707811356\n",
      "6090 test complete == accruacy : 0.9595959782600403, loss : 0.1530098170042038\n",
      "6100 train complete -- accruacy : 0.9824590086936951, loss : 0.05026153102517128\n",
      "6100 test complete == accruacy : 0.9450055956840515, loss : 0.1349092572927475\n",
      "6110 train complete -- accruacy : 0.9878886938095093, loss : 0.042241133749485016\n",
      "6110 test complete == accruacy : 0.9528619647026062, loss : 0.15516571700572968\n",
      "6120 train complete -- accruacy : 0.9848039150238037, loss : 0.04696826636791229\n",
      "6120 test complete == accruacy : 0.9674522876739502, loss : 0.11394326388835907\n",
      "6130 train complete -- accruacy : 0.9876019358634949, loss : 0.043429624289274216\n",
      "6130 test complete == accruacy : 0.9685746431350708, loss : 0.08810333907604218\n",
      "6140 train complete -- accruacy : 0.9899022579193115, loss : 0.03796575218439102\n",
      "6140 test complete == accruacy : 0.939393937587738, loss : 0.1811925321817398\n",
      "6150 train complete -- accruacy : 0.9830894470214844, loss : 0.053267329931259155\n",
      "6150 test complete == accruacy : 0.9371492862701416, loss : 0.1789533495903015\n",
      "6160 train complete -- accruacy : 0.9834415316581726, loss : 0.054504118859767914\n",
      "6160 test complete == accruacy : 0.9663299918174744, loss : 0.08831752091646194\n",
      "6170 train complete -- accruacy : 0.9899513721466064, loss : 0.037132181227207184\n",
      "6170 test complete == accruacy : 0.9652076363563538, loss : 0.09597679227590561\n",
      "6180 train complete -- accruacy : 0.9846278429031372, loss : 0.05012974888086319\n",
      "6180 test complete == accruacy : 0.9618406295776367, loss : 0.1346840113401413\n",
      "6190 train complete -- accruacy : 0.9775444269180298, loss : 0.06907700002193451\n",
      "6190 test complete == accruacy : 0.9573512673377991, loss : 0.12818437814712524\n",
      "6200 train complete -- accruacy : 0.9833871126174927, loss : 0.05606679245829582\n",
      "6200 test complete == accruacy : 0.9640852808952332, loss : 0.10676097124814987\n",
      "6210 train complete -- accruacy : 0.9735909700393677, loss : 0.07300576567649841\n",
      "6210 test complete == accruacy : 0.9461279511451721, loss : 0.15042702853679657\n",
      "6220 train complete -- accruacy : 0.9699357151985168, loss : 0.08999382704496384\n",
      "6220 test complete == accruacy : 0.9607182741165161, loss : 0.1268562227487564\n",
      "6230 train complete -- accruacy : 0.9791332483291626, loss : 0.06116136908531189\n",
      "6230 test complete == accruacy : 0.9618406295776367, loss : 0.101323202252388\n",
      "6240 train complete -- accruacy : 0.981249988079071, loss : 0.05535610392689705\n",
      "6240 test complete == accruacy : 0.9708192944526672, loss : 0.07992058247327805\n",
      "6250 train complete -- accruacy : 0.9824000000953674, loss : 0.05812232941389084\n",
      "6250 test complete == accruacy : 0.9472503066062927, loss : 0.14244692027568817\n",
      "6260 train complete -- accruacy : 0.984345018863678, loss : 0.056273043155670166\n",
      "6260 test complete == accruacy : 0.9450055956840515, loss : 0.18171359598636627\n",
      "6270 train complete -- accruacy : 0.9802232980728149, loss : 0.0636356994509697\n",
      "6270 test complete == accruacy : 0.9528619647026062, loss : 0.1449650675058365\n",
      "6280 train complete -- accruacy : 0.9909235835075378, loss : 0.033003851771354675\n",
      "6280 test complete == accruacy : 0.9696969985961914, loss : 0.08675345778465271\n",
      "6290 train complete -- accruacy : 0.981876015663147, loss : 0.05795783922076225\n",
      "6290 test complete == accruacy : 0.9528619647026062, loss : 0.13581734895706177\n",
      "6300 train complete -- accruacy : 0.9866666793823242, loss : 0.046214938163757324\n",
      "6300 test complete == accruacy : 0.9584736227989197, loss : 0.1188148707151413\n",
      "6310 train complete -- accruacy : 0.983993649482727, loss : 0.05403310805559158\n",
      "6310 test complete == accruacy : 0.9629629850387573, loss : 0.12869463860988617\n",
      "6320 train complete -- accruacy : 0.9909810423851013, loss : 0.031078623607754707\n",
      "6320 test complete == accruacy : 0.9696969985961914, loss : 0.10296826809644699\n",
      "6330 train complete -- accruacy : 0.9758293628692627, loss : 0.06987059116363525\n",
      "6330 test complete == accruacy : 0.9551066160202026, loss : 0.14941547811031342\n",
      "6340 train complete -- accruacy : 0.9761829376220703, loss : 0.0674014613032341\n",
      "6340 test complete == accruacy : 0.9629629850387573, loss : 0.12320216000080109\n",
      "6350 train complete -- accruacy : 0.9774802923202515, loss : 0.06130083277821541\n",
      "6350 test complete == accruacy : 0.9629629850387573, loss : 0.10667850077152252\n",
      "6360 train complete -- accruacy : 0.9790880680084229, loss : 0.06470075249671936\n",
      "6360 test complete == accruacy : 0.9573512673377991, loss : 0.14029447734355927\n",
      "6370 train complete -- accruacy : 0.9832025170326233, loss : 0.05541573464870453\n",
      "6370 test complete == accruacy : 0.9337822794914246, loss : 0.17581619322299957\n",
      "6380 train complete -- accruacy : 0.9822884202003479, loss : 0.05315076932311058\n",
      "6380 test complete == accruacy : 0.9528619647026062, loss : 0.12824088335037231\n",
      "6390 train complete -- accruacy : 0.9724569916725159, loss : 0.08347493410110474\n",
      "6390 test complete == accruacy : 0.9551066160202026, loss : 0.1369941681623459\n",
      "6400 train complete -- accruacy : 0.9893749952316284, loss : 0.035451408475637436\n",
      "6400 test complete == accruacy : 0.9753086566925049, loss : 0.09186483919620514\n",
      "6410 train complete -- accruacy : 0.9953197836875916, loss : 0.021526828408241272\n",
      "6410 test complete == accruacy : 0.9797979593276978, loss : 0.05914229899644852\n",
      "6420 train complete -- accruacy : 0.9923676252365112, loss : 0.030773956328630447\n",
      "6420 test complete == accruacy : 0.9708192944526672, loss : 0.08985131233930588\n",
      "6430 train complete -- accruacy : 0.9805598855018616, loss : 0.060877230018377304\n",
      "6430 test complete == accruacy : 0.9730639457702637, loss : 0.11157190799713135\n",
      "6440 train complete -- accruacy : 0.9795030951499939, loss : 0.06614205986261368\n",
      "6440 test complete == accruacy : 0.9584736227989197, loss : 0.12591537833213806\n",
      "6450 train complete -- accruacy : 0.9857364296913147, loss : 0.05254935473203659\n",
      "6450 test complete == accruacy : 0.9494949579238892, loss : 0.17664259672164917\n",
      "6460 train complete -- accruacy : 0.9955108165740967, loss : 0.02135101892054081\n",
      "6460 test complete == accruacy : 0.9517396092414856, loss : 0.14045070111751556\n",
      "6470 train complete -- accruacy : 0.9856259822845459, loss : 0.051490165293216705\n",
      "6470 test complete == accruacy : 0.9292929172515869, loss : 0.20395909249782562\n",
      "6480 train complete -- accruacy : 0.99043208360672, loss : 0.03716099634766579\n",
      "6480 test complete == accruacy : 0.9730639457702637, loss : 0.0937170535326004\n",
      "6490 train complete -- accruacy : 0.9835131168365479, loss : 0.050888847559690475\n",
      "6490 test complete == accruacy : 0.9607182741165161, loss : 0.10267417132854462\n",
      "6500 train complete -- accruacy : 0.9944615364074707, loss : 0.025528742000460625\n",
      "6500 test complete == accruacy : 0.9595959782600403, loss : 0.17498239874839783\n",
      "6510 train complete -- accruacy : 0.9852534532546997, loss : 0.05151450261473656\n",
      "6510 test complete == accruacy : 0.953984260559082, loss : 0.13465648889541626\n",
      "6520 train complete -- accruacy : 0.9809815883636475, loss : 0.06369302421808243\n",
      "6520 test complete == accruacy : 0.9573512673377991, loss : 0.14951905608177185\n",
      "6530 train complete -- accruacy : 0.9834609627723694, loss : 0.054782621562480927\n",
      "6530 test complete == accruacy : 0.9528619647026062, loss : 0.15545599162578583\n",
      "6540 train complete -- accruacy : 0.9775229096412659, loss : 0.06564387679100037\n",
      "6540 test complete == accruacy : 0.9629629850387573, loss : 0.10308948904275894\n",
      "6550 train complete -- accruacy : 0.9838168025016785, loss : 0.054570648819208145\n",
      "6550 test complete == accruacy : 0.9618406295776367, loss : 0.12385914474725723\n",
      "6560 train complete -- accruacy : 0.9817073345184326, loss : 0.05682006850838661\n",
      "6560 test complete == accruacy : 0.9708192944526672, loss : 0.09417816251516342\n",
      "6570 train complete -- accruacy : 0.987366795539856, loss : 0.04626919701695442\n",
      "6570 test complete == accruacy : 0.9528619647026062, loss : 0.1628246009349823\n",
      "6580 train complete -- accruacy : 0.983130693435669, loss : 0.057009655982255936\n",
      "6580 test complete == accruacy : 0.9730639457702637, loss : 0.07397568970918655\n",
      "6590 train complete -- accruacy : 0.9858877062797546, loss : 0.04542800784111023\n",
      "6590 test complete == accruacy : 0.9551066160202026, loss : 0.13611164689064026\n",
      "6600 train complete -- accruacy : 0.9781818389892578, loss : 0.06849458813667297\n",
      "6600 test complete == accruacy : 0.9719416499137878, loss : 0.102955661714077\n",
      "6610 train complete -- accruacy : 0.9792738556861877, loss : 0.06050926074385643\n",
      "6610 test complete == accruacy : 0.9618406295776367, loss : 0.12219682335853577\n",
      "6620 train complete -- accruacy : 0.9836857914924622, loss : 0.05234036222100258\n",
      "6620 test complete == accruacy : 0.9506173133850098, loss : 0.14161266386508942\n",
      "6630 train complete -- accruacy : 0.9846153855323792, loss : 0.04847830906510353\n",
      "6630 test complete == accruacy : 0.9573512673377991, loss : 0.14327660202980042\n",
      "6640 train complete -- accruacy : 0.9859939813613892, loss : 0.04729204252362251\n",
      "6640 test complete == accruacy : 0.9831649661064148, loss : 0.059205010533332825\n",
      "6650 train complete -- accruacy : 0.9852631688117981, loss : 0.047546543180942535\n",
      "6650 test complete == accruacy : 0.9696969985961914, loss : 0.08682090044021606\n",
      "6660 train complete -- accruacy : 0.9879879951477051, loss : 0.04382982850074768\n",
      "6660 test complete == accruacy : 0.9786756634712219, loss : 0.09268748760223389\n",
      "6670 train complete -- accruacy : 0.9899550080299377, loss : 0.036919478327035904\n",
      "6670 test complete == accruacy : 0.9595959782600403, loss : 0.12263397127389908\n",
      "6680 train complete -- accruacy : 0.9911676645278931, loss : 0.03170386329293251\n",
      "6680 test complete == accruacy : 0.9741863012313843, loss : 0.08489702641963959\n",
      "6690 train complete -- accruacy : 0.9908819198608398, loss : 0.032108552753925323\n",
      "6690 test complete == accruacy : 0.9618406295776367, loss : 0.11540105938911438\n",
      "6700 train complete -- accruacy : 0.9917910695075989, loss : 0.02912271022796631\n",
      "6700 test complete == accruacy : 0.9775533080101013, loss : 0.09261472523212433\n",
      "6710 train complete -- accruacy : 0.9842026829719543, loss : 0.047357480973005295\n",
      "6710 test complete == accruacy : 0.9349045753479004, loss : 0.21630264818668365\n",
      "6720 train complete -- accruacy : 0.9842261672019958, loss : 0.047405652701854706\n",
      "6720 test complete == accruacy : 0.9629629850387573, loss : 0.09724803268909454\n",
      "6730 train complete -- accruacy : 0.9826151728630066, loss : 0.05416403338313103\n",
      "6730 test complete == accruacy : 0.9786756634712219, loss : 0.0705578625202179\n",
      "6740 train complete -- accruacy : 0.9838278889656067, loss : 0.0520465262234211\n",
      "6740 test complete == accruacy : 0.9730639457702637, loss : 0.09298055619001389\n",
      "6750 train complete -- accruacy : 0.984000027179718, loss : 0.04780658334493637\n",
      "6750 test complete == accruacy : 0.9730639457702637, loss : 0.08741740882396698\n",
      "6760 train complete -- accruacy : 0.9831361174583435, loss : 0.05475538969039917\n",
      "6760 test complete == accruacy : 0.9696969985961914, loss : 0.12736822664737701\n",
      "6770 train complete -- accruacy : 0.987149178981781, loss : 0.04605347663164139\n",
      "6770 test complete == accruacy : 0.9562289714813232, loss : 0.11757294833660126\n",
      "6780 train complete -- accruacy : 0.9853982329368591, loss : 0.045618053525686264\n",
      "6780 test complete == accruacy : 0.9764309525489807, loss : 0.09519069641828537\n",
      "6790 train complete -- accruacy : 0.9930780529975891, loss : 0.02825033850967884\n",
      "6790 test complete == accruacy : 0.9741863012313843, loss : 0.08313223719596863\n",
      "6800 train complete -- accruacy : 0.9855882525444031, loss : 0.04774614796042442\n",
      "6800 test complete == accruacy : 0.9573512673377991, loss : 0.13241738080978394\n",
      "6810 train complete -- accruacy : 0.9883993864059448, loss : 0.03827540576457977\n",
      "6810 test complete == accruacy : 0.9618406295776367, loss : 0.10179310292005539\n",
      "6820 train complete -- accruacy : 0.9892961978912354, loss : 0.03798767551779747\n",
      "6820 test complete == accruacy : 0.9753086566925049, loss : 0.07060171663761139\n",
      "6830 train complete -- accruacy : 0.9824304580688477, loss : 0.056544262915849686\n",
      "6830 test complete == accruacy : 0.9506173133850098, loss : 0.15816596150398254\n",
      "6840 train complete -- accruacy : 0.9853801131248474, loss : 0.052324503660202026\n",
      "6840 test complete == accruacy : 0.9629629850387573, loss : 0.15838971734046936\n",
      "6850 train complete -- accruacy : 0.987737238407135, loss : 0.04356138035655022\n",
      "6850 test complete == accruacy : 0.9584736227989197, loss : 0.15023140609264374\n",
      "6860 train complete -- accruacy : 0.9781340956687927, loss : 0.06563501805067062\n",
      "6860 test complete == accruacy : 0.9607182741165161, loss : 0.13984978199005127\n",
      "6870 train complete -- accruacy : 0.9775837063789368, loss : 0.07090811431407928\n",
      "6870 test complete == accruacy : 0.9674522876739502, loss : 0.11635220050811768\n",
      "6880 train complete -- accruacy : 0.9843023419380188, loss : 0.05222383514046669\n",
      "6880 test complete == accruacy : 0.9663299918174744, loss : 0.10616184026002884\n",
      "6890 train complete -- accruacy : 0.9888243675231934, loss : 0.04036552086472511\n",
      "6890 test complete == accruacy : 0.9797979593276978, loss : 0.08731935918331146\n",
      "6900 train complete -- accruacy : 0.9868115782737732, loss : 0.043222974985837936\n",
      "6900 test complete == accruacy : 0.9652076363563538, loss : 0.08354948461055756\n",
      "6910 train complete -- accruacy : 0.9846599102020264, loss : 0.05187943950295448\n",
      "6910 test complete == accruacy : 0.982042670249939, loss : 0.07447545230388641\n",
      "6920 train complete -- accruacy : 0.9880057573318481, loss : 0.044208161532878876\n",
      "6920 test complete == accruacy : 0.9741863012313843, loss : 0.07434623688459396\n",
      "6930 train complete -- accruacy : 0.9854257106781006, loss : 0.04970286414027214\n",
      "6930 test complete == accruacy : 0.9696969985961914, loss : 0.10209206491708755\n",
      "6940 train complete -- accruacy : 0.9847262501716614, loss : 0.04941877722740173\n",
      "6940 test complete == accruacy : 0.9640852808952332, loss : 0.12527143955230713\n",
      "6950 train complete -- accruacy : 0.9929496645927429, loss : 0.026333510875701904\n",
      "6950 test complete == accruacy : 0.9786756634712219, loss : 0.05565068498253822\n",
      "6960 train complete -- accruacy : 0.9841954112052917, loss : 0.048492588102817535\n",
      "6960 test complete == accruacy : 0.9741863012313843, loss : 0.06961958855390549\n",
      "6970 train complete -- accruacy : 0.9791965484619141, loss : 0.06862346827983856\n",
      "6970 test complete == accruacy : 0.9371492862701416, loss : 0.19562585651874542\n",
      "6980 train complete -- accruacy : 0.9858165979385376, loss : 0.042329154908657074\n",
      "6980 test complete == accruacy : 0.9461279511451721, loss : 0.173503115773201\n",
      "6990 train complete -- accruacy : 0.980400562286377, loss : 0.05958624929189682\n",
      "6990 test complete == accruacy : 0.9708192944526672, loss : 0.10876299440860748\n",
      "7000 train complete -- accruacy : 0.9778571724891663, loss : 0.06957800686359406\n",
      "7000 test complete == accruacy : 0.9551066160202026, loss : 0.1238306537270546\n",
      "7010 train complete -- accruacy : 0.9805991649627686, loss : 0.05902103707194328\n",
      "7010 test complete == accruacy : 0.9719416499137878, loss : 0.10249600559473038\n",
      "7020 train complete -- accruacy : 0.9874643683433533, loss : 0.041578322649002075\n",
      "7020 test complete == accruacy : 0.9708192944526672, loss : 0.10130380094051361\n",
      "7030 train complete -- accruacy : 0.9830725193023682, loss : 0.05632617697119713\n",
      "7030 test complete == accruacy : 0.9696969985961914, loss : 0.1110190898180008\n",
      "7040 train complete -- accruacy : 0.9860795736312866, loss : 0.046838048845529556\n",
      "7040 test complete == accruacy : 0.9652076363563538, loss : 0.11187183856964111\n",
      "7050 train complete -- accruacy : 0.9970212578773499, loss : 0.015532467514276505\n",
      "7050 test complete == accruacy : 0.9842873215675354, loss : 0.0576438307762146\n",
      "7060 train complete -- accruacy : 0.9859773516654968, loss : 0.047882989048957825\n",
      "7060 test complete == accruacy : 0.9741863012313843, loss : 0.08020708709955215\n",
      "7070 train complete -- accruacy : 0.986280083656311, loss : 0.05271867662668228\n",
      "7070 test complete == accruacy : 0.9786756634712219, loss : 0.06734879314899445\n",
      "7080 train complete -- accruacy : 0.9833333492279053, loss : 0.05356812849640846\n",
      "7080 test complete == accruacy : 0.9663299918174744, loss : 0.1259872317314148\n",
      "7090 train complete -- accruacy : 0.9882933497428894, loss : 0.04326703026890755\n",
      "7090 test complete == accruacy : 0.9741863012313843, loss : 0.07636284828186035\n",
      "7100 train complete -- accruacy : 0.9819718599319458, loss : 0.054924480617046356\n",
      "7100 test complete == accruacy : 0.9562289714813232, loss : 0.12990108132362366\n",
      "7110 train complete -- accruacy : 0.9909985661506653, loss : 0.03311820700764656\n",
      "7110 test complete == accruacy : 0.9809203147888184, loss : 0.08321011066436768\n",
      "7120 train complete -- accruacy : 0.9834269881248474, loss : 0.05249395966529846\n",
      "7120 test complete == accruacy : 0.9685746431350708, loss : 0.10428789258003235\n",
      "7130 train complete -- accruacy : 0.9817671775817871, loss : 0.05561414733529091\n",
      "7130 test complete == accruacy : 0.9663299918174744, loss : 0.11765769869089127\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# x_train shape: (7130, 28, 28, 3)\n",
    "# x_test shape: (891, 28, 28, 3)\n",
    "# x_validation shape: (892, 28, 28, 3)\n",
    "\n",
    "def get_model_test_result(model, x, y, N):\n",
    "    test_loss, test_accuracy = model.evaluate(x, y, verbose=False)\n",
    "    print('{} test complete == accruacy : {}, loss : {}'.format(N, test_accuracy, test_loss))\n",
    "    return [test_loss, test_accuracy]\n",
    "\n",
    "def train_model_with_no_log(model, x, y, repeat, N):\n",
    "    model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x, y, epochs=repeat, verbose=False)\n",
    "    print('{} train complete -- accruacy : {}, loss : {}'.format(N, model.history.history['accuracy'][-1], model.history.history['loss'][-1]))\n",
    "\n",
    "loss_list = []\n",
    "accurcy_list = []\n",
    "\n",
    "N = 10\n",
    "while N <= 7130:\n",
    "    model = get_modal_type_A(64, 128, 128)\n",
    "    train_model_with_no_log(model, x_train[:N], y_train[:N], 10, N)\n",
    "    (loss, accuracy) = get_model_test_result(model, x_test, y_test, N)\n",
    "    \n",
    "    loss_list.append(loss)\n",
    "    accurcy_list.append(accuracy)\n",
    "    N += 10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_axis = np.arange(10, 7140, 10)\n",
    "score = [0.6] * 713\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,7)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.plot(x_axis, score, alpha=.6, color='gray')\n",
    "plt.plot(x_axis, accurcy_list, ':r',label='accuracy')\n",
    "plt.plot(x_axis, loss_list,':b',label='loss')\n",
    "plt.annotate('60%',xy=(6500,0.6),xytext=(6700,0.7),arrowprops={'color':'black'})\n",
    "plt.xlabel('number of train set')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6 threshold : 580\n",
      "min accuracy : 0.33670035004615784, max accuracy : 0.9842873215675354\n"
     ]
    }
   ],
   "source": [
    "min_60_x = 0\n",
    "for idx in range(0, 713):\n",
    "    min_60_x = idx if accurcy_list[idx] <= 0.6 else min_60_x\n",
    "\n",
    "print(\"0.6 threshold : {}\".format(min_60_x * 10 + 10))\n",
    "print(\"min accuracy : {}, max accuracy : {}\".format(min(accurcy_list), max(accurcy_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실험결과\n",
    "1. train set을 580개 이상 사용하게 되면 accurcy가 60% 이상임이 관찰 되었다.\n",
    "2. train set을 10개에서 7130개를 사용하였을 때, accuracy는 *0.33670035004615784 ~ 0.9842873215675354*사이의 값을 가지게 된다\n",
    "3. train set의 양이 증가함에 따라 accuracy가 증가하는 경향을 띄며 accuracy의 값은 1로 수렴해 간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "이번 프로젝트의 핵심은 **데이터의 중요성**이었다. 처음 300개의 데이터를 이용하여 학습을 할 때만 하여도 40% 근처의 accuracy를 얻을 수 있었지만, 다량의 데이터를 이용하여 학습을 진행하였더니 더 발전한 accuracy를 얹을 수 있었다. 더불어 데이터의 양 뿐만 아니라 질 또한 학습에 지대한 영향을 미치는 것을 체감할 수 있었다. 이 문서를 쓸 때는 사용하지 않았지만, 초기에 300개의 데이터를 사용한 실험 이후, 약 600개 정도의 데이터를 추가로 수집하여 모델을 학습 시켰었는데, 그 때의 accuracy는 58% 근처를 멤돌았었다. 그 후 기존 데이터를 모두 삭제하고 데이터 수집 단계부터 새로이 프로젝트를 진행하였고 그 결과가 지금 작성하고 있는 이 문서이다. 위의 실험결과에서 비슷한 양의 데이터와 동일한 하이퍼 파라매터를 사용하여 학습되었을 때, 이후에 학습 된 모델의 성능이 더 좋을 것을 확인할 수 있다. 이것은 학습에 사용된 데이터의 질일 후자가 더 좋았기 때문이라고 생각된다.\n",
    "\n",
    "프로젝트를 진행하면서 다양한 자료들을 읽고 많은 것을 배웠지만 아래의 사항들은 아직 모호나 채로 남아있다. 차후에 시간이 주어진다면 추가로 공부를 해보아야겠고, 아직 갈 길이 매우 멀다는 경각심이 든 이틀이었다.\n",
    "* train loss의 정확한 의미\n",
    "* 모델에 사용된 레이어(MaxPool2D, Conv2D, Dense)와 그 파라매터들의 정확한 의미\n",
    "* 효율적인 하이퍼 파라미터 선정 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ref\n",
    "* [[Python] sklearn의 train_test_split() 사용법](http://blog.naver.com/siniphia/221396370872)\n",
    "* [파이썬을 이용한 데이타 시각화 #1 - Matplotlib 기본 그래프 그리기](https://bcho.tistory.com/1201)\n",
    "* [두 연속변수의 관계 :: Python 데이터 시각화 기초](https://mindscale.kr/course/python-visualization-basic/relation)\n",
    "* [딥러닝 CNN 케라스(Keras) - ImageDataGenerator를 이용한 가위바위보 이미지 분류 모델](https://blog.naver.com/lois7109/222177838839)\n",
    "* [Convolution Neural Networks for MNIST data](https://medium.com/@mlguy/convolution-neural-networks-for-mnist-data-68807f662e7a)\n",
    "* [How to Treat Overfitting in Convolutional Neural Networks](https://www.analyticsvidhya.com/blog/2020/09/overfitting-in-cnn-show-to-treat-overfitting-in-convolutional-neural-networks/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
