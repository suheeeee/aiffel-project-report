{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45839151",
   "metadata": {},
   "source": [
    "# HuggingFace Custom Project ü§ó\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14fad0",
   "metadata": {},
   "source": [
    " GLUE datasetÏùò mnli taskÎ•º ÏàòÌñâÌïòÎäî ÌîÑÎ°úÏ†ùÌä∏Î•º Ïª§Ïä§ÌÖÄ ÌîÑÎ°úÏ†ùÌä∏ ÌòïÌÉúÎ°ú ÏßÑÌñâÌï¥ Î≥∏Îã§."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51bf80b",
   "metadata": {},
   "source": [
    "#### environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01f08b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:03.170888Z",
     "iopub.status.busy": "2021-05-18T09:34:03.170569Z",
     "iopub.status.idle": "2021-05-18T09:34:06.478950Z",
     "shell.execute_reply": "2021-05-18T09:34:06.478432Z",
     "shell.execute_reply.started": "2021-05-18T09:34:03.170791Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AutoConfig\n",
    "from transformers import BartTokenizer, BartForSequenceClassification, AutoConfig\n",
    "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification\n",
    "from dataclasses import asdict\n",
    "from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34c7b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:06.479918Z",
     "iopub.status.busy": "2021-05-18T09:34:06.479764Z",
     "iopub.status.idle": "2021-05-18T09:34:06.484999Z",
     "shell.execute_reply": "2021-05-18T09:34:06.484559Z",
     "shell.execute_reply.started": "2021-05-18T09:34:06.479897Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d938401d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:06.486106Z",
     "iopub.status.busy": "2021-05-18T09:34:06.485940Z",
     "iopub.status.idle": "2021-05-18T09:34:07.046581Z",
     "shell.execute_reply": "2021-05-18T09:34:07.045959Z",
     "shell.execute_reply.started": "2021-05-18T09:34:06.486083Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialization gpu ########################################################\n",
    "import tensorflow as tf\n",
    "def init_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\\n\\n\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e, \"\\n\\n\")\n",
    "        \n",
    "init_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f574bb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.047962Z",
     "iopub.status.busy": "2021-05-18T09:34:07.047753Z",
     "iopub.status.idle": "2021-05-18T09:34:07.050938Z",
     "shell.execute_reply": "2021-05-18T09:34:07.050236Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.047929Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "workspace_path = os.path.join(os.getenv('HOME'), 'workspace/aiffel-gd-nlp/GD18_hugging_face_custom')\n",
    "# data_dir_path = os.path.join(workspace_path, 'data')\n",
    "# model_dir_path = os.path.join(workspace_path, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b9f72",
   "metadata": {},
   "source": [
    "## Step 1. mnli Îç∞Ïù¥ÌÑ∞ÏÖã Î∂ÑÏÑù\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc841a",
   "metadata": {},
   "source": [
    "- Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f233cb6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.052014Z",
     "iopub.status.busy": "2021-05-18T09:34:07.051824Z",
     "iopub.status.idle": "2021-05-18T09:34:07.193486Z",
     "shell.execute_reply": "2021-05-18T09:34:07.192939Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.051983Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/aiffel-dj1/tensorflow_datasets/glue/mnli/1.0.0\n",
      "INFO:absl:Reusing dataset glue (/home/aiffel-dj1/tensorflow_datasets/glue/mnli/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset glue for split None, from /home/aiffel-dj1/tensorflow_datasets/glue/mnli/1.0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "392702"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, info = tfds.load('glue/mnli', with_info=True)\n",
    "info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018f226e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.194571Z",
     "iopub.status.busy": "2021-05-18T09:34:07.194399Z",
     "iopub.status.idle": "2021-05-18T09:34:07.198755Z",
     "shell.execute_reply": "2021-05-18T09:34:07.198141Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.194550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='glue',\n",
       "    full_name='glue/mnli/1.0.0',\n",
       "    description=\"\"\"\n",
       "    GLUE, the General Language Understanding Evaluation benchmark\n",
       "    (https://gluebenchmark.com/) is a collection of resources for training,\n",
       "    evaluating, and analyzing natural language understanding systems.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    The Multi-Genre Natural Language Inference Corpus is a crowdsourced\n",
       "    collection of sentence pairs with textual entailment annotations. Given a premise sentence\n",
       "    and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis\n",
       "    (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are\n",
       "    gathered from ten different sources, including transcribed speech, fiction, and government reports.\n",
       "    We use the standard test set, for which we obtained private labels from the authors, and evaluate\n",
       "    on both the matched (in-domain) and mismatched (cross-domain) section. We also use and recommend\n",
       "    the SNLI corpus as 550k examples of auxiliary training data.\n",
       "    \"\"\",\n",
       "    homepage='http://www.nyu.edu/projects/bowman/multinli/',\n",
       "    data_path='/home/aiffel-dj1/tensorflow_datasets/glue/mnli/1.0.0',\n",
       "    download_size=298.29 MiB,\n",
       "    dataset_size=100.56 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'hypothesis': Text(shape=(), dtype=tf.string),\n",
       "        'idx': tf.int32,\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
       "        'premise': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    supervised_keys=None,\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test_matched': <SplitInfo num_examples=9796, num_shards=1>,\n",
       "        'test_mismatched': <SplitInfo num_examples=9847, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=392702, num_shards=1>,\n",
       "        'validation_matched': <SplitInfo num_examples=9815, num_shards=1>,\n",
       "        'validation_mismatched': <SplitInfo num_examples=9832, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{N18-1101,\n",
       "      author = \"Williams, Adina\n",
       "                and Nangia, Nikita\n",
       "                and Bowman, Samuel\",\n",
       "      title = \"A Broad-Coverage Challenge Corpus for\n",
       "               Sentence Understanding through Inference\",\n",
       "      booktitle = \"Proceedings of the 2018 Conference of\n",
       "                   the North American Chapter of the\n",
       "                   Association for Computational Linguistics:\n",
       "                   Human Language Technologies, Volume 1 (Long\n",
       "                   Papers)\",\n",
       "      year = \"2018\",\n",
       "      publisher = \"Association for Computational Linguistics\",\n",
       "      pages = \"1112--1122\",\n",
       "      location = \"New Orleans, Louisiana\",\n",
       "      url = \"http://aclweb.org/anthology/N18-1101\"\n",
       "    }\n",
       "    @article{bowman2015large,\n",
       "      title={A large annotated corpus for learning natural language inference},\n",
       "      author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},\n",
       "      journal={arXiv preprint arXiv:1508.05326},\n",
       "      year={2015}\n",
       "    }\n",
       "    @inproceedings{wang2019glue,\n",
       "      title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
       "      author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
       "      note={In the Proceedings of ICLR.},\n",
       "      year={2019}\n",
       "    }\n",
       "    \n",
       "    Note that each GLUE dataset has its own citation. Please see the source to see\n",
       "    the correct citation for each contained dataset.\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f931faa",
   "metadata": {},
   "source": [
    "- Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8740c4af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.199735Z",
     "iopub.status.busy": "2021-05-18T09:34:07.199569Z",
     "iopub.status.idle": "2021-05-18T09:34:07.204356Z",
     "shell.execute_reply": "2021-05-18T09:34:07.203908Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.199712Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10df40d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.206166Z",
     "iopub.status.busy": "2021-05-18T09:34:07.205977Z",
     "iopub.status.idle": "2021-05-18T09:34:07.244140Z",
     "shell.execute_reply": "2021-05-18T09:34:07.243505Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.206142Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Meaningful partnerships with stakeholders is crucial.'\n",
      "16399\n",
      "1\n",
      "b'In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.'\n"
     ]
    }
   ],
   "source": [
    "examples = data['train'].take(1)\n",
    "for example in examples:\n",
    "    hypothesis = example['hypothesis']\n",
    "    idx = example['idx']\n",
    "    label = example['label']\n",
    "    premise = example['premise']\n",
    "    print(hypothesis.numpy())\n",
    "    print(idx.numpy())\n",
    "    print(label.numpy())\n",
    "    print(premise.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824195e1",
   "metadata": {},
   "source": [
    "## Step 2. MNLIProcessorÌÅ¥ÎûòÏä§Î•º Íµ¨ÌòÑ\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd91f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.245683Z",
     "iopub.status.busy": "2021-05-18T09:34:07.245475Z",
     "iopub.status.idle": "2021-05-18T09:34:07.251221Z",
     "shell.execute_reply": "2021-05-18T09:34:07.250672Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.245648Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"\n",
    "        Gets an example from a dict with tensorflow tensors.\n",
    "\n",
    "        Args:\n",
    "            tensor_dict: Keys and values should match the corresponding Glue\n",
    "                tensorflow_dataset examples.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the test set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tfds_map(self, example):\n",
    "        \"\"\"\n",
    "        Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are. This method converts\n",
    "        examples to the correct format.\n",
    "        \"\"\"\n",
    "        if len(self.get_labels()) > 1:\n",
    "            example.label = self.get_labels()[int(example.label)]\n",
    "        return example\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05c5ab01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.252219Z",
     "iopub.status.busy": "2021-05-18T09:34:07.251997Z",
     "iopub.status.idle": "2021-05-18T09:34:07.260377Z",
     "shell.execute_reply": "2021-05-18T09:34:07.259846Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.252197Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"premise\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"hypothesis\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        print(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = f\"{set_type}-{line[0]}\"\n",
    "            text_a = line[8]\n",
    "            text_b = line[9]\n",
    "            label = None if set_type.startswith(\"test\") else line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66a41127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.261564Z",
     "iopub.status.busy": "2021-05-18T09:34:07.261325Z",
     "iopub.status.idle": "2021-05-18T09:34:07.265585Z",
     "shell.execute_reply": "2021-05-18T09:34:07.264786Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.261540Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MnliMismatchedProcessor(MnliProcessor):\n",
    "    \"\"\"Processor for the MultiNLI Mismatched data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev_mismatched.tsv\")), \"dev_mismatched\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test_mismatched.tsv\")), \"test_mismatched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dcee89e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.266799Z",
     "iopub.status.busy": "2021-05-18T09:34:07.266606Z",
     "iopub.status.idle": "2021-05-18T09:34:07.299035Z",
     "shell.execute_reply": "2021-05-18T09:34:07.298439Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.266775Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ÏõêÎ≥∏Îç∞Ïù¥ÌÑ∞------\n",
      "{'hypothesis': <tf.Tensor: shape=(), dtype=string, numpy=b'Meaningful partnerships with stakeholders is crucial.'>, 'idx': <tf.Tensor: shape=(), dtype=int32, numpy=16399>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'premise': <tf.Tensor: shape=(), dtype=string, numpy=b'In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.'>}\n",
      "------processor Í∞ÄÍ≥µÎç∞Ïù¥ÌÑ∞------\n",
      "InputExample(guid=16399, text_a='In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.', text_b='Meaningful partnerships with stakeholders is crucial.', label='1')\n"
     ]
    }
   ],
   "source": [
    "processor = MnliProcessor()\n",
    "\n",
    "examples = data['train'].take(1)\n",
    "\n",
    "for example in examples:\n",
    "    print('------ÏõêÎ≥∏Îç∞Ïù¥ÌÑ∞------')\n",
    "    print(example)  \n",
    "    example = processor.get_example_from_tensor_dict(example)\n",
    "    print('------processor Í∞ÄÍ≥µÎç∞Ïù¥ÌÑ∞------')\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba72e85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.300159Z",
     "iopub.status.busy": "2021-05-18T09:34:07.299961Z",
     "iopub.status.idle": "2021-05-18T09:34:07.331460Z",
     "shell.execute_reply": "2021-05-18T09:34:07.330918Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.300135Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputExample(guid=16399, text_a='In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.', text_b='Meaningful partnerships with stakeholders is crucial.', label='1')\n"
     ]
    }
   ],
   "source": [
    "examples = (data['train'].take(1))\n",
    "for example in examples:\n",
    "    example = processor.get_example_from_tensor_dict(example)\n",
    "    example = processor.tfds_map(example)\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de965db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.332425Z",
     "iopub.status.busy": "2021-05-18T09:34:07.332253Z",
     "iopub.status.idle": "2021-05-18T09:34:07.336066Z",
     "shell.execute_reply": "2021-05-18T09:34:07.335530Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.332403Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = processor.get_labels()\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f27a4269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.336921Z",
     "iopub.status.busy": "2021-05-18T09:34:07.336751Z",
     "iopub.status.idle": "2021-05-18T09:34:07.340883Z",
     "shell.execute_reply": "2021-05-18T09:34:07.340213Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.336898Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1, '2': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805430f8",
   "metadata": {},
   "source": [
    "## Step 3. Îç∞Ïù¥ÌÑ∞ÏÖã Íµ¨ÏÑ±\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f1646d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:07.341923Z",
     "iopub.status.busy": "2021-05-18T09:34:07.341735Z",
     "iopub.status.idle": "2021-05-18T09:34:11.621133Z",
     "shell.execute_reply": "2021-05-18T09:34:11.620631Z",
     "shell.execute_reply.started": "2021-05-18T09:34:07.341892Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd193e4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:11.621989Z",
     "iopub.status.busy": "2021-05-18T09:34:11.621832Z",
     "iopub.status.idle": "2021-05-18T09:34:11.627495Z",
     "shell.execute_reply": "2021-05-18T09:34:11.626906Z",
     "shell.execute_reply.started": "2021-05-18T09:34:11.621968Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode=\"claasification\") :\n",
    "    if max_length is None :\n",
    "        max_length = tokenizer.max_len\n",
    "    if label_list is None:\n",
    "        label_list = processor.get_labels()\n",
    "        print(\"Using label list %s\" % (label_list))\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    labels = [label_map[example.label] for example in examples]\n",
    "\n",
    "    batch_encoding = tokenizer(\n",
    "        [(example.text_a, example.text_b) for example in examples],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "    for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "\n",
    "        feature = InputFeatures(**inputs, label=labels[i])\n",
    "        features.append(feature)\n",
    "\n",
    "    for i, example in enumerate(examples[:5]):\n",
    "        print(\"*** Example ***\")\n",
    "        print(\"guid: %s\" % (example.guid))\n",
    "        print(\"features: %s\" % features[i])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fb5b400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:11.628795Z",
     "iopub.status.busy": "2021-05-18T09:34:11.628538Z",
     "iopub.status.idle": "2021-05-18T09:34:11.635186Z",
     "shell.execute_reply": "2021-05-18T09:34:11.634561Z",
     "shell.execute_reply.started": "2021-05-18T09:34:11.628762Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode=\"classification\") :\n",
    "    \"\"\"\n",
    "    :param examples: tf.data.Dataset\n",
    "    :param tokenizer: pretrained tokenizer\n",
    "    :param max_length: exampleÏùò ÏµúÎåÄ Í∏∏Ïù¥(Í∏∞Î≥∏Í∞í : tokenizerÏùò max_len)\n",
    "    :param task: GLUE task Ïù¥Î¶Ñ\n",
    "    :param label_list: ÎùºÎ≤® Î¶¨Ïä§Ìä∏\n",
    "    :param output_mode: \"regression\" or \"classification\"\n",
    "\n",
    "    :return: taskÏóê ÎßûÎèÑÎ°ù featureÍ∞Ä Íµ¨ÏÑ±Îêú tf.data.Dataset\n",
    "    \"\"\"\n",
    "    examples = [processor.tfds_map(processor.get_example_from_tensor_dict(example)) for example in examples]\n",
    "    features = _glue_convert_examples_to_features(examples, tokenizer, max_length, processor)\n",
    "    label_type = tf.int64\n",
    "\n",
    "    def gen():\n",
    "        for ex in features:\n",
    "            d = {k: v for k, v in asdict(ex).items() if v is not None}\n",
    "            label = d.pop(\"label\")\n",
    "            yield (d, label)\n",
    "\n",
    "    input_names = [\"input_ids\"] + tokenizer.model_input_names\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({k: tf.int32 for k in input_names}, label_type),\n",
    "        ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf3f5a",
   "metadata": {},
   "source": [
    "- ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41743051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:11.636399Z",
     "iopub.status.busy": "2021-05-18T09:34:11.636115Z",
     "iopub.status.idle": "2021-05-18T09:34:11.639144Z",
     "shell.execute_reply": "2021-05-18T09:34:11.638531Z",
     "shell.execute_reply.started": "2021-05-18T09:34:11.636367Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd60b1be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:34:11.640251Z",
     "iopub.status.busy": "2021-05-18T09:34:11.640025Z",
     "iopub.status.idle": "2021-05-18T09:39:07.971882Z",
     "shell.execute_reply": "2021-05-18T09:39:07.971278Z",
     "shell.execute_reply.started": "2021-05-18T09:34:11.640219Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1', '2']\n",
      "*** Example ***\n",
      "guid: 16399\n",
      "features: InputFeatures(input_ids=[101, 1999, 5038, 1997, 2122, 13136, 1010, 1048, 11020, 2038, 2499, 29454, 29206, 14626, 2144, 2786, 2000, 16636, 1996, 10908, 1997, 1996, 2110, 4041, 6349, 1998, 2000, 5323, 15902, 13797, 2007, 22859, 6461, 2012, 6469, 2075, 1037, 2047, 25353, 14905, 10735, 2483, 2090, 1996, 2976, 10802, 1998, 15991, 1997, 3423, 2578, 4804, 1012, 102, 15902, 13797, 2007, 22859, 2003, 10232, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 206287\n",
      "features: InputFeatures(input_ids=[101, 1996, 7207, 7505, 21799, 2015, 2036, 2218, 1996, 2152, 2598, 1999, 1996, 6123, 2162, 1012, 102, 1996, 7207, 8771, 2921, 2000, 1996, 3020, 2598, 1999, 1996, 6594, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "*** Example ***\n",
      "guid: 352707\n",
      "features: InputFeatures(input_ids=[101, 8529, 1011, 14910, 2138, 2308, 2024, 1999, 2296, 2492, 2085, 1045, 2812, 1045, 2064, 1005, 1056, 2228, 1997, 1037, 2492, 2008, 2027, 1005, 2128, 2025, 2920, 1999, 102, 2308, 2031, 5841, 1999, 2035, 2752, 1997, 1996, 14877, 1010, 2027, 2024, 2471, 2893, 1996, 2168, 12678, 2004, 2087, 2273, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 372070\n",
      "features: InputFeatures(input_ids=[101, 5395, 2003, 2428, 14178, 2085, 102, 5395, 2003, 12809, 1998, 4318, 2157, 2085, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n",
      "*** Example ***\n",
      "guid: 160184\n",
      "features: InputFeatures(input_ids=[101, 2021, 2025, 2085, 1012, 102, 2021, 2027, 2876, 1005, 1056, 2022, 2975, 2157, 2085, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n"
     ]
    }
   ],
   "source": [
    "# train Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "train_dataset = tf_glue_convert_examples_to_features(data['train'], tokenizer, max_length=max_len, processor=processor)\n",
    "train_dataset_batch = train_dataset.shuffle(100).batch(batch_size).repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c569cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:39:07.972863Z",
     "iopub.status.busy": "2021-05-18T09:39:07.972697Z",
     "iopub.status.idle": "2021-05-18T09:39:16.119825Z",
     "shell.execute_reply": "2021-05-18T09:39:16.119205Z",
     "shell.execute_reply.started": "2021-05-18T09:39:07.972842Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1', '2']\n",
      "*** Example ***\n",
      "guid: 6287\n",
      "features: InputFeatures(input_ids=[101, 7910, 1011, 9616, 2821, 3398, 2035, 1996, 2111, 2005, 2157, 7910, 2166, 2030, 2242, 102, 3398, 7167, 1997, 2111, 2005, 1996, 2157, 2166, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "*** Example ***\n",
      "guid: 1579\n",
      "features: InputFeatures(input_ids=[101, 2036, 1010, 1045, 2097, 2022, 10262, 2008, 1996, 1020, 1012, 1014, 2050, 3465, 1997, 1996, 10690, 2326, 2000, 2202, 1996, 5653, 2013, 3937, 2000, 2573, 8167, 2098, 4650, 2003, 5377, 2004, 3132, 12450, 1997, 5653, 2693, 2067, 1998, 5743, 2090, 3937, 1998, 2573, 8167, 2098, 1012, 102, 1045, 2097, 2022, 10262, 2008, 1996, 1020, 1012, 1014, 2050, 3465, 1997, 1996, 10690, 2326, 2000, 2202, 1996, 5653, 2013, 3937, 2000, 2573, 8167, 2098, 4650, 2003, 5377, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "*** Example ***\n",
      "guid: 9787\n",
      "features: InputFeatures(input_ids=[101, 2579, 2039, 2011, 1996, 2821, 3100, 2821, 2061, 2017, 2113, 2092, 2008, 1005, 1055, 1045, 2018, 4999, 2823, 1045, 2354, 2008, 2045, 2001, 1037, 2843, 1997, 1037, 2843, 1997, 3947, 1998, 1037, 2843, 1997, 2147, 2253, 2046, 1037, 2843, 1997, 2008, 1998, 1045, 2074, 4999, 2065, 2065, 2009, 6354, 1998, 2065, 2009, 2165, 2017, 2113, 2066, 3398, 102, 2009, 3465, 1037, 2843, 1997, 2769, 2000, 2404, 5743, 2035, 2008, 3947, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "guid: 7631\n",
      "features: InputFeatures(input_ids=[101, 3398, 1045, 2113, 1996, 5013, 3514, 102, 1045, 2113, 2498, 2055, 1996, 5013, 3514, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n",
      "*** Example ***\n",
      "guid: 2077\n",
      "features: InputFeatures(input_ids=[101, 2116, 2697, 3122, 9275, 2036, 14686, 1037, 1040, 1010, 1038, 1998, 1038, 1006, 4596, 1010, 2793, 1010, 1998, 6350, 1007, 3446, 1010, 2029, 2950, 1996, 3944, 7954, 1998, 2003, 2411, 3243, 3465, 1011, 4621, 1012, 102, 2116, 9275, 1999, 2697, 3122, 2031, 1037, 3465, 1011, 4621, 5724, 1997, 4596, 1010, 2793, 1010, 1998, 6350, 2007, 2019, 3944, 7954, 1999, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n"
     ]
    }
   ],
   "source": [
    "# validation Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "# validation_dataset_match = tf_glue_convert_examples_to_features(data['validation_matched'], tokenizer, max_length=max_len, processor=processor)\n",
    "# validation_dataset_mismatch = tf_glue_convert_examples_to_features(data['validation_matched'], tokenizer, max_length=max_len, processor=processor_mismatched)\n",
    "# validation_dataset = validation_dataset_match.concatenate(validation_dataset_mismatch)\n",
    "validation_dataset = tf_glue_convert_examples_to_features(data['validation_matched'], tokenizer, max_length=max_len, processor=processor)\n",
    "validation_dataset_batch = validation_dataset.shuffle(100).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfe6a477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:39:16.120715Z",
     "iopub.status.busy": "2021-05-18T09:39:16.120553Z",
     "iopub.status.idle": "2021-05-18T09:39:23.424108Z",
     "shell.execute_reply": "2021-05-18T09:39:23.423478Z",
     "shell.execute_reply.started": "2021-05-18T09:39:16.120693Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1', '2']\n",
      "*** Example ***\n",
      "guid: 5398\n",
      "features: InputFeatures(input_ids=[101, 2092, 2003, 2045, 1037, 2210, 3751, 10216, 2017, 2404, 1999, 2045, 102, 2003, 2045, 1037, 10216, 2008, 2017, 2404, 1999, 2045, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n",
      "*** Example ***\n",
      "guid: 1921\n",
      "features: InputFeatures(input_ids=[101, 1996, 13410, 2139, 6517, 2063, 1005, 1055, 2132, 2003, 2124, 2000, 2022, 2682, 2598, 1010, 2004, 2027, 2360, 1999, 1996, 2449, 1012, 102, 1996, 13410, 2139, 6517, 2063, 1005, 1055, 2132, 2003, 9689, 2921, 5230, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n",
      "*** Example ***\n",
      "guid: 3336\n",
      "features: InputFeatures(input_ids=[101, 1996, 28928, 3325, 2012, 1996, 9985, 3640, 1037, 3486, 1011, 3371, 5746, 1011, 5107, 4955, 2000, 1996, 2555, 1025, 5165, 16722, 2448, 2000, 4880, 12789, 14820, 1998, 2060, 3182, 1025, 1998, 2017, 2064, 2036, 10887, 2235, 7477, 1012, 102, 2087, 5731, 3422, 1996, 2678, 2077, 2027, 2275, 2041, 2000, 8849, 1996, 2181, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n",
      "*** Example ***\n",
      "guid: 2611\n",
      "features: InputFeatures(input_ids=[101, 2002, 1005, 1040, 2488, 3198, 2079, 18992, 2015, 1010, 2030, 2028, 1997, 1996, 29229, 1010, 2065, 2002, 4122, 2000, 2113, 2055, 4157, 1011, 10268, 1012, 102, 2079, 18992, 2015, 1998, 1996, 29229, 2763, 2113, 2055, 4157, 10268, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n",
      "*** Example ***\n",
      "guid: 1922\n",
      "features: InputFeatures(input_ids=[101, 9658, 1010, 2006, 1996, 14607, 2044, 1037, 3391, 7823, 4116, 4586, 1006, 4419, 2739, 4465, 1007, 3160, 1010, 7822, 2015, 2007, 1996, 2087, 13338, 24405, 9354, 1997, 1996, 5353, 1012, 102, 9658, 2001, 2445, 1037, 2524, 3160, 2011, 4116, 4586, 1010, 2021, 2002, 3266, 2000, 26399, 2009, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n"
     ]
    }
   ],
   "source": [
    "# test Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "# test_dataset_match = tf_glue_convert_examples_to_features(data['test_matched'], tokenizer, max_length=max_len, processor=processor)\n",
    "# test_dataset_mismatch = tf_glue_convert_examples_to_features(data['test_mismatched'], tokenizer, max_length=max_len, processor=processor_mismatched)\n",
    "# test_dataset = test_dataset_match.concatenate(test_dataset_mismatch)\n",
    "test_dataset = tf_glue_convert_examples_to_features(data['test_matched'], tokenizer, max_length=max_len, processor=processor)\n",
    "test_dataset_batch = test_dataset.shuffle(100).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c38b1e",
   "metadata": {},
   "source": [
    "## Step 4. Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "236a54fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:39:23.425675Z",
     "iopub.status.busy": "2021-05-18T09:39:23.425518Z",
     "iopub.status.idle": "2021-05-18T09:39:23.429653Z",
     "shell.execute_reply": "2021-05-18T09:39:23.429138Z",
     "shell.execute_reply.started": "2021-05-18T09:39:23.425654Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = len(processor.get_labels())\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "## ÏïÑÎûòÏùò Í∞íÏù¥ OOMÏùÑ Ïú†Î∞úÌïòÏó¨ 200ÏúºÎ°ú Í≥†Ï†ïÏãúÏºú Ï£ºÏóàÎã§.\n",
    "steps_per_epoch = tf.data.experimental.cardinality(data['train']).numpy() // batch_size\n",
    "steps_per_epoch = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd4b06",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Î™®Îç∏ ÌïôÏäµ 1 - TFBertForSequenceClassification(bert-base-cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0173aea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T06:59:38.337546Z",
     "iopub.status.busy": "2021-05-18T06:59:38.337380Z",
     "iopub.status.idle": "2021-05-18T06:59:41.144604Z",
     "shell.execute_reply": "2021-05-18T06:59:41.144039Z",
     "shell.execute_reply.started": "2021-05-18T06:59:38.337524Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  2307      \n",
      "=================================================================\n",
      "Total params: 109,484,547\n",
      "Trainable params: 109,484,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "model_bert_base_cased = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "model_bert_base_cased.compile(optimizer=optimizer, loss=loss, metrics=['acc', \n",
    "                                                                       'sparse_categorical_accuracy'])\n",
    "model_bert_base_cased.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c500de6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T06:59:41.145410Z",
     "iopub.status.busy": "2021-05-18T06:59:41.145240Z",
     "iopub.status.idle": "2021-05-18T08:32:29.227254Z",
     "shell.execute_reply": "2021-05-18T08:32:29.226580Z",
     "shell.execute_reply.started": "2021-05-18T06:59:41.145390Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f70faa2dc90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f70faa2dc90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f70faa2dc90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 0s - loss: 0.7167 - acc: 0.6930 - sparse_categorical_accuracy: 0.6930WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1860s 185ms/step - loss: 0.7167 - acc: 0.6930 - sparse_categorical_accuracy: 0.6930 - val_loss: 0.5374 - val_acc: 0.7892 - val_sparse_categorical_accuracy: 0.7892\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 1816s 182ms/step - loss: 0.5560 - acc: 0.7779 - sparse_categorical_accuracy: 0.7779 - val_loss: 0.5338 - val_acc: 0.7882 - val_sparse_categorical_accuracy: 0.7882\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 1892s 189ms/step - loss: 0.5287 - acc: 0.7891 - sparse_categorical_accuracy: 0.7891 - val_loss: 0.4944 - val_acc: 0.8006 - val_sparse_categorical_accuracy: 0.8006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6d2bba2690>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ïù¥Ï†Ñ Ïä§ÌÖùÏóêÏÑú Î∞∞ÏπòÏ≤òÎ¶¨Î•º ÏßÑÌñâÌïú Îç∞Ïù¥ÌÑ∞ÏÖã(xxxx_dataset_batch)ÏùÑ ÌôúÏö©\n",
    "model_bert_base_cased.fit(train_dataset_batch, epochs=3, steps_per_epoch=10000, \n",
    "                             validation_data=validation_dataset_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d3851a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T08:32:29.228483Z",
     "iopub.status.busy": "2021-05-18T08:32:29.228246Z",
     "iopub.status.idle": "2021-05-18T08:33:44.406898Z",
     "shell.execute_reply": "2021-05-18T08:33:44.406468Z",
     "shell.execute_reply.started": "2021-05-18T08:32:29.228451Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1225/1225 [==============================] - 75s 61ms/step - loss: 2.1504 - acc: 0.3529 - sparse_categorical_accuracy: 0.3529\n"
     ]
    }
   ],
   "source": [
    "result = model_bert_base_cased.evaluate(test_dataset_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f04a189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T08:33:44.407743Z",
     "iopub.status.busy": "2021-05-18T08:33:44.407592Z",
     "iopub.status.idle": "2021-05-18T08:35:01.211613Z",
     "shell.execute_reply": "2021-05-18T08:35:01.211017Z",
     "shell.execute_reply.started": "2021-05-18T08:33:44.407723Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1227/1227 [==============================] - 77s 63ms/step - loss: 0.4944 - acc: 0.8006 - sparse_categorical_accuracy: 0.8006\n"
     ]
    }
   ],
   "source": [
    "result = model_bert_base_cased.evaluate(validation_dataset_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9264e5f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T08:35:01.212590Z",
     "iopub.status.busy": "2021-05-18T08:35:01.212427Z",
     "iopub.status.idle": "2021-05-18T08:35:03.541517Z",
     "shell.execute_reply": "2021-05-18T08:35:03.540979Z",
     "shell.execute_reply.started": "2021-05-18T08:35:01.212569Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[-0.29195213,  1.557335  , -0.7351178 ],\n",
       "       [ 2.3099828 , -0.5003828 , -1.5834221 ],\n",
       "       [-3.0622053 , -1.8030615 ,  4.020719  ],\n",
       "       [ 2.932993  , -1.0716059 , -1.8001673 ],\n",
       "       [-2.1897635 , -1.7420418 ,  2.9177992 ],\n",
       "       [-1.9516946 , -0.07686663,  1.578659  ],\n",
       "       [-1.2401516 ,  1.4058137 ,  0.16447748],\n",
       "       [ 2.858041  , -1.1485687 , -1.7349013 ]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = model_bert_base_cased.predict(validation_dataset_batch.take(1))\n",
    "test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b5348",
   "metadata": {},
   "source": [
    "validation setÏóê ÎåÄÌïú ÏµúÏ¢Ö accaurcyÍ∞Ä `0.8006`Ïù∏ Î∞òÎ©¥ test setÏóê ÎåÄÌïú ÏµúÏ¢Ö accuracyÎäî `0.3529`Î•º Í∏∞Î°ùÌïòÏòÄÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983d489",
   "metadata": {},
   "source": [
    "### Î™®Îç∏ ÌïôÏäµ 2 - TFBertForSequenceClassification(bert-base-cased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de7739",
   "metadata": {},
   "source": [
    "Îç∞Ïù¥ÌÑ∞Ïóê Ï¢Ä Îçî robuster Ìï† Ïàò ÏûàÎèÑÎ°ùÌïòÎäî Î∞©ÏïàÏùÑ ÏÉùÍ∞ÅÌïòÎçò Ï§ë dropoutÏùÑ Ï¶ùÍ∞ÄÏãúÏºú Î≥¥Î©¥ Ïñ¥Îñ®ÍπåÌïòÎäî Í≤ÉÏù¥ Îñ†Ïò¨ÎûêÎã§. defalut Í∞íÏù¥ 0,1Ïù∏ dropout Í∞íÏùÑ 0.3ÏúºÎ°ú Ï¶ùÍ∞ÄÏãúÏºú ÌïôÏäµÏùÑ ÏßÑÌñâÌïòÏó¨ Î≥¥ÏïòÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9d41f72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:58:30.162673Z",
     "iopub.status.busy": "2021-05-18T09:58:30.161705Z",
     "iopub.status.idle": "2021-05-18T09:58:30.175262Z",
     "shell.execute_reply": "2021-05-18T09:58:30.173703Z",
     "shell.execute_reply.started": "2021-05-18T09:58:30.162553Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=0, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd80b3fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T09:58:30.315607Z",
     "iopub.status.busy": "2021-05-18T09:58:30.315323Z",
     "iopub.status.idle": "2021-05-18T09:58:32.394937Z",
     "shell.execute_reply": "2021-05-18T09:58:32.394380Z",
     "shell.execute_reply.started": "2021-05-18T09:58:30.315569Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification at 0x7f63dd7f9610>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "model_bert_base_cased_dr_3 = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
    "                                                                                    num_labels=num_classes, \n",
    "                                                                                    hidden_dropout_prob=0.3,\n",
    "                                                                                    attention_probs_dropout_prob=0.3)\n",
    "model_bert_base_cased_dr_3.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "model_bert_base_cased_dr_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77b54f07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T10:17:36.625410Z",
     "iopub.status.busy": "2021-05-18T10:17:36.625114Z",
     "iopub.status.idle": "2021-05-18T10:51:23.714651Z",
     "shell.execute_reply": "2021-05-18T10:51:23.714180Z",
     "shell.execute_reply.started": "2021-05-18T10:17:36.625377Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 253s 253ms/step - loss: 0.7369 - acc: 0.6852 - val_loss: 0.7260 - val_acc: 0.7125\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.7309 - acc: 0.6949 - val_loss: 0.6897 - val_acc: 0.7205\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 253s 253ms/step - loss: 0.7327 - acc: 0.6877 - val_loss: 0.6838 - val_acc: 0.7338\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 254s 254ms/step - loss: 0.7222 - acc: 0.6967 - val_loss: 0.6593 - val_acc: 0.7412\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 255s 255ms/step - loss: 0.7234 - acc: 0.6951 - val_loss: 0.6534 - val_acc: 0.7475\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 256s 256ms/step - loss: 0.6930 - acc: 0.7076 - val_loss: 0.6028 - val_acc: 0.7574\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 254s 254ms/step - loss: 0.6777 - acc: 0.7197 - val_loss: 0.5995 - val_acc: 0.7616\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 255s 255ms/step - loss: 0.6867 - acc: 0.7113 - val_loss: 0.6158 - val_acc: 0.7623\n"
     ]
    }
   ],
   "source": [
    "# Ïù¥Ï†Ñ Ïä§ÌÖùÏóêÏÑú Î∞∞ÏπòÏ≤òÎ¶¨Î•º ÏßÑÌñâÌïú Îç∞Ïù¥ÌÑ∞ÏÖã(xxxx_dataset_batch)ÏùÑ ÌôúÏö©\n",
    "history = model_bert_base_cased_dr_3.fit(train_dataset_batch, \n",
    "                                         epochs=20, \n",
    "                                         steps_per_epoch=1000, \n",
    "                                         validation_data=validation_dataset_batch,\n",
    "                                         callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c33241a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T10:53:02.361722Z",
     "iopub.status.busy": "2021-05-18T10:53:02.361507Z",
     "iopub.status.idle": "2021-05-18T10:54:18.001044Z",
     "shell.execute_reply": "2021-05-18T10:54:18.000485Z",
     "shell.execute_reply.started": "2021-05-18T10:53:02.361690Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1225/1225 [==============================] - 76s 62ms/step - loss: 1.6921 - acc: 0.2710 - sparse_categorical_accuracy: 0.2710\n"
     ]
    }
   ],
   "source": [
    "result_3 = model_bert_base_cased_wich_config.evaluate(test_dataset_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b8d64",
   "metadata": {},
   "source": [
    "validation setÏóê ÎåÄÌïú ÏµúÏ¢Ö accaurcyÍ∞Ä `0.7623`, test setÏóê ÎåÄÌïú accuracyÎäî `0.2710`ÏùÑ Í∏∞Î°ùÌïòÏòÄÎã§. ÌïôÏäµ epochÎ•º Îçî ÎäòÎ¶∞Îã§Î©¥ Ïù¥ Î∞©Î≤ïÏù¥ Îçî Ï¢ãÏùÑ Ïàò ÏûàÏßÄ ÏïäÏùÑÍπå?ÌïòÎäî ÏÉùÍ∞ÅÏùÄ Îì§ÏóàÏßÄÎßå ÌòÑÏû¨ ÏÉÅÌÉúÎ°úÎäî Î™®Îç∏Ïùò ÌïôÏäµÍ≤∞Í≥ºÍ∞Ä Ïù¥Ï†ÑÏóê ÎπÑÌï¥ Í∞úÏÑ†ÎêòÏßÄ ÏïäÏïòÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23b9bd",
   "metadata": {},
   "source": [
    "## Step 5. Í≤∞Î°†\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b5f20",
   "metadata": {},
   "source": [
    "### ÌîÑÎ°úÏ†ùÌä∏ ÌèâÍ∞Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333727a4",
   "metadata": {},
   "source": [
    "1. MNLI Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Ï≤òÎ¶¨ÌïòÎäî Ï†ÑÏö© Processor ÌÅ¥ÎûòÏä§Î•º Ï†ïÏÉÅÏ†ÅÏúºÎ°ú Íµ¨ÌòÑÌïòÏòÄÎã§. <br>Processor ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌï¥ 1Í∞ú Ïù¥ÏÉÅÏùò exampleÏóê ÎåÄÌïú Îã®ÏúÑÌÖåÏä§Ìä∏Í∞Ä Ï†ïÏÉÅ ÏßÑÌñâÎêòÏóàÎã§. \n",
    "    - 3Í∞úÏùò ÌÅ¥ÎûòÏä§Î•º Í∞ÄÏßÑ Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎåÄÌïòÏó¨ Processor ÌÅ¥ÎûòÏä§Í∞Ä Ï†ïÏÉÅÏ†ÅÏúºÎ°ú ÏûëÎèô ÎêòÏóàÎã§.\n",
    "2. BERT tokenizerÏôÄ ProcessorÎ•º Í≤∞Ìï©ÌïòÏó¨ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Ï†ïÏÉÅÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÌïòÏòÄÎã§. <br>MNLI Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÏûÖÎ†•Í≥º ÎùºÎ≤®Ïùò Ï†ïÏùòÏóê Ïûò ÎßûÎäî tf.data.Dataset Ïù∏Ïä§ÌÑ¥Ïä§Í∞Ä ÏñªÏñ¥Ï°åÎã§.\n",
    "    - Îç∞Ïù¥ÌÑ∞ÏÖãÏù¥ Ï†ïÏÉÅÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÎã§.\n",
    "3. MNLI Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎåÄÌï¥ BERT Î™®Îç∏ÏùÑ fine-tuningÌïú Í≤∞Í≥º Í∏∞ÎåÄÌïú ÏàòÏ§ÄÏùò Î™®Îç∏ ÏÑ±Îä•ÏùÑ ÌôïÏù∏ÌïòÏòÄÎã§.<br>ÌÖåÏä§Ìä∏ÏÖãÏóê ÎåÄÌï¥ 80% Ïù¥ÏÉÅÏùò accuracyÎ•º ÌôïÏù∏ÌïòÏòÄÎã§. \n",
    "    - ÏµúÍ≥† test accruacyÎäî 0.3529Î°ú Í∏∞Ï§ÄÏóê ÎØ∏ÏπòÏßÄ Î™ªÌïòÏòÄÎã§.\n",
    "    - Îã§Îßå validation setÏóê ÎåÄÌïú accuracyÎäî 0.8006ÏùÑ Í∏∞Î°ùÌïòÏòÄÎã§."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
